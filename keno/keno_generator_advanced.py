#!/usr/bin/env python3
"""
============================================================================
üé≤ G√âN√âRATEUR INTELLIGENT DE GRILLES KENO - VERSION AVANC√âE üé≤
============================================================================

G√©n√©rateur de grilles Keno utilisant l'apprentissage automatique (XGBoost) 
et l'analyse statistique pour optimiser les combinaisons.

Caract√©ristiques:
- Machine Learning avec XGBoost pour pr√©dire les num√©ros probables
- Strat√©gie adaptative avec pond√©ration dynamique ML/Fr√©quence  
- Analyse de patterns temporels et cycliques
- Optimisation des combinaisons selon crit√®res statistiques
- G√©n√©ration de rapports d√©taill√©s avec visualisations

Auteur: Assistant IA
Version: 2.0
Date: Ao√ªt 2025
============================================================================
"""

import pandas as pd
import numpy as np
import random
import argparse
import sys
import time
import warnings
from datetime import datetime, timedelta
from pathlib import Path
import os
import json
import pickle
from typing import List, Tuple, Dict, Optional, Any
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
import multiprocessing as mp
from dataclasses import dataclass
from tqdm import tqdm
import logging

# ML et analyse
try:
    import xgboost as xgb
    from sklearn.model_selection import train_test_split, cross_val_score
    from sklearn.metrics import accuracy_score, classification_report
    from sklearn.preprocessing import StandardScaler
    HAS_ML = True
except ImportError:
    HAS_ML = False
    print("‚ö†Ô∏è  Modules ML non disponibles. Mode fr√©quence uniquement.")

# Visualisation et analyse
try:
    import matplotlib.pyplot as plt
    import seaborn as sns
    from scipy import stats
    from scipy.fft import fft, fftfreq
    from scipy.signal import find_peaks
    from statsmodels.tsa.seasonal import STL
    from statsmodels.tsa.stattools import acf
    HAS_VIZ = True
except ImportError:
    HAS_VIZ = False
    print("‚ö†Ô∏è  Modules de visualisation non disponibles.")

# Configuration des warnings
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)

# ==============================================================================
# üîß CONFIGURATION ET CONSTANTES
# ==============================================================================

# Param√®tres sp√©cifiques au Keno
KENO_PARAMS = {
    'total_numbers': 70,        # Num√©ros de 1 √† 70
    'numbers_per_draw': 20,     # 20 num√©ros tir√©s par tirage
    'player_selection': 10,     # Le joueur s√©lectionne typiquement 10 num√©ros
    'min_selection': 2,         # Minimum de num√©ros s√©lectionnables
    'max_selection': 10,        # Maximum de num√©ros s√©lectionnables
}

# Chemins des fichiers
BASE_DIR = Path(__file__).parent
DATA_DIR = BASE_DIR / "keno_data"
MODELS_DIR = BASE_DIR.parent / "keno_models"
OUTPUT_DIR = BASE_DIR.parent / "keno_output"

# Configuration ML
ML_CONFIG = {
    'n_estimators': 100,
    'max_depth': 6,
    'learning_rate': 0.1,
    'random_state': 42,
    'n_jobs': -1,
    'objective': 'binary:logistic'
}

def get_training_params(profile: str) -> dict:
    """
    Retourne les param√®tres d'entra√Ænement selon le profil s√©lectionn√©
    
    Args:
        profile: Profil d'entra√Ænement ('quick', 'balanced', 'comprehensive', 'intensive')
    
    Returns:
        dict: Param√®tres d'entra√Ænement pour RandomForest
    """
    if profile == "quick":
        return {
            'n_estimators': 50,        # Arbres r√©duits pour la vitesse
            'max_depth': 8,            # Profondeur mod√©r√©e
            'min_samples_split': 10,   # √âviter l'overfitting
            'min_samples_leaf': 5,     # √âviter l'overfitting
            'max_features': 'sqrt',    # Features r√©duites
            'random_state': 42,
            'n_jobs': -1,
            'class_weight': 'balanced'
        }
    elif profile == "balanced":
        return {
            'n_estimators': 100,       # Standard
            'max_depth': 12,           # Profondeur √©quilibr√©e
            'min_samples_split': 5,    # Standard
            'min_samples_leaf': 3,     # Standard
            'max_features': 'sqrt',    # Standard
            'random_state': 42,
            'n_jobs': -1,
            'class_weight': 'balanced'
        }
    elif profile == "comprehensive":
        return {
            'n_estimators': 200,       # Plus d'arbres pour meilleure pr√©cision
            'max_depth': 15,           # Profondeur √©lev√©e
            'min_samples_split': 4,    # Plus de splits
            'min_samples_leaf': 2,     # Feuilles plus petites
            'max_features': 'log2',    # Plus de features
            'random_state': 42,
            'n_jobs': -1,
            'class_weight': 'balanced'
        }
    elif profile == "intensive":
        return {
            'n_estimators': 300,       # Maximum d'arbres
            'max_depth': 20,           # Profondeur maximale
            'min_samples_split': 3,    # Splits agressifs
            'min_samples_leaf': 1,     # Feuilles minimales
            'max_features': None,      # Toutes les features
            'random_state': 42,
            'n_jobs': -1,
            'class_weight': 'balanced'
        }
    else:
        # Fallback sur balanced
        return get_training_params("balanced")

@dataclass
class KenoStats:
    """Structure pour stocker les statistiques Keno"""
    frequences: Dict[int, int]
    retards: Dict[int, int]
    paires_freq: Dict[Tuple[int, int], int]
    zones_freq: Dict[str, int]
    derniers_tirages: List[List[int]]

# ==============================================================================
# üéØ CLASSE PRINCIPALE GENERATEUR KENO
# ==============================================================================

class KenoGeneratorAdvanced:
    """G√©n√©rateur avanc√© de grilles Keno avec ML et analyse statistique"""
    
    def __init__(self, data_path: str = None, silent: bool = False, training_profile: str = "balanced"):
        """
        Initialise le g√©n√©rateur Keno
        
        Args:
            data_path: Chemin vers les donn√©es historiques
            silent: Mode silencieux pour r√©duire les sorties
            training_profile: Profil d'entra√Ænement ('quick', 'balanced', 'comprehensive', 'intensive')
        """
        self.silent = silent
        self.data_path = data_path or str(DATA_DIR / "keno_202010.parquet")
        self.models_dir = MODELS_DIR
        self.output_dir = OUTPUT_DIR
        self.training_profile = training_profile
        
        # Cr√©er les r√©pertoires n√©cessaires
        self.models_dir.mkdir(exist_ok=True)
        self.output_dir.mkdir(exist_ok=True)
        
        # Initialisation des composants
        self.data = None
        self.stats = None
        self.ml_models = {}
        self.metadata = {}
        self.cache = {}
        
        # Strat√©gie adaptative
        self.adaptive_weights = {
            'ml_weight': 0.6,      # 60% ML par d√©faut
            'freq_weight': 0.4,    # 40% fr√©quence par d√©faut  
            'performance_history': [],
            'last_update': datetime.now()
        }
        
        self._log("üé≤ G√©n√©rateur Keno Avanc√© v2.0 initialis√©")
        
    def _log(self, message: str, level: str = "INFO"):
        """Syst√®me de logging configur√©"""
        if not self.silent or level == "ERROR":
            timestamp = datetime.now().strftime("%H:%M:%S")
            print(f"[{timestamp}] {message}")
    
    def load_data(self) -> bool:
        """
        Charge les donn√©es historiques des tirages Keno
        
        Returns:
            bool: True si les donn√©es sont charg√©es avec succ√®s
        """
        try:
            self._log("üìä Chargement des donn√©es historiques Keno...")
            
            if not Path(self.data_path).exists():
                self._log(f"‚ùå Fichier non trouv√©: {self.data_path}", "ERROR")
                return False
                
            # Chargement depuis Parquet
            self.data = pd.read_parquet(self.data_path)
            
            # Validation des colonnes requises
            required_cols = ['date_de_tirage'] + [f'boule{i}' for i in range(1, 21)]
            missing_cols = [col for col in required_cols if col not in self.data.columns]
            
            if missing_cols:
                self._log(f"‚ùå Colonnes manquantes: {missing_cols}", "ERROR")
                return False
            
            # Tri par date
            self.data = self.data.sort_values('date_de_tirage').reset_index(drop=True)
            
            self._log(f"‚úÖ {len(self.data)} tirages Keno charg√©s ({self.data['date_de_tirage'].min()} √† {self.data['date_de_tirage'].max()})")
            return True
            
        except Exception as e:
            self._log(f"‚ùå Erreur lors du chargement des donn√©es: {e}", "ERROR")
            return False
    
    def analyze_patterns(self) -> KenoStats:
        """
        Analyse les patterns et statistiques des tirages Keno
        
        Returns:
            KenoStats: Statistiques calcul√©es
        """
        self._log("üîç Analyse des patterns Keno...")
        
        # Initialisation des statistiques
        frequences = {i: 0 for i in range(1, KENO_PARAMS['total_numbers'] + 1)}
        retards = {i: 0 for i in range(1, KENO_PARAMS['total_numbers'] + 1)}
        paires_freq = {}
        zones_freq = {"zone1_17": 0, "zone18_35": 0, "zone36_52": 0, "zone53_70": 0}
        
        # Extraction des num√©ros de tous les tirages
        all_draws = []
        for _, row in self.data.iterrows():
            draw = [int(row[f'boule{i}']) for i in range(1, 21)]
            all_draws.append(sorted(draw))
            
            # Comptage des fr√©quences
            for num in draw:
                frequences[num] += 1
            
            # Analyse par zones
            for num in draw:
                if 1 <= num <= 17:
                    zones_freq["zone1_17"] += 1
                elif 18 <= num <= 35:
                    zones_freq["zone18_35"] += 1
                elif 36 <= num <= 52:
                    zones_freq["zone36_52"] += 1
                else:  # 53-70
                    zones_freq["zone53_70"] += 1
        
        # Calcul des retards
        for num in range(1, KENO_PARAMS['total_numbers'] + 1):
            retard = 0
            for draw in reversed(all_draws):
                if num in draw:
                    break
                retard += 1
            retards[num] = retard
        
        # Analyse des paires fr√©quentes  
        for draw in all_draws:
            for i in range(len(draw)):
                for j in range(i + 1, len(draw)):
                    pair = tuple(sorted([draw[i], draw[j]]))
                    paires_freq[pair] = paires_freq.get(pair, 0) + 1
        
        # Garder les derniers tirages pour l'analyse
        derniers_tirages = all_draws[-50:] if len(all_draws) >= 50 else all_draws
        
        self.stats = KenoStats(
            frequences=frequences,
            retards=retards, 
            paires_freq=paires_freq,
            zones_freq=zones_freq,
            derniers_tirages=derniers_tirages
        )
        
        self._log(f"‚úÖ Analyse termin√©e - {len(all_draws)} tirages analys√©s")
        return self.stats
    
    def add_cyclic_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Ajoute des features cycliques pour l'apprentissage automatique
        
        Args:
            df: DataFrame avec les donn√©es
            
        Returns:
            DataFrame avec les features ajout√©es
        """
        df = df.copy()
        
        # Features temporelles cycliques
        df['day_sin'] = np.sin(2 * np.pi * df['date_de_tirage'].dt.day / 31)
        df['day_cos'] = np.cos(2 * np.pi * df['date_de_tirage'].dt.day / 31)
        df['month_sin'] = np.sin(2 * np.pi * df['date_de_tirage'].dt.month / 12)  
        df['month_cos'] = np.cos(2 * np.pi * df['date_de_tirage'].dt.month / 12)
        
        # Features cycliques pour chaque num√©ro (1-70 pour Keno)
        for i in range(1, 11):  # Pour les 10 premi√®res boules principales
            if f'boule{i}' in df.columns:
                df[f'boule{i}_sin'] = np.sin(2 * np.pi * df[f'boule{i}'] / KENO_PARAMS['total_numbers'])
                df[f'boule{i}_cos'] = np.cos(2 * np.pi * df[f'boule{i}'] / KENO_PARAMS['total_numbers'])
        
        return df
    
    def train_xgboost_models(self, retrain: bool = False) -> bool:
        """
        Entra√Æne un mod√®le XGBoost multi-label pour pr√©dire les num√©ros Keno
        
        Args:
            retrain: Force le r√©entra√Ænement m√™me si le mod√®le existe
            
        Returns:
            bool: True si l'entra√Ænement est r√©ussi
        """
        if not HAS_ML:
            self._log("‚ùå Modules ML non disponibles", "ERROR")
            return False
        
        # V√©rification du mod√®le existant (un seul mod√®le multi-label)
        model_file = self.models_dir / "xgb_keno_multilabel.pkl"
        if not retrain and model_file.exists():
            self._log("‚úÖ Mod√®le XGBoost Keno multi-label existant trouv√©")
            return self.load_ml_models()
        
        self._log("ü§ñ Entra√Ænement du mod√®le XGBoost Keno multi-label...")
        self._log("   üìä Strat√©gie: 1 mod√®le multi-label pour apprendre les corr√©lations")
        
        try:
            # Pr√©paration des donn√©es avec features enrichies (optimis√© pour √©viter la fragmentation)
            df_features = self.add_cyclic_features(self.data)
            
            # Features temporelles
            feature_cols = ['day_sin', 'day_cos', 'month_sin', 'month_cos']
            
            # Pr√©paration des features d'historique avec pd.concat pour √©viter la fragmentation
            self._log("   üìù Cr√©ation des features d'historique...")
            lag_features = {}
            
            for lag in range(1, 6):
                for ball_num in range(1, 21):
                    col_name = f'lag{lag}_boule{ball_num}'
                    if lag < len(df_features):
                        lag_features[col_name] = df_features[f'boule{ball_num}'].shift(lag).fillna(0)
                    else:
                        lag_features[col_name] = pd.Series([0] * len(df_features))
                    feature_cols.append(col_name)
            
            # Ajout des features d'historique en une seule fois
            lag_df = pd.DataFrame(lag_features, index=df_features.index)
            df_features = pd.concat([df_features, lag_df], axis=1)
            
            # Features de fr√©quence par zone (calcul√©es efficacement)
            self._log("   üìù Calcul des features de zones...")
            zone_features = {
                'zone1_count': [],
                'zone2_count': [],
                'zone3_count': [],
                'zone4_count': []
            }
            
            for idx, row in df_features.iterrows():
                draw_numbers = [int(row[f'boule{i}']) for i in range(1, 21)]
                zone_features['zone1_count'].append(sum(1 for n in draw_numbers if 1 <= n <= 17))
                zone_features['zone2_count'].append(sum(1 for n in draw_numbers if 18 <= n <= 35))
                zone_features['zone3_count'].append(sum(1 for n in draw_numbers if 36 <= n <= 52))
                zone_features['zone4_count'].append(sum(1 for n in draw_numbers if 53 <= n <= 70))
            
            # Ajout des features de zones
            for zone_name, zone_values in zone_features.items():
                df_features[zone_name] = zone_values
                feature_cols.append(zone_name)
            
            X = df_features[feature_cols].fillna(0)
            
            # Cr√©ation du target multi-label (matrice 70 colonnes, une par num√©ro)
            y = np.zeros((len(df_features), KENO_PARAMS['total_numbers']))
            
            for idx, row in df_features.iterrows():
                draw_numbers = [int(row[f'boule{i}']) for i in range(1, 21)]
                for num in draw_numbers:
                    if 1 <= num <= KENO_PARAMS['total_numbers']:
                        y[idx, num - 1] = 1  # Index 0-69 pour num√©ros 1-70
            
            self._log(f"   üìù Donn√©es pr√©par√©es: {X.shape[0]} tirages, {X.shape[1]} features")
            self._log(f"   üìù Target multi-label: {y.shape[1]} num√©ros √† pr√©dire")
            
            # Split train/test
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42
            )
            
            # Configuration sp√©ciale pour multi-label
            ml_config = ML_CONFIG.copy()
            ml_config['objective'] = 'multi:logistic'
            ml_config['num_class'] = 2  # Binaire pour chaque label
            
            # Entra√Ænement du mod√®le multi-label avec RandomForest (meilleur pour les corr√©lations)
            from sklearn.ensemble import RandomForestClassifier
            from sklearn.multioutput import MultiOutputClassifier
            
            # Obtenir les param√®tres selon le profil d'entra√Ænement
            rf_params = get_training_params(self.training_profile)
            
            # Afficher le profil utilis√©
            profile_names = {
                'quick': 'Ultra-rapide',
                'balanced': '√âquilibr√©', 
                'comprehensive': 'Complet',
                'intensive': 'Intensif'
            }
            profile_name = profile_names.get(self.training_profile, '√âquilibr√©')
            self._log(f"üìä Profil d'entra√Ænement: {profile_name} ({self.training_profile})")
            self._log(f"   ‚Ä¢ Arbres: {rf_params['n_estimators']}")
            self._log(f"   ‚Ä¢ Profondeur max: {rf_params['max_depth']}")
            self._log(f"   ‚Ä¢ Features: {rf_params['max_features']}")
            
            # RandomForest configur√© selon le profil
            base_model = RandomForestClassifier(**rf_params)
            
            # Alternative: Essayer aussi XGBoost avec MultiOutputClassifier
            # base_model = xgb.XGBClassifier(
            #     objective='binary:logistic',
            #     n_estimators=150,
            #     max_depth=8,
            #     learning_rate=0.05,
            #     random_state=42,
            #     n_jobs=-1
            # )
            
            # Mod√®le multi-output
            model = MultiOutputClassifier(base_model, n_jobs=-1)
            
            self._log("   üîÑ Entra√Ænement du mod√®le RandomForest multi-label...")
            model.fit(X_train, y_train)
            
            # √âvaluation rapide
            y_pred = model.predict(X_test)
            
            # Calcul de l'accuracy moyenne sur tous les labels
            accuracies = []
            for i in range(y.shape[1]):
                acc = accuracy_score(y_test[:, i], y_pred[:, i])
                accuracies.append(acc)
            
            mean_accuracy = np.mean(accuracies)
            self._log(f"   üìä Accuracy moyenne: {mean_accuracy:.4f}")
            
            # Sauvegarde du mod√®le
            with open(model_file, 'wb') as f:
                pickle.dump(model, f)
            
            self.ml_models['multilabel'] = model
            
            # Sauvegarde des m√©tadonn√©es
            self.metadata = {
                'features_count': len(feature_cols),
                'model_type': 'xgboost_keno_multilabel',
                'created_date': datetime.now().strftime('%Y-%m-%d'),
                'version': '3.0',  # Version avec multi-label
                'keno_params': KENO_PARAMS,
                'mean_accuracy': mean_accuracy,
                'feature_names': feature_cols
            }
            
            metadata_path = self.models_dir / "metadata.json"
            with open(metadata_path, 'w') as f:
                json.dump(self.metadata, f, indent=2)
            
            self._log("‚úÖ Entra√Ænement du mod√®le multi-label Keno termin√©")
            return True
            
        except Exception as e:
            self._log(f"‚ùå Erreur lors de l'entra√Ænement: {e}", "ERROR")
            import traceback
            self._log(f"D√©tails: {traceback.format_exc()}")
            return False
    
    def load_ml_models(self) -> bool:
        """Charge le mod√®le ML multi-label pr√©-entra√Æn√©"""
        try:
            self._log("üì• Chargement du mod√®le ML Keno multi-label...")
            
            # Chargement des m√©tadonn√©es
            metadata_path = self.models_dir / "metadata.json"
            if metadata_path.exists():
                with open(metadata_path, 'r') as f:
                    self.metadata = json.load(f)
            
            # Chargement du mod√®le multi-label
            model_path = self.models_dir / "xgb_keno_multilabel.pkl"
            if model_path.exists():
                with open(model_path, 'rb') as f:
                    self.ml_models['multilabel'] = pickle.load(f)
                
                self._log("‚úÖ Mod√®le multi-label Keno charg√© avec succ√®s")
                if 'mean_accuracy' in self.metadata:
                    self._log(f"   üìä Accuracy du mod√®le: {self.metadata['mean_accuracy']:.4f}")
                return True
            else:
                self._log("‚ùå Mod√®le multi-label non trouv√©", "ERROR")
                return False
                
        except Exception as e:
            self._log(f"‚ùå Erreur lors du chargement du mod√®le: {e}", "ERROR")
            return False
    
    def predict_numbers_ml(self, num_grids: int = 10) -> List[List[int]]:
        """
        Pr√©dit les num√©ros avec le mod√®le ML multi-label
        
        Args:
            num_grids: Nombre de grilles √† g√©n√©rer
            
        Returns:
            List[List[int]]: Liste des grilles pr√©dites
        """
        if 'multilabel' not in self.ml_models:
            self._log("‚ùå Mod√®le ML multi-label non disponible", "ERROR")
            return []
        
        try:
            predictions = []
            model = self.ml_models['multilabel']
            
            # Pr√©paration des features pour la pr√©diction
            current_date = pd.Timestamp.now()
            df_predict = pd.DataFrame({
                'date_de_tirage': [current_date],
                **{f'boule{i}': [0] for i in range(1, 21)}  # Valeurs dummy
            })
            
            # Ajout des features temporelles
            df_features = self.add_cyclic_features(df_predict)
            
            # Reconstruction des m√™mes features que lors de l'entra√Ænement (optimis√©)
            feature_cols = ['day_sin', 'day_cos', 'month_sin', 'month_cos']
            
            # Features d'historique (utiliser les derniers tirages disponibles)
            lag_features = {}
            
            if self.stats and self.stats.derniers_tirages:
                for lag in range(1, 6):
                    for ball_num in range(1, 21):
                        col_name = f'lag{lag}_boule{ball_num}'
                        # Utiliser les derniers tirages pour construire l'historique
                        if lag <= len(self.stats.derniers_tirages):
                            last_draw = self.stats.derniers_tirages[-(lag)]
                            if ball_num <= len(last_draw):
                                lag_features[col_name] = last_draw[ball_num - 1] if ball_num <= len(last_draw) else 0
                            else:
                                lag_features[col_name] = 0
                        else:
                            lag_features[col_name] = 0
                        feature_cols.append(col_name)
            else:
                # Valeurs par d√©faut si pas d'historique
                for lag in range(1, 6):
                    for ball_num in range(1, 21):
                        col_name = f'lag{lag}_boule{ball_num}'
                        lag_features[col_name] = 0
                        feature_cols.append(col_name)
            
            # Ajout des features d'historique en une fois pour √©viter la fragmentation
            lag_df = pd.DataFrame(lag_features, index=df_features.index)
            
            # Features de zones (moyennes historiques)
            zone_features = {}
            if self.stats:
                total_draws = len(self.stats.derniers_tirages) if self.stats.derniers_tirages else 1
                zone_features['zone1_count'] = self.stats.zones_freq.get("zone1_17", 0) / total_draws
                zone_features['zone2_count'] = self.stats.zones_freq.get("zone18_35", 0) / total_draws 
                zone_features['zone3_count'] = self.stats.zones_freq.get("zone36_52", 0) / total_draws
                zone_features['zone4_count'] = self.stats.zones_freq.get("zone53_70", 0) / total_draws
            else:
                zone_features['zone1_count'] = 5  # Valeurs moyennes
                zone_features['zone2_count'] = 5
                zone_features['zone3_count'] = 5
                zone_features['zone4_count'] = 5
            
            # Cr√©ation du DataFrame pour les zones
            zone_df = pd.DataFrame([zone_features], index=df_features.index)
            
            # Concat√©nation de toutes les nouvelles features en une seule fois
            df_features = pd.concat([df_features, lag_df, zone_df], axis=1)
                
            feature_cols.extend(['zone1_count', 'zone2_count', 'zone3_count', 'zone4_count'])
            
            # Pr√©paration des features pour la pr√©diction
            X_pred = df_features[feature_cols].fillna(0)
            
            # G√©n√©ration de grilles avec variation
            for grid_idx in range(num_grids):
                # Ajout de petites variations al√©atoires pour diversifier les pr√©dictions
                X_pred_variant = X_pred.copy()
                
                # Petites variations sur les features temporelles
                noise_factor = 0.1 * (grid_idx / max(1, num_grids - 1))  # Variation progressive
                X_pred_variant['day_sin'] += np.random.normal(0, noise_factor)
                X_pred_variant['day_cos'] += np.random.normal(0, noise_factor)
                
                # Pr√©diction des probabilit√©s pour tous les num√©ros
                probabilities = model.predict_proba(X_pred_variant)
                
                # Extraction des probabilit√©s pour la classe positive (num√©ro tir√©)
                # probabilities est une liste de probabilit√©s pour chaque output
                number_probs = {}
                for i in range(KENO_PARAMS['total_numbers']):
                    num = i + 1  # Num√©ros de 1 √† 70
                    if len(probabilities[i][0]) > 1:  # V√©rifier qu'on a bien les 2 classes
                        prob = probabilities[i][0][1]  # Probabilit√© de la classe positive
                    else:
                        prob = 0.5  # Valeur par d√©faut
                    number_probs[num] = prob
                
                # Simulation d'un tirage r√©aliste Keno
                # Dans un vrai tirage, on tire 20 num√©ros, mais le joueur en s√©lectionne 10
                
                # S√©lection des num√©ros avec pond√©ration par probabilit√©
                numbers = list(range(1, KENO_PARAMS['total_numbers'] + 1))
                probs = [number_probs[num] for num in numbers]
                
                # Normalisation des probabilit√©s
                probs_array = np.array(probs)
                probs_normalized = probs_array / np.sum(probs_array)
                
                # Ajustement pour tenir compte des corr√©lations
                # Boost des num√©ros qui apparaissent souvent ensemble
                if self.stats and self.stats.paires_freq:
                    correlation_boost = {num: 0 for num in numbers}
                    
                    # Identifier les paires fr√©quentes
                    top_pairs = sorted(self.stats.paires_freq.items(), 
                                     key=lambda x: x[1], reverse=True)[:50]
                    
                    for (num1, num2), freq in top_pairs:
                        if num1 in number_probs and num2 in number_probs:
                            # Si les deux num√©ros ont des probabilit√©s √©lev√©es, boost mutuel
                            if number_probs[num1] > 0.5 and number_probs[num2] > 0.5:
                                correlation_boost[num1] += 0.1 * (freq / 1000)
                                correlation_boost[num2] += 0.1 * (freq / 1000)
                    
                    # Application du boost
                    for num in numbers:
                        idx = num - 1
                        probs_normalized[idx] += correlation_boost[num]
                    
                    # Re-normalisation
                    probs_normalized = probs_normalized / np.sum(probs_normalized)
                
                # S√©lection pond√©r√©e de 10 num√©ros (pour le joueur)
                try:
                    selected = np.random.choice(
                        numbers, 
                        size=10, 
                        replace=False, 
                        p=probs_normalized
                    )
                    grid_numbers = sorted(selected.tolist())
                except:
                    # Fallback en cas d'erreur
                    top_indices = np.argsort(probs_normalized)[-10:]
                    grid_numbers = sorted([numbers[i] for i in top_indices])
                
                predictions.append(grid_numbers)
            
            return predictions
            
        except Exception as e:
            self._log(f"‚ùå Erreur lors de la pr√©diction ML: {e}", "ERROR")
            import traceback
            self._log(f"D√©tails: {traceback.format_exc()}")
            return []
    
    def generate_frequency_based_grids(self, num_grids: int = 10) -> List[List[int]]:
        """
        G√©n√®re des grilles bas√©es sur l'analyse de fr√©quence
        
        Args:
            num_grids: Nombre de grilles √† g√©n√©rer
            
        Returns:
            List[List[int]]: Liste des grilles g√©n√©r√©es
        """
        if not self.stats:
            self._log("‚ùå Statistiques non disponibles", "ERROR")
            return []
        
        grids = []
        
        # Pr√©paration des listes de num√©ros par crit√®res
        freq_sorted = sorted(self.stats.frequences.items(), key=lambda x: x[1], reverse=True)
        retard_sorted = sorted(self.stats.retards.items(), key=lambda x: x[1], reverse=True)
        
        hot_numbers = [num for num, _ in freq_sorted[:35]]  # Top 35 num√©ros fr√©quents
        cold_numbers = [num for num, _ in retard_sorted[:35]]  # Top 35 num√©ros en retard
        
        for _ in range(num_grids):
            grid = []
            
            # Strat√©gie mixte: hot + cold + al√©atoire
            # 5 num√©ros chauds
            hot_selection = random.sample(hot_numbers, min(5, len(hot_numbers)))
            grid.extend(hot_selection)
            
            # 3 num√©ros froids  
            available_cold = [n for n in cold_numbers if n not in grid]
            cold_selection = random.sample(available_cold, min(3, len(available_cold)))
            grid.extend(cold_selection)
            
            # 2 num√©ros al√©atoires
            available_random = [n for n in range(1, KENO_PARAMS['total_numbers'] + 1) if n not in grid]
            random_selection = random.sample(available_random, min(2, len(available_random)))
            grid.extend(random_selection)
            
            # Assurer qu'on a exactement 10 num√©ros
            while len(grid) < 10:
                available = [n for n in range(1, KENO_PARAMS['total_numbers'] + 1) if n not in grid]
                if available:
                    grid.append(random.choice(available))
                else:
                    break
            
            grids.append(sorted(grid[:10]))
        
        return grids
    
    def calculate_grid_score(self, grid: List[int]) -> float:
        """
        Calcule un score de qualit√© pour une grille Keno
        
        Args:
            grid: Liste des num√©ros de la grille
            
        Returns:
            float: Score de qualit√©
        """
        if not self.stats:
            return 0.0
        
        score = 0.0
        
        # Score bas√© sur les fr√©quences normalis√©es
        total_freq = sum(self.stats.frequences.values())
        for num in grid:
            freq_score = self.stats.frequences[num] / total_freq
            score += freq_score
        
        # Bonus pour la r√©partition par zones
        zones = {
            "zone1_17": sum(1 for n in grid if 1 <= n <= 17),
            "zone18_35": sum(1 for n in grid if 18 <= n <= 35), 
            "zone36_52": sum(1 for n in grid if 36 <= n <= 52),
            "zone53_70": sum(1 for n in grid if 53 <= n <= 70)
        }
        
        # P√©nalit√© pour d√©s√©quilibre extr√™me des zones
        zone_counts = list(zones.values())
        if max(zone_counts) <= 4 and min(zone_counts) >= 1:  # R√©partition √©quilibr√©e
            score += 0.1
        
        # Bonus pour les paires fr√©quentes
        for i in range(len(grid)):
            for j in range(i + 1, len(grid)):
                pair = tuple(sorted([grid[i], grid[j]]))
                if pair in self.stats.paires_freq:
                    pair_freq = self.stats.paires_freq[pair]
                    score += pair_freq / 1000  # Normalisation
        
        # Score de dispersion (√©viter les num√©ros trop cons√©cutifs)
        consecutive_count = 0
        sorted_grid = sorted(grid)
        for i in range(len(sorted_grid) - 1):
            if sorted_grid[i + 1] - sorted_grid[i] == 1:
                consecutive_count += 1
        
        if consecutive_count <= 2:  # Maximum 2 paires cons√©cutives acceptables
            score += 0.05
        
        return score
    
    def generate_optimized_grids(self, num_grids: int = 100) -> List[Tuple[List[int], float]]:
        """
        G√©n√®re des grilles optimis√©es en combinant ML et analyse fr√©quentielle
        
        Args:
            num_grids: Nombre de grilles √† g√©n√©rer
            
        Returns:
            List[Tuple[List[int], float]]: Liste des grilles avec leurs scores
        """
        self._log(f"üéØ G√©n√©ration de {num_grids} grilles Keno optimis√©es...")
        
        all_grids = []
        
        # R√©partition selon les poids adaptatifs
        ml_count = int(num_grids * self.adaptive_weights['ml_weight'])
        freq_count = num_grids - ml_count
        
        self._log(f"   ü§ñ Poids adaptatifs: ML={self.adaptive_weights['ml_weight']:.1%}, Freq={self.adaptive_weights['freq_weight']:.1%}")
        
        # G√©n√©ration ML
        if ml_count > 0 and self.ml_models:
            ml_grids = self.predict_numbers_ml(ml_count)
            all_grids.extend(ml_grids)
        
        # G√©n√©ration bas√©e sur les fr√©quences
        if freq_count > 0:
            freq_grids = self.generate_frequency_based_grids(freq_count)
            all_grids.extend(freq_grids)
        
        # Calcul des scores et tri
        scored_grids = []
        for grid in all_grids:
            if len(grid) == 10:  # Validation
                score = self.calculate_grid_score(grid)
                scored_grids.append((grid, score))
        
        # Tri par score d√©croissant
        scored_grids.sort(key=lambda x: x[1], reverse=True)
        
        # Suppression des doublons
        unique_grids = []
        seen = set()
        for grid, score in scored_grids:
            grid_tuple = tuple(sorted(grid))
            if grid_tuple not in seen:
                unique_grids.append((grid, score))
                seen.add(grid_tuple)
        
        self._log(f"‚úÖ {len(unique_grids)} grilles uniques g√©n√©r√©es")
        return unique_grids[:num_grids]
    
    def save_results(self, grids_with_scores: List[Tuple[List[int], float]], 
                    filename: str = None) -> str:
        """
        Sauvegarde les r√©sultats dans un fichier CSV
        
        Args:
            grids_with_scores: Liste des grilles avec scores
            filename: Nom du fichier (optionnel)
            
        Returns:
            str: Chemin du fichier sauvegard√©
        """
        if not filename:
            # Utilisation d'un nom fixe qui remplace le fichier pr√©c√©dent
            filename = "grilles_keno.csv"
        
        filepath = self.output_dir / filename
        
        # Pr√©paration des donn√©es
        data_rows = []
        for grid, score in grids_with_scores:
            row = {}
            for i, num in enumerate(grid, 1):
                row[f'numero_{i}'] = num
            row['score'] = score
            data_rows.append(row)
        
        # Sauvegarde
        df_results = pd.DataFrame(data_rows)
        df_results.to_csv(filepath, index=False)
        
        self._log(f"üíæ R√©sultats sauvegard√©s: {filepath}")
        return str(filepath)
    
    def generate_report(self, grids_with_scores: List[Tuple[List[int], float]]) -> str:
        """
        G√©n√®re un rapport d'analyse d√©taill√©
        
        Args:
            grids_with_scores: Grilles avec leurs scores
            
        Returns:
            str: Contenu du rapport
        """
        if not grids_with_scores:
            return "Aucune grille g√©n√©r√©e."
        
        report = []
        report.append("# üé≤ RAPPORT D'ANALYSE KENO")
        report.append("=" * 50)
        report.append(f"üìÖ **Date**: {datetime.now().strftime('%d/%m/%Y %H:%M')}")
        report.append(f"üìä **Grilles g√©n√©r√©es**: {len(grids_with_scores)}")
        report.append("")
        
        # Top 5 des grilles
        report.append("## üèÜ TOP 5 DES GRILLES RECOMMAND√âES")
        report.append("")
        for i, (grid, score) in enumerate(grids_with_scores[:5], 1):
            nums_str = " - ".join(f"{n:2d}" for n in grid)
            report.append(f"**{i}.** [{nums_str}] | Score: {score:.4f}")
        report.append("")
        
        # Statistiques
        scores = [score for _, score in grids_with_scores]
        report.append("## üìà STATISTIQUES")
        report.append("")
        report.append(f"- **Score moyen**: {np.mean(scores):.4f}")
        report.append(f"- **Score m√©dian**: {np.median(scores):.4f}")
        report.append(f"- **Meilleur score**: {max(scores):.4f}")
        report.append(f"- **Score minimum**: {min(scores):.4f}")
        report.append("")
        
        # Analyse des num√©ros les plus s√©lectionn√©s
        all_numbers = []
        for grid, _ in grids_with_scores:
            all_numbers.extend(grid)
        
        num_counts = {}
        for num in all_numbers:
            num_counts[num] = num_counts.get(num, 0) + 1
        
        top_numbers = sorted(num_counts.items(), key=lambda x: x[1], reverse=True)[:10]
        
        report.append("## üî• NUM√âROS LES PLUS S√âLECTIONN√âS")
        report.append("")
        for num, count in top_numbers:
            percentage = (count / len(grids_with_scores)) * 100
            report.append(f"- **{num:2d}**: {count} fois ({percentage:.1f}%)")
        report.append("")
        
        # Strat√©gie adaptative
        report.append("## ü§ñ STRAT√âGIE ADAPTATIVE")
        report.append("")
        report.append(f"- **Poids ML**: {self.adaptive_weights['ml_weight']:.1%}")
        report.append(f"- **Poids Fr√©quence**: {self.adaptive_weights['freq_weight']:.1%}")
        report.append("")
        
        return "\n".join(report)
    
    def run(self, num_grids: int = 100, save_file: str = None, 
           retrain: bool = False) -> bool:
        """
        Lance la g√©n√©ration compl√®te de grilles Keno
        
        Args:
            num_grids: Nombre de grilles √† g√©n√©rer
            save_file: Nom du fichier de sauvegarde
            retrain: Force le r√©entra√Ænement des mod√®les
            
        Returns:
            bool: True si succ√®s
        """
        start_time = time.time()
        
        try:
            # 1. Chargement des donn√©es
            if not self.load_data():
                return False
            
            # 2. Analyse des patterns
            self.analyze_patterns()
            
            # 3. Mod√®les ML
            if HAS_ML:
                if retrain or not self.load_ml_models():
                    if not self.train_xgboost_models(retrain=retrain):
                        self._log("‚ö†Ô∏è  Utilisation du mode fr√©quence uniquement")
                        self.adaptive_weights['ml_weight'] = 0.0
                        self.adaptive_weights['freq_weight'] = 1.0
            else:
                self._log("‚ö†Ô∏è  Mode fr√©quence uniquement")
                self.adaptive_weights['ml_weight'] = 0.0
                self.adaptive_weights['freq_weight'] = 1.0
            
            # 4. G√©n√©ration des grilles
            grids_with_scores = self.generate_optimized_grids(num_grids)
            
            if not grids_with_scores:
                self._log("‚ùå Aucune grille g√©n√©r√©e", "ERROR")
                return False
            
            # 5. Sauvegarde
            result_file = self.save_results(grids_with_scores, save_file)
            
            # 6. Rapport
            report = self.generate_report(grids_with_scores)
            
            # Affichage des r√©sultats
            self._log("\n" + "=" * 70)
            self._log("üéØ G√âN√âRATION KENO TERMIN√âE")
            self._log("=" * 70)
            
            # Top 5
            self._log("\nüèÜ Top 5 des grilles recommand√©es:")
            for i, (grid, score) in enumerate(grids_with_scores[:5], 1):
                nums_str = " - ".join(f"{n:2d}" for n in grid)
                self._log(f"   {i}. [{nums_str}] | Score: {score:.4f}")
            
            # Statistiques
            scores = [score for _, score in grids_with_scores]
            self._log(f"\nüìä Statistiques:")
            self._log(f"   - Grilles g√©n√©r√©es: {len(grids_with_scores):,}")
            self._log(f"   - Score moyen: {np.mean(scores):.4f}")
            self._log(f"   - Meilleur score: {max(scores):.4f}")
            
            # Temps d'ex√©cution
            elapsed = time.time() - start_time
            self._log(f"\n‚è±Ô∏è  Temps d'ex√©cution: {elapsed:.2f} secondes")
            self._log(f"üìÅ R√©sultats sauvegard√©s: {result_file}")
            
            # Sauvegarde du rapport
            report_file = self.output_dir / "rapport_keno.md"
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write(report)
            self._log(f"üìÑ Rapport sauvegard√©: {report_file}")
            
            return True
            
        except Exception as e:
            self._log(f"‚ùå Erreur lors de l'ex√©cution: {e}", "ERROR")
            return False

# ==============================================================================
# üöÄ POINT D'ENTR√âE PRINCIPAL
# ==============================================================================

def main():
    """Point d'entr√©e principal du script"""
    
    parser = argparse.ArgumentParser(
        description="üé≤ G√©n√©rateur Intelligent de Grilles Keno v2.0",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples d'utilisation:
  python keno_generator_advanced.py --quick                    # 10 grilles, entra√Ænement rapide
  python keno_generator_advanced.py --balanced                 # 100 grilles, entra√Ænement √©quilibr√© (d√©faut)
  python keno_generator_advanced.py --comprehensive            # 500 grilles, entra√Ænement complet
  python keno_generator_advanced.py --intensive                # 1000 grilles, entra√Ænement intensif
  python keno_generator_advanced.py --retrain --comprehensive  # Force r√©-entra√Ænement + mode complet
  python keno_generator_advanced.py --grids 50 --silent        # 50 grilles en mode silencieux

Profils d'entra√Ænement:
  - quick:        Rapide (50 arbres, profondeur 8)
  - balanced:     √âquilibr√© (100 arbres, profondeur 12) [D√âFAUT]
  - comprehensive: Complet (200 arbres, profondeur 15)
  - intensive:    Intensif (300 arbres, profondeur 20)
        """
    )
    
    parser.add_argument('--grids', type=int, default=100,
                       help='Nombre de grilles √† g√©n√©rer (d√©faut: 100)')
    parser.add_argument('--output', type=str,
                       help='Nom du fichier de sortie (optionnel)')
    parser.add_argument('--retrain', action='store_true',
                       help='Force le r√©entra√Ænement des mod√®les ML')
    parser.add_argument('--quick', action='store_true',
                       help='Mode rapide (10 grilles, entra√Ænement ultra-rapide)')
    parser.add_argument('--balanced', action='store_true',
                       help='Mode √©quilibr√© (100 grilles, entra√Ænement standard)')
    parser.add_argument('--comprehensive', action='store_true',
                       help='Mode complet (500 grilles, entra√Ænement optimis√©)')
    parser.add_argument('--intensive', action='store_true',
                       help='Mode intensif (1000 grilles, entra√Ænement maximal)')
    parser.add_argument('--silent', action='store_true',
                       help='Mode silencieux')
    parser.add_argument('--data', type=str,
                       help='Chemin vers les donn√©es (optionnel)')
    
    args = parser.parse_args()
    
    # Gestion des profils d'entra√Ænement mutuellement exclusifs
    profile_count = sum([args.quick, args.balanced, args.comprehensive, args.intensive])
    if profile_count > 1:
        print("‚ùå Erreur: Un seul profil d'entra√Ænement peut √™tre s√©lectionn√© √† la fois")
        sys.exit(1)
    
    # Configuration selon le profil s√©lectionn√©
    if args.quick:
        args.grids = 10
        training_profile = "quick"
    elif args.balanced:
        args.grids = 100
        training_profile = "balanced"
    elif args.comprehensive:
        args.grids = 500
        training_profile = "comprehensive"
    elif args.intensive:
        args.grids = 1000
        training_profile = "intensive"
    else:
        # Mode par d√©faut (√©quilibr√©)
        training_profile = "balanced"
    
    # Banner
    if not args.silent:
        profile_names = {
            'quick': 'Ultra-rapide',
            'balanced': '√âquilibr√©', 
            'comprehensive': 'Complet',
            'intensive': 'Intensif'
        }
        profile_display = profile_names.get(training_profile, '√âquilibr√©')
        
        print("=" * 70)
        print("üé≤ G√âN√âRATEUR INTELLIGENT DE GRILLES KENO v2.0 üé≤")
        print("=" * 70)
        print("Machine Learning + Analyse Statistique")
        print("Optimisation des combinaisons Keno")
        print(f"üìä Profil: {profile_display} ({args.grids} grilles)")
        print("=" * 70)
    
    # Initialisation et ex√©cution
    generator = KenoGeneratorAdvanced(
        data_path=args.data,
        silent=args.silent,
        training_profile=training_profile
    )
    
    success = generator.run(
        num_grids=args.grids,
        save_file=args.output,
        retrain=args.retrain
    )
    
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()
