#!/usr/bin/env python3
"""
============================================================================
üé≤ G√âN√âRATEUR INTELLIGENT DE GRILLES KENO - VERSION AVANC√âE üé≤
============================================================================

G√©n√©rateur de grilles Keno utilisant l'apprentissage automatique (XGBoost) 
et l'analyse statistique pour optimiser les combinaisons.

Caract√©ristiques:
- Machine Learning avec XGBoost pour pr√©dire les num√©ros probables
- Strat√©gie adaptative avec pond√©ration dynamique ML/Fr√©quence  
- Analyse de patterns temporels et cycliques
- Optimisation des combinaisons selon crit√®res statistiques
- G√©n√©ration de rapports d√©taill√©s avec visualisations

Auteur: Assistant IA
Version: 2.0
Date: Ao√ªt 2025
============================================================================
"""

import pandas as pd
import numpy as np
import random
import argparse
import sys
import time
import warnings
from datetime import datetime, timedelta
from pathlib import Path
import os
import json
import pickle
from typing import List, Tuple, Dict, Optional, Any
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
import multiprocessing as mp
from dataclasses import dataclass
from tqdm import tqdm
import logging
from collections import defaultdict, deque

# DuckDB pour optimiser les requ√™tes
try:
    import duckdb
    HAS_DUCKDB = True
except ImportError:
    HAS_DUCKDB = False
    print("‚ö†Ô∏è  DuckDB non disponible. Utilisation de Pandas uniquement.")

# ML et analyse
try:
    import xgboost as xgb
    from sklearn.model_selection import train_test_split, cross_val_score
    from sklearn.metrics import accuracy_score, classification_report
    from sklearn.preprocessing import StandardScaler
    HAS_ML = True
except ImportError:
    HAS_ML = False
    print("‚ö†Ô∏è  Modules ML non disponibles. Mode fr√©quence uniquement.")

# Visualisation et analyse
try:
    import matplotlib.pyplot as plt
    import seaborn as sns
    from scipy import stats
    from scipy.fft import fft, fftfreq
    from scipy.signal import find_peaks
    from statsmodels.tsa.seasonal import STL
    from statsmodels.tsa.stattools import acf
    HAS_VIZ = True
except ImportError:
    HAS_VIZ = False
    print("‚ö†Ô∏è  Modules de visualisation non disponibles.")

# Configuration des warnings
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)

# ==============================================================================
# üîß CONFIGURATION ET CONSTANTES
# ==============================================================================

# Param√®tres sp√©cifiques au Keno
KENO_PARAMS = {
    'total_numbers': 70,        # Num√©ros de 1 √† 70
    'numbers_per_draw': 20,     # 20 num√©ros tir√©s par tirage
    'player_selection': 10,     # Le joueur s√©lectionne typiquement 10 num√©ros
    'min_selection': 7,         # Minimum de num√©ros s√©lectionnables (modifi√©)
    'max_selection': 10,        # Maximum de num√©ros s√©lectionnables
}

# Chemins des fichiers
BASE_DIR = Path(__file__).parent
DATA_DIR = BASE_DIR / "keno_data"
MODELS_DIR = BASE_DIR.parent / "keno_models"
OUTPUT_DIR = BASE_DIR.parent / "keno_output"

# Configuration ML
ML_CONFIG = {
    'n_estimators': 100,
    'max_depth': 6,
    'learning_rate': 0.1,
    'random_state': 42,
    'n_jobs': -1,
    'objective': 'binary:logistic'
}

# Configuration de l'apprentissage incr√©mental
INCREMENTAL_CONFIG = {
    'performance_window': 50,      # Fen√™tre pour le calcul de performance
    'adaptation_threshold': 0.05,  # Seuil pour ajuster les poids
    'min_samples_update': 10,      # Minimum d'√©chantillons pour mise √† jour
    'max_history_size': 1000,      # Taille max de l'historique des performances
    'weight_decay': 0.95,          # Facteur d'oubli pour les anciennes performances
}

def get_training_params(profile: str) -> dict:
    """
    Retourne les param√®tres d'entra√Ænement selon le profil s√©lectionn√©
    
    Args:
        profile: Profil d'entra√Ænement ('quick', 'balanced', 'comprehensive', 'intensive')
    
    Returns:
        dict: Param√®tres d'entra√Ænement pour RandomForest
    """
    if profile == "quick":
        return {
            'n_estimators': 50,        # Arbres r√©duits pour la vitesse
            'max_depth': 8,            # Profondeur mod√©r√©e
            'min_samples_split': 10,   # √âviter l'overfitting
            'min_samples_leaf': 5,     # √âviter l'overfitting
            'max_features': 'sqrt',    # Features r√©duites
            'random_state': 42,
            'n_jobs': -1,
            'class_weight': 'balanced'
        }
    elif profile == "balanced":
        return {
            'n_estimators': 100,       # Standard
            'max_depth': 12,           # Profondeur √©quilibr√©e
            'min_samples_split': 5,    # Standard
            'min_samples_leaf': 3,     # Standard
            'max_features': 'sqrt',    # Standard
            'random_state': 42,
            'n_jobs': -1,
            'class_weight': 'balanced'
        }
    elif profile == "comprehensive":
        return {
            'n_estimators': 200,       # Plus d'arbres pour meilleure pr√©cision
            'max_depth': 15,           # Profondeur √©lev√©e
            'min_samples_split': 4,    # Plus de splits
            'min_samples_leaf': 2,     # Feuilles plus petites
            'max_features': 'log2',    # Plus de features
            'random_state': 42,
            'n_jobs': -1,
            'class_weight': 'balanced'
        }
    elif profile == "intensive":
        return {
            'n_estimators': 300,       # Maximum d'arbres
            'max_depth': 20,           # Profondeur maximale
            'min_samples_split': 3,    # Splits agressifs
            'min_samples_leaf': 1,     # Feuilles minimales
            'max_features': None,      # Toutes les features
            'random_state': 42,
            'n_jobs': -1,
            'class_weight': 'balanced'
        }
    else:
        # Fallback sur balanced
        return get_training_params("balanced")

@dataclass
class LearningPerformance:
    """Structure pour stocker les performances d'apprentissage"""
    timestamp: datetime
    prediction_accuracy: float
    top30_hit_rate: float
    ml_weight_used: float
    freq_weight_used: float
    model_version: str
    sample_size: int
    feedback_score: float = 0.0  # Score bas√© sur les r√©sultats r√©els

@dataclass
class IncrementalLearningState:
    """√âtat de l'apprentissage incr√©mental"""
    performance_history: deque          # Historique des performances
    current_weights: Dict[str, float]   # Poids actuels (ml_weight, freq_weight)
    total_predictions: int              # Nombre total de pr√©dictions
    successful_predictions: int         # Pr√©dictions r√©ussies
    model_version: int                  # Version actuelle du mod√®le
    last_update: datetime               # Derni√®re mise √† jour
    adaptation_rate: float              # Taux d'adaptation actuel
    learning_momentum: float            # Momentum d'apprentissage

@dataclass
class KenoStats:
    """Structure pour stocker les statistiques Keno compl√®tes"""
    # 1. Fr√©quences d'apparition des num√©ros
    frequences: Dict[int, int]                           # Fr√©quence globale
    frequences_recentes: Dict[int, int]                  # Fr√©quence sur 100 derniers tirages
    frequences_50: Dict[int, int]                        # Fr√©quence sur 50 derniers tirages
    frequences_20: Dict[int, int]                        # Fr√©quence sur 20 derniers tirages
    
    # 2. Num√©ros en retard (overdue numbers)
    retards: Dict[int, int]                              # Retard actuel de chaque num√©ro
    retards_historiques: Dict[int, List[int]]            # Historique des retards
    
    # 3. Combinaisons et patterns r√©currents
    paires_freq: Dict[Tuple[int, int], int]              # Paires fr√©quentes
    trios_freq: Dict[Tuple[int, int, int], int]          # Trios fr√©quents
    patterns_parit√©: Dict[str, int]                      # Distribution pair/impair
    patterns_sommes: Dict[int, int]                      # Distribution des sommes
    patterns_zones: Dict[str, int]                       # R√©partition par zones/dizaines
    
    # 4. Analyse par p√©riode
    tendances_10: Dict[int, float]                       # Tendances sur 10 tirages
    tendances_50: Dict[int, float]                       # Tendances sur 50 tirages  
    tendances_100: Dict[int, float]                      # Tendances sur 100 tirages
    
    # Donn√©es brutes pour analyses avanc√©es
    zones_freq: Dict[str, int]                           # Ancien format maintenu pour compatibilit√©
    derniers_tirages: List[List[int]]                    # Derniers tirages
    tous_tirages: List[List[int]]                        # Tous les tirages pour DuckDB

# ==============================================================================
# üéØ CLASSE PRINCIPALE GENERATEUR KENO
# ==============================================================================

class KenoGeneratorAdvanced:
    """G√©n√©rateur avanc√© de grilles Keno avec ML et analyse statistique"""
    
    def __init__(self, data_path: str = None, silent: bool = False, training_profile: str = "balanced", grid_size: int = 10):
        """
        Initialise le g√©n√©rateur Keno
        
        Args:
            data_path: Chemin vers les donn√©es historiques
            silent: Mode silencieux pour r√©duire les sorties
            training_profile: Profil d'entra√Ænement ('quick', 'balanced', 'comprehensive', 'intensive')
            grid_size: Taille des grilles (7 √† 10 num√©ros)
        """
        self.silent = silent
        self.data_path = data_path or str(DATA_DIR / "keno_202010.parquet")
        self.models_dir = MODELS_DIR
        self.output_dir = OUTPUT_DIR
        self.training_profile = training_profile
        
        # Validation de la taille de grille
        if not (7 <= grid_size <= 10):
            self._log(f"‚ö†Ô∏è  Taille de grille invalide ({grid_size}). Utilisation de 10 par d√©faut.", "ERROR")
            grid_size = 10
        self.grid_size = grid_size
        
        # Cr√©er les r√©pertoires n√©cessaires
        self.models_dir.mkdir(exist_ok=True)
        self.output_dir.mkdir(exist_ok=True)
        
        # Initialisation des composants
        self.data = None
        self.stats = None
        self.ml_models = {}
        self.metadata = {}
        self.cache = {}
        
        # Strat√©gie adaptative avec apprentissage incr√©mental
        self.adaptive_weights = {
            'ml_weight': 0.6,      # 60% ML par d√©faut
            'freq_weight': 0.4,    # 40% fr√©quence par d√©faut  
            'performance_history': [],
            'last_update': datetime.now()
        }
        
        # √âtat d'apprentissage incr√©mental
        self.incremental_state = IncrementalLearningState(
            performance_history=deque(maxlen=INCREMENTAL_CONFIG['max_history_size']),
            current_weights={'ml_weight': 0.6, 'freq_weight': 0.4},
            total_predictions=0,
            successful_predictions=0,
            model_version=1,
            last_update=datetime.now(),
            adaptation_rate=0.1,
            learning_momentum=0.9
        )
        
        # Chargement de l'√©tat pr√©c√©dent s'il existe
        self._load_incremental_state()
        
        self._log(f"üé≤ G√©n√©rateur Keno Avanc√© v2.0 initialis√© (grilles de {self.grid_size} num√©ros)")

    def _log(self, message: str, level: str = "INFO"):
        """Syst√®me de logging configur√©"""
        if not self.silent or level == "ERROR":
            print(f"{message}")
    
    def _load_incremental_state(self):
        """Charge l'√©tat d'apprentissage incr√©mental depuis le disque"""
        state_file = self.models_dir / "incremental_state.pkl"
        if state_file.exists():
            try:
                with open(state_file, 'rb') as f:
                    saved_state = pickle.load(f)
                    # Mise √† jour de l'√©tat actuel
                    if 'current_weights' in saved_state:
                        self.incremental_state.current_weights = saved_state['current_weights']
                        self.adaptive_weights['ml_weight'] = saved_state['current_weights']['ml_weight']
                        self.adaptive_weights['freq_weight'] = saved_state['current_weights']['freq_weight']
                    if 'performance_history' in saved_state:
                        self.incremental_state.performance_history = deque(
                            saved_state['performance_history'],
                            maxlen=INCREMENTAL_CONFIG['max_history_size']
                        )
                    if 'model_version' in saved_state:
                        self.incremental_state.model_version = saved_state['model_version']
                    if 'total_predictions' in saved_state:
                        self.incremental_state.total_predictions = saved_state['total_predictions']
                    if 'successful_predictions' in saved_state:
                        self.incremental_state.successful_predictions = saved_state['successful_predictions']
                        
                self._log(f"‚úÖ √âtat d'apprentissage incr√©mental charg√© (version {self.incremental_state.model_version})")
                self._log(f"   üìä Performance: {self.incremental_state.successful_predictions}/{self.incremental_state.total_predictions}")
                self._log(f"   ‚öñÔ∏è  Poids: ML={self.incremental_state.current_weights['ml_weight']:.2f}, Freq={self.incremental_state.current_weights['freq_weight']:.2f}")
            except Exception as e:
                self._log(f"‚ö†Ô∏è  Erreur lors du chargement de l'√©tat incr√©mental: {e}")
    
    def _save_incremental_state(self):
        """Sauvegarde l'√©tat d'apprentissage incr√©mental"""
        state_file = self.models_dir / "incremental_state.pkl"
        try:
            state_data = {
                'current_weights': self.incremental_state.current_weights,
                'performance_history': list(self.incremental_state.performance_history),
                'model_version': self.incremental_state.model_version,
                'total_predictions': self.incremental_state.total_predictions,
                'successful_predictions': self.incremental_state.successful_predictions,
                'last_update': self.incremental_state.last_update,
                'adaptation_rate': self.incremental_state.adaptation_rate,
                'learning_momentum': self.incremental_state.learning_momentum
            }
            
            with open(state_file, 'wb') as f:
                pickle.dump(state_data, f)
                
            self._log(f"üíæ √âtat d'apprentissage sauvegard√© (version {self.incremental_state.model_version})")
        except Exception as e:
            self._log(f"‚ùå Erreur lors de la sauvegarde de l'√©tat: {e}", "ERROR")
    
    def update_performance(self, prediction_accuracy: float, top30_hit_rate: float, 
                          feedback_score: float = 0.0, sample_size: int = 1):
        """
        Met √† jour les performances et ajuste les poids adaptatifs
        
        Args:
            prediction_accuracy: Pr√©cision de la pr√©diction (0-1)
            top30_hit_rate: Taux de r√©ussite du TOP 30 (0-1) 
            feedback_score: Score de retour d'exp√©rience (0-1)
            sample_size: Taille de l'√©chantillon √©valu√©
        """
        # Cr√©er l'enregistrement de performance
        performance = LearningPerformance(
            timestamp=datetime.now(),
            prediction_accuracy=prediction_accuracy,
            top30_hit_rate=top30_hit_rate,
            ml_weight_used=self.incremental_state.current_weights['ml_weight'],
            freq_weight_used=self.incremental_state.current_weights['freq_weight'],
            model_version=str(self.incremental_state.model_version),
            sample_size=sample_size,
            feedback_score=feedback_score
        )
        
        # Ajouter √† l'historique
        self.incremental_state.performance_history.append(performance)
        
        # Mettre √† jour les compteurs
        self.incremental_state.total_predictions += sample_size
        self.incremental_state.successful_predictions += int(prediction_accuracy * sample_size)
        
        # Calculer la performance moyenne r√©cente
        recent_performances = list(self.incremental_state.performance_history)[-INCREMENTAL_CONFIG['performance_window']:]
        if len(recent_performances) >= INCREMENTAL_CONFIG['min_samples_update']:
            avg_accuracy = np.mean([p.prediction_accuracy for p in recent_performances])
            avg_top30_rate = np.mean([p.top30_hit_rate for p in recent_performances])
            avg_feedback = np.mean([p.feedback_score for p in recent_performances if p.feedback_score > 0])
            
            # Score composite de performance
            composite_score = (avg_accuracy * 0.4 + avg_top30_rate * 0.4 + avg_feedback * 0.2)
            
            # Ajustement adaptatif des poids
            self._adapt_weights(composite_score, avg_accuracy, avg_top30_rate)
            
            self._log(f"üìä Mise √† jour performance: Accuracy={avg_accuracy:.3f}, TOP30={avg_top30_rate:.3f}, Composite={composite_score:.3f}")
        
        # Sauvegarde de l'√©tat
        self._save_incremental_state()
    
    def _adapt_weights(self, composite_score: float, ml_performance: float, freq_performance: float):
        """
        Ajuste adaptivement les poids ML vs Fr√©quence selon les performances
        
        Args:
            composite_score: Score composite de performance
            ml_performance: Performance du ML seul
            freq_performance: Performance des fr√©quences seules
        """
        # Performance historique moyenne pour comparaison
        if len(self.incremental_state.performance_history) > 1:
            historical_scores = [
                (p.prediction_accuracy * 0.4 + p.top30_hit_rate * 0.4 + p.feedback_score * 0.2)
                for p in list(self.incremental_state.performance_history)[:-INCREMENTAL_CONFIG['performance_window']]
                if p.feedback_score > 0
            ]
            historical_avg = np.mean(historical_scores) if historical_scores else 0.5
        else:
            historical_avg = 0.5
        
        # D√©terminer la direction d'ajustement
        performance_delta = composite_score - historical_avg
        
        if abs(performance_delta) > INCREMENTAL_CONFIG['adaptation_threshold']:
            # Ajustement bas√© sur la performance relative ML vs Freq
            if ml_performance > freq_performance:
                # ML performe mieux, augmenter son poids
                weight_adjustment = self.incremental_state.adaptation_rate * performance_delta
                new_ml_weight = min(0.8, max(0.2, self.incremental_state.current_weights['ml_weight'] + weight_adjustment))
            else:
                # Fr√©quences performent mieux, augmenter leur poids
                weight_adjustment = self.incremental_state.adaptation_rate * performance_delta
                new_ml_weight = min(0.8, max(0.2, self.incremental_state.current_weights['ml_weight'] - weight_adjustment))
            
            new_freq_weight = 1.0 - new_ml_weight
            
            # Application du momentum pour lisser les changements
            momentum_ml = (self.incremental_state.learning_momentum * self.incremental_state.current_weights['ml_weight'] + 
                          (1 - self.incremental_state.learning_momentum) * new_ml_weight)
            momentum_freq = 1.0 - momentum_ml
            
            # Mise √† jour des poids
            old_ml_weight = self.incremental_state.current_weights['ml_weight']
            self.incremental_state.current_weights['ml_weight'] = momentum_ml
            self.incremental_state.current_weights['freq_weight'] = momentum_freq
            self.adaptive_weights['ml_weight'] = momentum_ml
            self.adaptive_weights['freq_weight'] = momentum_freq
            
            self._log(f"üéØ Ajustement poids adaptatif: ML {old_ml_weight:.3f}‚Üí{momentum_ml:.3f}, Freq {1-old_ml_weight:.3f}‚Üí{momentum_freq:.3f}")
            self._log(f"   üìà Performance delta: {performance_delta:+.3f}, Score composite: {composite_score:.3f}")
    
    def add_feedback(self, predicted_numbers: List[int], actual_draw: List[int], 
                     prediction_timestamp: datetime = None):
        """
        Ajoute un feedback bas√© sur un tirage r√©el pour am√©liorer l'apprentissage
        
        Args:
            predicted_numbers: Num√©ros pr√©dits par le mod√®le
            actual_draw: Num√©ros r√©ellement tir√©s
            prediction_timestamp: Timestamp de la pr√©diction (optionnel)
        """
        if not predicted_numbers or not actual_draw:
            return
        
        # Calcul du score de feedback
        predicted_set = set(predicted_numbers[:30])  # TOP 30 pr√©dit
        actual_set = set(actual_draw)
        
        # M√©triques de succ√®s
        hit_count = len(predicted_set.intersection(actual_set))
        hit_rate = hit_count / len(actual_set)  # Sur les 20 num√©ros tir√©s
        precision = hit_count / len(predicted_set) if predicted_set else 0
        
        # Score composite de feedback (0-1)
        feedback_score = (hit_rate * 0.6 + precision * 0.4)
        
        # Mise √† jour des performances avec ce feedback
        self.update_performance(
            prediction_accuracy=hit_rate,
            top30_hit_rate=precision,
            feedback_score=feedback_score,
            sample_size=1
        )
        
        self._log(f"üì• Feedback ajout√©: {hit_count}/20 num√©ros trouv√©s, Score={feedback_score:.3f}")
        self._log(f"   üéØ Pr√©cision TOP30: {precision:.3f}, Taux de r√©ussite: {hit_rate:.3f}")
        
        # Si le feedback est suffisamment r√©cent et significatif, d√©clencher une mise √† jour incr√©mentale
        if feedback_score > 0.3 and len(self.incremental_state.performance_history) >= 5:
            self._trigger_incremental_update(predicted_numbers, actual_draw)
    
    def _trigger_incremental_update(self, predicted_numbers: List[int], actual_draw: List[int]):
        """
        D√©clenche une mise √† jour incr√©mentale du mod√®le bas√©e sur le feedback
        
        Args:
            predicted_numbers: Num√©ros pr√©dits
            actual_draw: Tirage r√©el
        """
        if not HAS_ML or 'multilabel' not in self.ml_models:
            return
            
        try:
            self._log("üîÑ D√©clenchement d'une mise √† jour incr√©mentale du mod√®le...")
            
            # Cr√©er un √©chantillon d'entra√Ænement √† partir du feedback
            current_date = datetime.now()
            feedback_data = {
                'date_de_tirage': [current_date],
                **{f'boule{i}': [actual_draw[i-1] if i-1 < len(actual_draw) else 0] for i in range(1, 21)}
            }
            
            df_feedback = pd.DataFrame(feedback_data)
            
            # Ajouter les m√™mes features que lors de l'entra√Ænement initial
            df_features = self.add_cyclic_features(df_feedback)
            df_features = self.enrich_features(df_features)
            
            # ... (suite de la pr√©paration des features comme dans train_xgboost_models)
            # Note: Cette partie n√©cessiterait une refactorisation pour √©viter la duplication de code
            
            # Pour l'instant, incr√©menter la version du mod√®le et marquer pour r√©entra√Ænement
            self.incremental_state.model_version += 1
            self.incremental_state.last_update = datetime.now()
            
            self._log(f"‚úÖ Mise √† jour incr√©mentale planifi√©e (version {self.incremental_state.model_version})")
            
        except Exception as e:
            self._log(f"‚ùå Erreur lors de la mise √† jour incr√©mentale: {e}", "ERROR")
    
    def get_learning_report(self) -> str:
        """
        G√©n√®re un rapport d√©taill√© sur l'√©tat de l'apprentissage
        
        Returns:
            str: Rapport format√©
        """
        report = "# üìä Rapport d'Apprentissage Incr√©mental Keno\n\n"
        
        # Statistiques g√©n√©rales
        report += "## üìà Statistiques G√©n√©rales\n"
        report += f"- **Version du mod√®le**: {self.incremental_state.model_version}\n"
        report += f"- **Pr√©dictions totales**: {self.incremental_state.total_predictions}\n"
        report += f"- **Pr√©dictions r√©ussies**: {self.incremental_state.successful_predictions}\n"
        
        if self.incremental_state.total_predictions > 0:
            success_rate = self.incremental_state.successful_predictions / self.incremental_state.total_predictions
            report += f"- **Taux de succ√®s global**: {success_rate:.1%}\n"
        
        report += f"- **Derni√®re mise √† jour**: {self.incremental_state.last_update.strftime('%Y-%m-%d %H:%M:%S')}\n\n"
        
        # Poids actuels
        report += "## ‚öñÔ∏è Poids Adaptatifs Actuels\n"
        report += f"- **ML Weight**: {self.incremental_state.current_weights['ml_weight']:.3f}\n"
        report += f"- **Frequency Weight**: {self.incremental_state.current_weights['freq_weight']:.3f}\n"
        report += f"- **Taux d'adaptation**: {self.incremental_state.adaptation_rate:.3f}\n"
        report += f"- **Momentum d'apprentissage**: {self.incremental_state.learning_momentum:.3f}\n\n"
        
        # Performances r√©centes
        if self.incremental_state.performance_history:
            recent_performances = list(self.incremental_state.performance_history)[-10:]
            report += "## üìä Performances R√©centes (10 derni√®res)\n"
            
            for i, perf in enumerate(recent_performances, 1):
                report += f"**{i}.** {perf.timestamp.strftime('%Y-%m-%d %H:%M')} - "
                report += f"Acc: {perf.prediction_accuracy:.3f}, "
                report += f"TOP30: {perf.top30_hit_rate:.3f}, "
                report += f"Feedback: {perf.feedback_score:.3f}\n"
            
            # Moyennes r√©centes
            avg_acc = np.mean([p.prediction_accuracy for p in recent_performances])
            avg_top30 = np.mean([p.top30_hit_rate for p in recent_performances])
            avg_feedback = np.mean([p.feedback_score for p in recent_performances if p.feedback_score > 0])
            
            report += f"\n**Moyennes r√©centes:**\n"
            report += f"- Accuracy: {avg_acc:.3f}\n"
            report += f"- TOP30 Hit Rate: {avg_top30:.3f}\n"
            report += f"- Feedback Score: {avg_feedback:.3f}\n\n"
        
        # Recommandations
        report += "## üéØ Recommandations\n"
        if self.incremental_state.total_predictions < 50:
            report += "- ‚ö†Ô∏è Donn√©es insuffisantes pour des recommandations fiables\n"
            report += "- üìù Continuez √† utiliser le syst√®me pour collecter plus de donn√©es\n"
        else:
            if len(self.incremental_state.performance_history) >= 20:
                recent_trend = np.polyfit(
                    range(len(recent_performances)), 
                    [p.prediction_accuracy for p in recent_performances], 
                    1
                )[0]
                if recent_trend > 0.01:
                    report += "- ‚úÖ Tendance d'am√©lioration d√©tect√©e - Le mod√®le apprend efficacement\n"
                elif recent_trend < -0.01:
                    report += "- ‚ö†Ô∏è Tendance de d√©gradation - Consid√©rer un r√©entra√Ænement complet\n"
                else:
                    report += "- üìà Performance stable - Le mod√®le converge\n"
        
        return report
    
    def _ensure_zone_freq_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Ajoute les colonnes zoneX_freq manquantes pour compatibilit√© avec le mod√®le ML.
        """
        for freq_col in ['zone1_freq', 'zone2_freq', 'zone3_freq', 'zone4_freq']:
            if freq_col not in df.columns:
                df[freq_col] = 0
        return df

    
    def load_data(self) -> bool:
        """
        Charge les donn√©es historiques des tirages Keno
        
        Returns:
            bool: True si les donn√©es sont charg√©es avec succ√®s
        """
        try:
            self._log("üìä Chargement des donn√©es historiques Keno...")
            
            if not Path(self.data_path).exists():
                self._log(f"‚ùå Fichier non trouv√©: {self.data_path}", "ERROR")
                return False
                
            # Chargement depuis Parquet
            self.data = pd.read_parquet(self.data_path)
            
            # Validation des colonnes requises
            required_cols = ['date_de_tirage'] + [f'boule{i}' for i in range(1, 21)]
            missing_cols = [col for col in required_cols if col not in self.data.columns]
            
            if missing_cols:
                self._log(f"‚ùå Colonnes manquantes: {missing_cols}", "ERROR")
                return False
            
            # Tri par date
            self.data = self.data.sort_values('date_de_tirage').reset_index(drop=True)
            
            self._log(f"‚úÖ {len(self.data)} tirages Keno charg√©s ({self.data['date_de_tirage'].min()} √† {self.data['date_de_tirage'].max()})")
            return True
            
        except Exception as e:
            self._log(f"‚ùå Erreur lors du chargement des donn√©es: {e}", "ERROR")
            return False
    
    def analyze_patterns(self) -> KenoStats:
        """
        Analyse compl√®te des patterns et statistiques Keno avec DuckDB
        
        Returns:
            KenoStats: Statistiques compl√®tes calcul√©es
        """
        self._log("üîç Analyse compl√®te des patterns Keno avec DuckDB...")
        
        # Extraction des num√©ros de tous les tirages
        all_draws = []
        for _, row in self.data.iterrows():
            # Support des diff√©rents formats de colonnes
            if 'b1' in self.data.columns:
                draw = [int(row[f'boule{i}']) for i in range(1, 21)]
            else:
                draw = [int(row[f'boule{i}']) for i in range(1, 21)]
            all_draws.append(sorted(draw))
        
        self._log(f"üìä Analyse de {len(all_draws)} tirages")
        
        if HAS_DUCKDB:
            return self._analyze_with_duckdb(all_draws)
        else:
            return self._analyze_with_pandas(all_draws)
    
    def _analyze_with_duckdb(self, all_draws: List[List[int]]) -> KenoStats:
        """Analyse optimis√©e avec DuckDB"""
        self._log("üöÄ Utilisation de DuckDB pour l'analyse optimis√©e")
        
        # Cr√©er une connexion DuckDB
        conn = duckdb.connect(':memory:')
        
        # Pr√©parer les donn√©es pour DuckDB
        draws_data = []
        for i, draw in enumerate(all_draws):
            for num in draw:
                draws_data.append({'tirage_id': i, 'numero': num, 'position': draw.index(num)})
        
        draws_df = pd.DataFrame(draws_data)
        
        # Cr√©er la table dans DuckDB
        conn.register('tirages', draws_df)
        
        # 1. FR√âQUENCES D'APPARITION
        self._log("üìä Calcul des fr√©quences d'apparition...")
        
        # Fr√©quence globale
        freq_global = conn.execute("""
            SELECT numero, COUNT(*) as freq 
            FROM tirages 
            GROUP BY numero 
            ORDER BY numero
        """).fetchdf()
        frequences = dict(zip(freq_global['numero'], freq_global['freq']))
        
        # Fr√©quences r√©centes (100, 50, 20 derniers tirages)
        max_tirage = len(all_draws) - 1
        
        freq_100 = conn.execute(f"""
            SELECT numero, COUNT(*) as freq 
            FROM tirages 
            WHERE tirage_id >= {max(0, max_tirage - 99)}
            GROUP BY numero
        """).fetchdf()
        frequences_100 = dict(zip(freq_100['numero'], freq_100['freq']))
        
        freq_50 = conn.execute(f"""
            SELECT numero, COUNT(*) as freq 
            FROM tirages 
            WHERE tirage_id >= {max(0, max_tirage - 49)}
            GROUP BY numero
        """).fetchdf()
        frequences_50 = dict(zip(freq_50['numero'], freq_50['freq']))
        
        freq_20 = conn.execute(f"""
            SELECT numero, COUNT(*) as freq 
            FROM tirages 
            WHERE tirage_id >= {max(0, max_tirage - 19)}
            GROUP BY numero
        """).fetchdf()
        frequences_20 = dict(zip(freq_20['numero'], freq_20['freq']))
        
        # 2. CALCUL DES RETARDS
        self._log("‚è∞ Calcul des retards des num√©ros...")
        retards = {}
        retards_historiques = {}
        
        for num in range(1, KENO_PARAMS['total_numbers'] + 1):
            # Retard actuel
            retard = 0
            for draw in reversed(all_draws):
                if num in draw:
                    break
                retard += 1
            retards[num] = retard
            
            # Historique des retards
            historique = []
            dernier_tirage = -1
            for i, draw in enumerate(all_draws):
                if num in draw:
                    if dernier_tirage >= 0:
                        historique.append(i - dernier_tirage - 1)
                    dernier_tirage = i
            retards_historiques[num] = historique
        
        # 3. PATTERNS ET COMBINAISONS
        self._log("üîó Analyse des patterns et combinaisons...")
        
        # Paires fr√©quentes
        paires_freq = {}
        for draw in all_draws:
            for i in range(len(draw)):
                for j in range(i + 1, len(draw)):
                    pair = tuple(sorted([draw[i], draw[j]]))
                    paires_freq[pair] = paires_freq.get(pair, 0) + 1
        
        # Trios fr√©quents (limit√© aux plus fr√©quents pour performance)
        trios_freq = {}
        for draw in all_draws[-1000:]:  # Seulement les 1000 derniers pour performance
            for i in range(len(draw)):
                for j in range(i + 1, len(draw)):
                    for k in range(j + 1, len(draw)):
                        trio = tuple(sorted([draw[i], draw[j], draw[k]]))
                        trios_freq[trio] = trios_freq.get(trio, 0) + 1
        
        # Patterns de parit√©
        patterns_parit√© = {"tout_pair": 0, "tout_impair": 0, "mixte": 0}
        for draw in all_draws:
            pairs = sum(1 for num in draw if num % 2 == 0)
            if pairs == len(draw):
                patterns_parit√©["tout_pair"] += 1
            elif pairs == 0:
                patterns_parit√©["tout_impair"] += 1
            else:
                patterns_parit√©["mixte"] += 1
        
        # Patterns de sommes
        patterns_sommes = {}
        for draw in all_draws:
            somme = sum(draw)
            patterns_sommes[somme] = patterns_sommes.get(somme, 0) + 1
        
        # Patterns de zones/dizaines
        patterns_zones = {
            "zone_1_17": 0, "zone_18_35": 0, "zone_36_52": 0, "zone_53_70": 0,
            "dizaine_1_10": 0, "dizaine_11_20": 0, "dizaine_21_30": 0, 
            "dizaine_31_40": 0, "dizaine_41_50": 0, "dizaine_51_60": 0, "dizaine_61_70": 0
        }
        
        for draw in all_draws:
            # Comptage par zones
            for num in draw:
                if 1 <= num <= 17:
                    patterns_zones["zone_1_17"] += 1
                elif 18 <= num <= 35:
                    patterns_zones["zone_18_35"] += 1
                elif 36 <= num <= 52:
                    patterns_zones["zone_36_52"] += 1
                else:  # 53-70
                    patterns_zones["zone_53_70"] += 1
                
                # Comptage par dizaines
                dizaine = (num - 1) // 10 + 1
                if dizaine <= 7:
                    patterns_zones[f"dizaine_{(dizaine-1)*10+1}_{dizaine*10}"] += 1
        
        # 4. ANALYSE PAR P√âRIODE ET TENDANCES
        self._log("üìà Calcul des tendances par p√©riode...")
        
        tendances_10 = self._calculer_tendances(all_draws, 10)
        tendances_50 = self._calculer_tendances(all_draws, 50)
        tendances_100 = self._calculer_tendances(all_draws, 100)
        
        # Zones compatibilit√© (ancien format)
        zones_freq = {
            "zone1_17": patterns_zones["zone_1_17"],
            "zone18_35": patterns_zones["zone_18_35"], 
            "zone36_52": patterns_zones["zone_36_52"],
            "zone53_70": patterns_zones["zone_53_70"]
        }
        
        # Garder les derniers tirages
        derniers_tirages = all_draws[-50:] if len(all_draws) >= 50 else all_draws
        
        conn.close()
        
        # Initialiser avec des valeurs par d√©faut pour les champs manquants
        for num in range(1, KENO_PARAMS['total_numbers'] + 1):
            if num not in frequences:
                frequences[num] = 0
            if num not in frequences_100:
                frequences_100[num] = 0
            if num not in frequences_50:
                frequences_50[num] = 0
            if num not in frequences_20:
                frequences_20[num] = 0
        
        self.stats = KenoStats(
            frequences=frequences,
            frequences_recentes=frequences_100,
            frequences_50=frequences_50,
            frequences_20=frequences_20,
            retards=retards,
            retards_historiques=retards_historiques,
            paires_freq=paires_freq,
            trios_freq=trios_freq,
            patterns_parit√©=patterns_parit√©,
            patterns_sommes=patterns_sommes,
            patterns_zones=patterns_zones,
            tendances_10=tendances_10,
            tendances_50=tendances_50,
            tendances_100=tendances_100,
            zones_freq=zones_freq,
            derniers_tirages=derniers_tirages,
            tous_tirages=all_draws
        )
        
        self._log(f"‚úÖ Analyse DuckDB termin√©e - {len(all_draws)} tirages analys√©s")
        return self.stats
    
    def _analyze_with_pandas(self, all_draws: List[List[int]]) -> KenoStats:
        """Analyse de fallback avec Pandas seulement"""
        self._log("‚ö†Ô∏è  Analyse de fallback avec Pandas (DuckDB non disponible)")
        
        # Impl√©mentation simplifi√©e pour compatibilit√©
        frequences = {i: 0 for i in range(1, KENO_PARAMS['total_numbers'] + 1)}
        retards = {i: 0 for i in range(1, KENO_PARAMS['total_numbers'] + 1)}
        paires_freq = {}
        zones_freq = {"zone1_17": 0, "zone18_35": 0, "zone36_52": 0, "zone53_70": 0}
        
        # Comptage basique des fr√©quences
        for draw in all_draws:
            for num in draw:
                frequences[num] += 1
                
                # Zones
                if 1 <= num <= 17:
                    zones_freq["zone1_17"] += 1
                elif 18 <= num <= 35:
                    zones_freq["zone18_35"] += 1
                elif 36 <= num <= 52:
                    zones_freq["zone36_52"] += 1
                else:
                    zones_freq["zone53_70"] += 1
        
        # Calcul des retards
        for num in range(1, KENO_PARAMS['total_numbers'] + 1):
            retard = 0
            for draw in reversed(all_draws):
                if num in draw:
                    break
                retard += 1
            retards[num] = retard
        
        # Paires basiques
        for draw in all_draws:
            for i in range(len(draw)):
                for j in range(i + 1, len(draw)):
                    pair = tuple(sorted([draw[i], draw[j]]))
                    paires_freq[pair] = paires_freq.get(pair, 0) + 1
        
        derniers_tirages = all_draws[-50:] if len(all_draws) >= 50 else all_draws
        
        # Initialiser avec des valeurs par d√©faut
        self.stats = KenoStats(
            frequences=frequences,
            frequences_recentes=frequences.copy(),
            frequences_50=frequences.copy(),
            frequences_20=frequences.copy(),
            retards=retards,
            retards_historiques={i: [] for i in range(1, 71)},
            paires_freq=paires_freq,
            trios_freq={},
            patterns_parit√©={"tout_pair": 0, "tout_impair": 0, "mixte": len(all_draws)},
            patterns_sommes={},
            patterns_zones={},
            tendances_10={i: 0.0 for i in range(1, 71)},
            tendances_50={i: 0.0 for i in range(1, 71)},
            tendances_100={i: 0.0 for i in range(1, 71)},
            zones_freq=zones_freq,
            derniers_tirages=derniers_tirages,
            tous_tirages=all_draws
        )
        
        self._log(f"‚úÖ Analyse Pandas termin√©e - {len(all_draws)} tirages analys√©s")
        return self.stats
    
    def _calculer_tendances(self, all_draws: List[List[int]], periode: int) -> Dict[int, float]:
        """Calcule les tendances d'apparition sur une p√©riode donn√©e"""
        if len(all_draws) < periode:
            return {i: 0.0 for i in range(1, KENO_PARAMS['total_numbers'] + 1)}
        
        tendances = {}
        for num in range(1, KENO_PARAMS['total_numbers'] + 1):
            # Fr√©quence r√©cente vs fr√©quence historique
            recent_draws = all_draws[-periode:]
            freq_recent = sum(1 for draw in recent_draws if num in draw)
            freq_moyenne = freq_recent / periode
            
            # Fr√©quence historique
            freq_historique = sum(1 for draw in all_draws[:-periode] if num in draw)
            if len(all_draws) > periode:
                freq_historique_moyenne = freq_historique / (len(all_draws) - periode)
            else:
                freq_historique_moyenne = freq_moyenne
            
            # Tendance = ratio r√©cent/historique
            if freq_historique_moyenne > 0:
                tendances[num] = freq_moyenne / freq_historique_moyenne
            else:
                tendances[num] = freq_moyenne * 2  # Bonus si nouveau num√©ro
        
        return tendances
    
    def add_cyclic_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Ajoute des features cycliques pour l'apprentissage automatique
        
        Args:
            df: DataFrame avec les donn√©es
            
        Returns:
            DataFrame avec les features ajout√©es
        """
        df = df.copy()
        
        # Features temporelles cycliques
        df['day_sin'] = np.sin(2 * np.pi * df['date_de_tirage'].dt.day / 31)
        df['day_cos'] = np.cos(2 * np.pi * df['date_de_tirage'].dt.day / 31)
        df['month_sin'] = np.sin(2 * np.pi * df['date_de_tirage'].dt.month / 12)  
        df['month_cos'] = np.cos(2 * np.pi * df['date_de_tirage'].dt.month / 12)
        
        # Features cycliques pour chaque num√©ro (1-70 pour Keno)
        for i in range(1, 11):  # Pour les 10 premi√®res boules principales
            if f'boule{i}' in df.columns:
                df[f'boule{i}_sin'] = np.sin(2 * np.pi * df[f'boule{i}'] / KENO_PARAMS['total_numbers'])
                df[f'boule{i}_cos'] = np.cos(2 * np.pi * df[f'boule{i}'] / KENO_PARAMS['total_numbers'])
        
        return df
    
    def enrich_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Ajoute des features avanc√©es pour le ML : s√©quences, clusters, dispersion, stats paires/trios, features cycliques, etc.
        """
        df = df.copy()
        # Features cycliques d√©j√† pr√©sentes
        # Ajout de la dispersion (√©cart-type des num√©ros tir√©s)
        df['dispersion'] = df[[f'boule{i}' for i in range(1, 21)]].std(axis=1)
        # Ajout du nombre de paires et trios fr√©quents dans le tirage
        if hasattr(self, 'stats') and self.stats:
            def count_pairs(row):
                nums = [int(row[f'boule{i}']) for i in range(1, 21)]
                return sum(1 for i in range(len(nums)) for j in range(i+1, len(nums)) if tuple(sorted([nums[i], nums[j]])) in self.stats.paires_freq)
            def count_trios(row):
                nums = [int(row[f'boule{i}']) for i in range(1, 21)]
                return sum(1 for i in range(len(nums)) for j in range(i+1, len(nums)) for k in range(j+1, len(nums)) if tuple(sorted([nums[i], nums[j], nums[k]])) in self.stats.trios_freq)
            df['nb_paires_freq'] = df.apply(count_pairs, axis=1)
            df['nb_trios_freq'] = df.apply(count_trios, axis=1)
        else:
            df['nb_paires_freq'] = 0
            df['nb_trios_freq'] = 0
        # Ajout de la somme des num√©ros tir√©s
        df['somme_tirage'] = df[[f'boule{i}' for i in range(1, 21)]].sum(axis=1)
        # Ajout de la parit√© (nombre de pairs)
        df['nb_pairs'] = df[[f'boule{i}' for i in range(1, 21)]].apply(lambda x: sum(1 for n in x if n % 2 == 0), axis=1)
        # Ajout de la feature cluster (nombre de groupes de num√©ros cons√©cutifs)
        def count_clusters(row):
            nums = sorted([int(row[f'boule{i}']) for i in range(1, 21)])
            clusters = 1
            for i in range(1, len(nums)):
                if nums[i] - nums[i-1] > 1:
                    clusters += 1
            return clusters
        df['nb_clusters'] = df.apply(count_clusters, axis=1)
        return df
    
    def train_xgboost_models(self, retrain: bool = False) -> bool:
        """
        Entra√Æne un mod√®le XGBoost multi-label pour pr√©dire les num√©ros Keno
        
        Args:
            retrain: Force le r√©entra√Ænement m√™me si le mod√®le existe
            
        Returns:
            bool: True si l'entra√Ænement est r√©ussi
        """
        if not HAS_ML:
            self._log("‚ùå Modules ML non disponibles", "ERROR")
            return False
        
        # V√©rification du mod√®le existant (un seul mod√®le multi-label)
        model_file = self.models_dir / "xgb_keno_multilabel.pkl"
        if not retrain and model_file.exists():
            self._log("‚úÖ Mod√®le XGBoost Keno multi-label existant trouv√©")
            return self.load_ml_models()
        
        self._log("ü§ñ Entra√Ænement du mod√®le XGBoost Keno multi-label...")
        self._log("   üìä Strat√©gie: 1 mod√®le multi-label pour apprendre les corr√©lations")
        
        try:
            # Pr√©paration des donn√©es avec features enrichies (optimis√© pour √©viter la fragmentation)
            df_features = self.add_cyclic_features(self.data)
            df_features = self.enrich_features(df_features)
            
            # Features temporelles
            feature_cols = ['day_sin', 'day_cos', 'month_sin', 'month_cos']
            
            # Pr√©paration des features d'historique avec pd.concat pour √©viter la fragmentation
            self._log("   üìù Cr√©ation des features d'historique...")
            lag_features = {}
            
            for lag in range(1, 6):
                for ball_num in range(1, 21):
                    col_name = f'lag{lag}_boule{ball_num}'
                    if lag < len(df_features):
                        lag_features[col_name] = df_features[f'boule{ball_num}'].shift(lag).fillna(0)
                    else:
                        lag_features[col_name] = pd.Series([0] * len(df_features))
                    feature_cols.append(col_name)
            
            # Ajout des features d'historique en une seule fois
            lag_df = pd.DataFrame(lag_features, index=df_features.index)
            df_features = pd.concat([df_features, lag_df], axis=1)
            
            # Features de fr√©quence par zone (calcul√©es efficacement)
            self._log("   üìù Calcul des features de zones...")
            zone_features = {
                'zone1_count': [],
                'zone2_count': [],
                'zone3_count': [],
                'zone4_count': [],
                'zone1_freq': [],
                'zone2_freq': [],
                'zone3_freq': [],
                'zone4_freq': []
            }
            
            for idx, row in df_features.iterrows():
                draw_numbers = [int(row[f'boule{i}']) for i in range(1, 21)]
                zone_features['zone1_count'].append(sum(1 for n in draw_numbers if 1 <= n <= 17))
                zone_features['zone2_count'].append(sum(1 for n in draw_numbers if 18 <= n <= 35))
                zone_features['zone3_count'].append(sum(1 for n in draw_numbers if 36 <= n <= 52))
                zone_features['zone4_count'].append(sum(1 for n in draw_numbers if 53 <= n <= 70))
            
            # Ajout des features de zones
            for zone_name, zone_values in zone_features.items():
                # Correction : si la liste est vide ou de mauvaise taille, remplis avec des z√©ros
                if len(zone_values) != len(df_features):
                    zone_values = [0] * len(df_features)
                df_features[zone_name] = zone_values
                feature_cols.append(zone_name)

            # AJOUT CRUCIAL: Features statistiques √©tendues (manquantes √† l'entra√Ænement)
            if self.stats:
                # Pr√©parer toutes les features statistiques en une fois avec un dictionnaire
                stats_features = {}
                for num in range(1, 71):
                    stats_features[f'freq_recent_{num}'] = [self.stats.frequences_recentes.get(num, 0)] * len(df_features)
                    stats_features[f'retard_{num}'] = [self.stats.retards.get(num, 0)] * len(df_features)
                    stats_features[f'tendance_{num}'] = [self.stats.tendances_50.get(num, 1.0)] * len(df_features)
                
                # Ajouter toutes ces features en une seule fois avec pd.concat
                stats_df = pd.DataFrame(stats_features, index=df_features.index)
                df_features = pd.concat([df_features, stats_df], axis=1)
                
                # Ajouter les noms des features
                for feature_name in stats_features.keys():
                    feature_cols.append(feature_name)
            else:
                # Cr√©er des valeurs par d√©faut pour toutes les features statistiques en une fois
                stats_features = {}
                for num in range(1, 71):
                    stats_features[f'freq_recent_{num}'] = [0] * len(df_features)
                    stats_features[f'retard_{num}'] = [0] * len(df_features)
                    stats_features[f'tendance_{num}'] = [1.0] * len(df_features)
                
                stats_df = pd.DataFrame(stats_features, index=df_features.index)
                df_features = pd.concat([df_features, stats_df], axis=1)
                
                for feature_name in stats_features.keys():
                    feature_cols.append(feature_name)
            
            X = df_features[feature_cols].fillna(0)
            
            # Cr√©ation du target multi-label (matrice 70 colonnes, une par num√©ro)
            y = np.zeros((len(df_features), KENO_PARAMS['total_numbers']))
            
            for idx, row in df_features.iterrows():
                draw_numbers = [int(row[f'boule{i}']) for i in range(1, 21)]
                for num in draw_numbers:
                    if 1 <= num <= KENO_PARAMS['total_numbers']:
                        y[idx, num - 1] = 1  # Index 0-69 pour num√©ros 1-70
            
            self._log(f"   üìù Donn√©es pr√©par√©es: {X.shape[0]} tirages, {X.shape[1]} features")
            self._log(f"   üìù Target multi-label: {y.shape[1]} num√©ros √† pr√©dire")
            
            # Split train/test
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42
            )
            
            # Configuration sp√©ciale pour multi-label
            ml_config = ML_CONFIG.copy()
            ml_config['objective'] = 'multi:logistic'
            ml_config['num_class'] = 2  # Binaire pour chaque label
            
            # Entra√Ænement du mod√®le multi-label avec RandomForest (meilleur pour les corr√©lations)
            from sklearn.ensemble import RandomForestClassifier
            from sklearn.multioutput import MultiOutputClassifier
            
            # Obtenir les param√®tres selon le profil d'entra√Ænement
            rf_params = get_training_params(self.training_profile)
            
            # Afficher le profil utilis√©
            profile_names = {
                'quick': 'Ultra-rapide',
                'balanced': '√âquilibr√©', 
                'comprehensive': 'Complet',
                'intensive': 'Intensif'
            }
            profile_name = profile_names.get(self.training_profile, '√âquilibr√©')
            self._log(f"üìä Profil d'entra√Ænement: {profile_name} ({self.training_profile})")
            self._log(f"   ‚Ä¢ Arbres: {rf_params['n_estimators']}")
            self._log(f"   ‚Ä¢ Profondeur max: {rf_params['max_depth']}")
            self._log(f"   ‚Ä¢ Features: {rf_params['max_features']}")
            
            # RandomForest configur√© selon le profil
            base_model = RandomForestClassifier(**rf_params)
            
            # Alternative: Essayer aussi XGBoost avec MultiOutputClassifier
            # base_model = xgb.XGBClassifier(
            #     objective='binary:logistic',
            #     n_estimators=150,
            #     max_depth=8,
            #     learning_rate=0.05,
            #     random_state=42,
            #     n_jobs=-1
            # )
            
            # Mod√®le multi-output
            model = MultiOutputClassifier(base_model, n_jobs=-1)
            
            self._log("   üîÑ Entra√Ænement du mod√®le RandomForest multi-label...")
            model.fit(X_train, y_train)
            
            # √âvaluation rapide
            y_pred = model.predict(X_test)
            
            # Calcul de l'accuracy moyenne sur tous les labels
            accuracies = []
            for i in range(y.shape[1]):
                acc = accuracy_score(y_test[:, i], y_pred[:, i])
                accuracies.append(acc)
            
            mean_accuracy = np.mean(accuracies)
            self._log(f"   üìä Accuracy moyenne: {mean_accuracy:.4f}")
            
            # Validation crois√©e
            from sklearn.model_selection import cross_val_score
            cv_scores = cross_val_score(base_model, X_train, y_train, cv=5, n_jobs=-1)
            self._log(f"   üìä Validation crois√©e (CV=5): Score moyen = {np.mean(cv_scores):.4f}")
            
            # Sauvegarde du mod√®le
            with open(model_file, 'wb') as f:
                pickle.dump(model, f)
            
            self.ml_models['multilabel'] = model
            
            # Sauvegarde des m√©tadonn√©es
            self.metadata = {
                'features_count': len(feature_cols),
                'model_type': 'xgboost_keno_multilabel',
                'created_date': datetime.now().strftime('%Y-%m-%d'),
                'version': '3.0',  # Version avec multi-label
                'keno_params': KENO_PARAMS,
                'mean_accuracy': mean_accuracy,
                'feature_names': feature_cols
            }
            
            metadata_path = self.models_dir / "metadata.json"
            with open(metadata_path, 'w') as f:
                json.dump(self.metadata, f, indent=2)
            
            self._log("‚úÖ Entra√Ænement du mod√®le multi-label Keno termin√©")
            return True
            
        except Exception as e:
            self._log(f"‚ùå Erreur lors de l'entra√Ænement: {e}", "ERROR")
            import traceback
            self._log(f"D√©tails: {traceback.format_exc()}")
            return False
    
    def load_ml_models(self) -> bool:
        """Charge le mod√®le ML multi-label pr√©-entra√Æn√©"""
        try:
            self._log("üì• Chargement du mod√®le ML Keno multi-label...")
            
            # Chargement des m√©tadonn√©es
            metadata_path = self.models_dir / "metadata.json"
            if metadata_path.exists():
                with open(metadata_path, 'r') as f:
                    self.metadata = json.load(f)
            
            # Chargement du mod√®le multi-label
            model_path = self.models_dir / "xgb_keno_multilabel.pkl"
            if model_path.exists():
                with open(model_path, 'rb') as f:
                    self.ml_models['multilabel'] = pickle.load(f)
                
                self._log("‚úÖ Mod√®le multi-label Keno charg√© avec succ√®s")
                if 'mean_accuracy' in self.metadata:
                    self._log(f"   üìä Accuracy du mod√®le: {self.metadata['mean_accuracy']:.4f}")
                return True
            else:
                self._log("‚ùå Mod√®le multi-label non trouv√©", "ERROR")
                return False
                
        except Exception as e:
            self._log(f"‚ùå Erreur lors du chargement du mod√®le: {e}", "ERROR")
            return False
    
    def predict_numbers_ml(self, num_grids: int = 10) -> List[List[int]]:
        """
        Pr√©dit les num√©ros avec le mod√®le ML multi-label
        
        Args:
            num_grids: Nombre de grilles √† g√©n√©rer
            
        Returns:
            List[List[int]]: Liste des grilles pr√©dites
        """
        if 'multilabel' not in self.ml_models:
            self._log("‚ùå Mod√®le ML multi-label non disponible", "ERROR")
            return []
        
        try:
            model = self.ml_models['multilabel']
            
            # Pr√©paration des features pour la pr√©diction
            current_date = pd.Timestamp.now()
            df_predict = pd.DataFrame({
                'date_de_tirage': [current_date],
                **{f'boule{i}': [0] for i in range(1, 21)}  # Valeurs dummy
            })
            
            # Ajout des features temporelles ET enrichies (comme √† l'entra√Ænement)
            df_features = self.add_cyclic_features(df_predict)
            df_features = self.enrich_features(df_features)
            
            # Features d'historique
            lag_features = {}
            if self.stats and self.stats.derniers_tirages:
                for lag in range(1, 6):
                    for ball_num in range(1, 21):
                        col_name = f'lag{lag}_boule{ball_num}'
                        if lag <= len(self.stats.derniers_tirages):
                            last_draw = self.stats.derniers_tirages[-(lag)]
                            lag_features[col_name] = last_draw[ball_num - 1] if ball_num <= len(last_draw) else 0
                        else:
                            lag_features[col_name] = 0
                lag_df = pd.DataFrame([lag_features], index=df_features.index)
            else:
                # Valeurs par d√©faut si pas d'historique
                lag_features = {f'lag{lag}_boule{ball_num}': 0 for lag in range(1, 6) for ball_num in range(1, 21)}
                lag_df = pd.DataFrame([lag_features], index=df_features.index)

            df_features = pd.concat([df_features, lag_df], axis=1)

            # Features de fr√©quence par zone
            zone_features = {
                'zone1_count': [0],
                'zone2_count': [0],
                'zone3_count': [0],
                'zone4_count': [0],
                'zone1_freq': [0],
                'zone2_freq': [0],
                'zone3_freq': [0],
                'zone4_freq': [0]
            }
            
            for zone_name, zone_values in zone_features.items():
                df_features[zone_name] = zone_values

            # AJOUT CRUCIAL: Features statistiques √©tendues pour TOUS les num√©ros (1-70)
            if self.stats:
                stats_data = {
                    f'freq_recent_{num}': self.stats.frequences_recentes.get(num, 0)
                    for num in range(1, 71)
                }
                stats_data.update({
                    f'retard_{num}': self.stats.retards.get(num, 0)
                    for num in range(1, 71)
                })
                stats_data.update({
                    f'tendance_{num}': self.stats.tendances_50.get(num, 1.0)
                    for num in range(1, 71)
                })
                stats_df = pd.DataFrame([stats_data], index=df_features.index)
                df_features = pd.concat([df_features, stats_df], axis=1)
            else:
                # Cr√©er des valeurs par d√©faut pour toutes les features statistiques
                stats_data = {}
                for num in range(1, 71):
                    stats_data[f'freq_recent_{num}'] = 0
                    stats_data[f'retard_{num}'] = 0
                    stats_data[f'tendance_{num}'] = 1.0
                stats_df = pd.DataFrame([stats_data], index=df_features.index)
                df_features = pd.concat([df_features, stats_df], axis=1)

            # Utiliser la liste de features de l'entra√Ænement
            feature_cols = self.metadata.get('feature_names', [])

            # CORRECTION D√âFINITIVE: Garantir exactement les m√™mes features
            for c in feature_cols:
                if c not in df_features.columns:
                    df_features[c] = 0.0

            # Supprimer les colonnes en trop et r√©ordonner exactement comme √† l'entra√Ænement
            df_features = df_features.loc[:, feature_cols]

            X_pred = df_features.fillna(0)
            X_pred = X_pred.select_dtypes(include=[np.number])

            # V√©rification du nombre de features
            if X_pred.shape[1] != len(feature_cols):
                self._log(f"‚ö†Ô∏è  Mismatch: {X_pred.shape[1]} features vs {len(feature_cols)} attendues", "ERROR")
                return []

            # Normalisation des features
            from sklearn.preprocessing import StandardScaler
            scaler = StandardScaler()
            X_pred_scaled = scaler.fit_transform(X_pred)

            # Pr√©diction des probabilit√©s pour tous les num√©ros
            probabilities = model.predict_proba(X_pred_scaled)

            number_probs = []
            for i in range(min(KENO_PARAMS['total_numbers'], len(probabilities))):
                num = i + 1
                if len(probabilities[i][0]) > 1:
                    prob = probabilities[i][0][1]
                else:
                    prob = 0.5
                number_probs.append((num, prob))

            number_probs.sort(key=lambda x: x[1], reverse=True)
            top30 = number_probs[:30]

            self._log(f"‚úÖ TOP 30 ML calcul√© - Probabilit√© moyenne: {np.mean([prob for _, prob in top30]):.4f}")
            return top30

        except Exception as e:
            self._log(f"‚ùå Erreur lors du calcul TOP 30 ML: {e}", "ERROR")
            import traceback
            self._log(f"D√©tails: {traceback.format_exc()}")
            return []

    def get_top30_numbers_advanced(self) -> list:
        """
        S√©lectionne les 30 meilleurs num√©ros selon un score composite avanc√©
        (fr√©quence globale, fr√©quence r√©cente, retard, tendance, score ML, paires/trios)
        """
        if not self.stats:
            self._log("Stats non disponibles, impossible de calculer le TOP 30.", "ERROR")
            return []
        scores = {}
        max_freq = max(self.stats.frequences.values()) if self.stats.frequences else 1
        max_freq_recent = max(self.stats.frequences_recentes.values()) if self.stats.frequences_recentes else 1
        max_retard = max(self.stats.retards.values()) if self.stats.retards else 1
        max_tendance = max(self.stats.tendances_50.values()) if self.stats.tendances_50 else 1.0

        # Pond√©rations dynamiques (exemple)
        ml_weight = 0.15
        freq_weight = 0.25
        recent_weight = 0.20
        retard_weight = 0.15
        tendance_weight = 0.15
        pair_weight = 0.05
        trio_weight = 0.05
        lift_weight = 0.10
        diversity_penalty = 0.10

        # Calcul du lift des paires
        def lift(i, j):
            cofreq = self.stats.paires_freq.get(tuple(sorted([i, j])), 0)
            freq_i = self.stats.frequences.get(i, 1)
            freq_j = self.stats.frequences.get(j, 1)
            return cofreq / (freq_i * freq_j) if freq_i and freq_j else 0

        for num in range(1, KENO_PARAMS['total_numbers'] + 1):
            score = 0.0
            score += (self.stats.frequences.get(num, 0) / max_freq) * freq_weight
            score += (self.stats.frequences_recentes.get(num, 0) / max_freq_recent) * recent_weight
            score += (1 - self.stats.retards.get(num, 0) / max_retard) * retard_weight
            score += (self.stats.tendances_50.get(num, 1.0) / max_tendance) * tendance_weight
            if hasattr(self, 'ml_scores') and self.ml_scores and num in self.ml_scores:
                score += self.ml_scores[num] * ml_weight

            # Bonus lift sur les paires
            lift_score = 0
            for other in range(1, 71):
                if other != num:
                    lift_score += lift(num, other)
            score += (lift_score / 69) * lift_weight

            # Bonus pour les paires/trios fr√©quents
            pair_bonus = sum([self.stats.paires_freq.get(tuple(sorted([num, other])), 0) for other in range(1, 71) if other != num])
            score += (pair_bonus / 1000) * pair_weight
            trio_bonus = sum([self.stats.trios_freq.get(tuple(sorted([num, other1, other2]),), 0)
                              for other1 in range(1, 71) for other2 in range(other1+1, 71)
                              if other1 != num and other2 != num])
            score += (trio_bonus / 5000) * trio_weight

            # Diversit√©‚ÄØ: p√©nalit√© si trop de num√©ros dans la m√™me dizaine ou m√™me parit√©
            dizaine = (num - 1) // 10
            parite = num % 2
            same_dizaine = sum(1 for other in range(1, 71) if other != num and (other - 1) // 10 == dizaine)
            same_parite = sum(1 for other in range(1, 71) if other != num and other % 2 == parite)
            score -= ((same_dizaine / 69) + (same_parite / 69)) * diversity_penalty

            scores[num] = score

        top30 = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:30]
        return [num for num, score in top30]
        
    def run_full_pipeline(self, num_grids: int = 40, profile: str = "balanced"):
        """
        Pipeline complet : chargement des donn√©es, analyse, entra√Ænement ML, g√©n√©ration des grilles.
        Inclut maintenant l'apprentissage incr√©mental.
        """
        self._log("üöÄ D√©marrage du pipeline complet Keno avec apprentissage incr√©mental...")
        if not self.load_data():
            self._log("‚ùå Chargement des donn√©es impossible.", "ERROR")
            return
        self.stats = self.analyze_patterns()
        self.train_xgboost_models(retrain=False)
        self.load_ml_models()
        
        # √âvaluation initiale des performances si nous avons des donn√©es
        if len(self.data) > 100:
            self._evaluate_initial_performance()
        
        self._log("‚úÖ Pipeline complet termin√© avec apprentissage incr√©mental activ√©.")
    
    def _evaluate_initial_performance(self):
        """√âvalue les performances initiales du mod√®le sur les donn√©es de test"""
        try:
            # Utiliser les 10% derniers tirages comme test
            test_size = int(len(self.data) * 0.1)
            test_data = self.data.tail(test_size).copy()
            
            self._log(f"üìä √âvaluation initiale des performances sur {test_size} tirages de test...")
            
            # G√©n√©rer des pr√©dictions pour chaque tirage de test
            accurate_predictions = 0
            total_hit_rate = 0.0
            
            for idx in range(min(10, len(test_data))):  # Limiter √† 10 √©valuations pour la vitesse
                test_row = test_data.iloc[idx]
                actual_draw = [int(test_row[f'boule{i}']) for i in range(1, 21)]
                
                # Obtenir une pr√©diction TOP 30
                top30_predictions = self.predict_numbers_ml(num_grids=1)
                if top30_predictions:
                    predicted_numbers = [num for num, _ in top30_predictions][:30]
                    
                    # Calculer les m√©triques
                    hit_count = len(set(predicted_numbers).intersection(set(actual_draw)))
                    hit_rate = hit_count / len(actual_draw)
                    accuracy = 1.0 if hit_count >= 5 else hit_count / 20.0  # 5+ hits = succ√®s
                    
                    accurate_predictions += accuracy
                    total_hit_rate += hit_rate
            
            # Calculer les moyennes
            avg_accuracy = accurate_predictions / 10 if total_hit_rate > 0 else 0.5
            avg_hit_rate = total_hit_rate / 10 if total_hit_rate > 0 else 0.3
            
            # Mettre √† jour les performances initiales
            self.update_performance(
                prediction_accuracy=avg_accuracy,
                top30_hit_rate=avg_hit_rate,
                feedback_score=avg_hit_rate,
                sample_size=10
            )
            
            self._log(f"üìä Performance initiale: Accuracy={avg_accuracy:.3f}, Hit rate={avg_hit_rate:.3f}")
            
        except Exception as e:
            self._log(f"‚ö†Ô∏è Erreur lors de l'√©valuation initiale: {e}")
    
    def simulate_learning_improvement(self, num_simulations: int = 20):
        """
        Simule l'am√©lioration de l'apprentissage avec des tirages synth√©tiques
        Utile pour d√©montrer les capacit√©s d'apprentissage incr√©mental
        
        Args:
            num_simulations: Nombre de simulations √† effectuer
        """
        self._log(f"üß™ Simulation de {num_simulations} cycles d'apprentissage incr√©mental...")
        
        for i in range(num_simulations):
            # G√©n√©rer un tirage synth√©tique r√©aliste
            # (bas√© sur les patterns des donn√©es existantes)
            if self.stats:
                # Utiliser les fr√©quences pour g√©n√©rer un tirage probable
                weights = np.array([self.stats.frequences.get(num, 1) for num in range(1, 71)])
                weights = weights / weights.sum()
                
                synthetic_draw = sorted(np.random.choice(
                    range(1, 71),
                    size=20,
                    replace=False,
                    p=weights
                ))
            else:
                synthetic_draw = sorted(random.sample(range(1, 71), 20))
            
            # Obtenir une pr√©diction
            top30_predictions = self.predict_numbers_ml(num_grids=1)
            if top30_predictions:
                predicted_numbers = [num for num, _ in top30_predictions][:30]
                
                # Simuler le feedback
                self.add_feedback(predicted_numbers, synthetic_draw)
                
                if (i + 1) % 5 == 0:
                    self._log(f"   üìà Cycle {i+1}/{num_simulations} termin√©")
        
        # Afficher le rapport final
        report = self.get_learning_report()
        print("\n" + "="*80)
        print(report)
        print("="*80)
        
        self._log(f"‚úÖ Simulation d'apprentissage termin√©e apr√®s {num_simulations} cycles")
    
    def retrain_with_incremental_data(self):
        """
        R√©entra√Æne le mod√®le en int√©grant les donn√©es d'apprentissage incr√©mental
        """
        self._log("üîÑ R√©entra√Ænement avec donn√©es d'apprentissage incr√©mental...")
        
        # Sauvegarder l'ancien mod√®le
        old_model_path = self.models_dir / f"xgb_keno_multilabel_v{self.incremental_state.model_version-1}.pkl"
        current_model_path = self.models_dir / "xgb_keno_multilabel.pkl"
        
        if current_model_path.exists():
            import shutil
            shutil.copy2(current_model_path, old_model_path)
            self._log(f"üíæ Ancien mod√®le sauvegard√© comme version {self.incremental_state.model_version-1}")
        
        # R√©entra√Æner avec les nouveaux poids adaptatifs
        success = self.train_xgboost_models(retrain=True)
        
        if success:
            self.incremental_state.model_version += 1
            self._save_incremental_state()
            self._log(f"‚úÖ R√©entra√Ænement r√©ussi - Nouvelle version {self.incremental_state.model_version}")
        else:
            self._log("‚ùå √âchec du r√©entra√Ænement - Conservation du mod√®le pr√©c√©dent", "ERROR")

    def generate_optimized_grids(self, num_grids: int = 40) -> list:
        """
        G√©n√®re des grilles optimis√©es en privil√©giant le TOP 30 ML.
        """
        self._log(f"üéØ G√©n√©ration de {num_grids} grilles Keno optimis√©es (TOP 30 ML privil√©gi√©, {self.grid_size} num√©ros/grille)...")
        top30_ml = [num for num, _ in self.predict_numbers_ml()]
        if not top30_ml or len(top30_ml) < self.grid_size:
            self._log("‚ùå TOP 30 ML indisponible, g√©n√©ration al√©atoire.", "ERROR")
            # Fallback : g√©n√©ration al√©atoire
            return [sorted(random.sample(range(1, 71), self.grid_size)) for _ in range(num_grids)]
        grids = []
        for _ in range(num_grids):
            grid = sorted(random.sample(top30_ml, self.grid_size))
            grids.append(grid)
        self._log(f"‚úÖ {len(grids)} grilles de {self.grid_size} num√©ros g√©n√©r√©es √† partir du TOP 30 ML")
        return grids

    def save_results(self, grids: list):
        """Sauvegarde les grilles g√©n√©r√©es dans un fichier CSV avec colonnes adapt√©es √† la taille"""
        output_path = self.output_dir / "grilles_keno.csv"
        columns = [f"numero_{i}" for i in range(1, self.grid_size + 1)]
        df = pd.DataFrame(grids, columns=columns)
        df.to_csv(output_path, index=False)
        self._log(f"üíæ Grilles de {self.grid_size} num√©ros sauvegard√©es dans {output_path}")

    def save_top30_ml_csv(self):
        """Sauvegarde le TOP 30 ML dans un fichier CSV"""
        top30_ml = [num for num, _ in self.predict_numbers_ml()]
        output_path = self.output_dir / "top30_ml.csv"
        df = pd.DataFrame(top30_ml, columns=["Num√©ro"])
        df.to_csv(output_path, index=False)
        self._log(f"üíæ TOP 30 ML sauvegard√© dans {output_path}")

    def generate_report(self, grids: list) -> str:
        """
        G√©n√®re un rapport d√©taill√© sur les grilles produites.
        """
        report = "# Rapport d√©taill√© des grilles Keno\n\n"
        report += f"Nombre de grilles g√©n√©r√©es : {len(grids)}\n\n"
        report += "## Grilles\n"
        for idx, grid in enumerate(grids, 1):
            report += f"- Grille {idx}: {grid}\n"
        report += "\n"
        # Statistiques sur la diversit√© des num√©ros
        all_numbers = [num for grid in grids for num in grid]
        unique_numbers = set(all_numbers)
        report += f"Nombre total de num√©ros uniques utilis√©s : {len(unique_numbers)}\n"
        report += f"Liste des num√©ros uniques : {sorted(unique_numbers)}\n"
        return report

    def update_and_retrain(self):
        """
        Recharge les donn√©es, analyse, et r√©entra√Æne le mod√®le ML sur tous les tirages.
        """
        self._log("üîÑ Mise √† jour des donn√©es et r√©entra√Ænement du mod√®le ML...")
        self.load_data()
        self.stats = self.analyze_patterns()
        self.train_xgboost_models(retrain=True)
        self.load_ml_models()
        self._log("‚úÖ Mod√®le ML r√©entra√Æn√© avec les nouveaux tirages.")

    def evaluate_grids_with_model(self, grids: list) -> list:
        """
        √âvalue chaque grille g√©n√©r√©e avec le mod√®le ML et retourne les scores/probabilit√©s.
        """
        if 'multilabel' not in self.ml_models:
            self._log("‚ùå Mod√®le ML non disponible pour l'√©valuation.", "ERROR")
            return []
        model = self.ml_models['multilabel']
        scores = []
        for grid in grids:
            current_date = pd.Timestamp.now()
            # Adapter le DataFrame aux grilles de taille variable
            df_data = {'date_de_tirage': [current_date]}
            for i in range(1, 21):
                if i <= len(grid):
                    df_data[f'boule{i}'] = [grid[i-1]]
                else:
                    df_data[f'boule{i}'] = [0]  # Padding avec des z√©ros
            
            df = pd.DataFrame(df_data)

            # Ajout des features temporelles ET enrichies (comme √† l'entra√Ænement)
            df_features = self.add_cyclic_features(df)
            df_features = self.enrich_features(df_features)

            # Reconstruction des features d'historique - EXACTEMENT comme √† l'entra√Ænement
            lag_features = {}
            if self.stats and self.stats.derniers_tirages:
                for lag in range(1, 6):
                    for ball_num in range(1, 21):
                        col_name = f'lag{lag}_boule{ball_num}'
                        if lag <= len(self.stats.derniers_tirages):
                            last_draw = self.stats.derniers_tirages[-(lag)]
                            lag_features[col_name] = last_draw[ball_num - 1] if ball_num <= len(last_draw) else 0
                        else:
                            lag_features[col_name] = 0
                lag_df = pd.DataFrame([lag_features], index=df_features.index)
            else:
                # Valeurs par d√©faut si pas d'historique
                lag_features = {f'lag{lag}_boule{ball_num}': 0 for lag in range(1, 6) for ball_num in range(1, 21)}
                lag_df = pd.DataFrame([lag_features], index=df_features.index)

            df_features = pd.concat([df_features, lag_df], axis=1)

            # Features de fr√©quence par zone - EXACTEMENT comme √† l'entra√Ænement
            zone_features = {
                'zone1_count': [],
                'zone2_count': [],
                'zone3_count': [],
                'zone4_count': [],
                'zone1_freq': [],
                'zone2_freq': [],
                'zone3_freq': [],
                'zone4_freq': []
            }
            
            for idx, row in df_features.iterrows():
                draw_numbers = [int(row[f'boule{i}']) for i in range(1, 21) if row[f'boule{i}'] > 0]
                zone_features['zone1_count'].append(sum(1 for n in draw_numbers if 1 <= n <= 17))
                zone_features['zone2_count'].append(sum(1 for n in draw_numbers if 18 <= n <= 35))
                zone_features['zone3_count'].append(sum(1 for n in draw_numbers if 36 <= n <= 52))
                zone_features['zone4_count'].append(sum(1 for n in draw_numbers if 53 <= n <= 70))
                zone_features['zone1_freq'].append(0)
                zone_features['zone2_freq'].append(0)
                zone_features['zone3_freq'].append(0)
                zone_features['zone4_freq'].append(0)

            # Ajout des features de zones
            for zone_name, zone_values in zone_features.items():
                if len(zone_values) != len(df_features):
                    zone_values = [0] * len(df_features)
                df_features[zone_name] = zone_values

            # AJOUT CRUCIAL: Features statistiques √©tendues comme √† l'entra√Ænement
            if self.stats:
                stats_data = {
                    f'freq_recent_{num}': self.stats.frequences_recentes.get(num, 0)
                    for num in range(1, 71)
                }
                stats_data.update({
                    f'retard_{num}': self.stats.retards.get(num, 0)
                    for num in range(1, 71)
                })
                stats_data.update({
                    f'tendance_{num}': self.stats.tendances_50.get(num, 1.0)
                    for num in range(1, 71)
                })
                stats_df = pd.DataFrame([stats_data], index=df_features.index)
                df_features = pd.concat([df_features, stats_df], axis=1)
            else:
                # Cr√©er des valeurs par d√©faut pour toutes les features statistiques
                stats_data = {}
                for num in range(1, 71):
                    stats_data[f'freq_recent_{num}'] = 0
                    stats_data[f'retard_{num}'] = 0
                    stats_data[f'tendance_{num}'] = 1.0
                stats_df = pd.DataFrame([stats_data], index=df_features.index)
                df_features = pd.concat([df_features, stats_df], axis=1)

            # Utiliser la liste de features de l'entra√Ænement
            feature_cols = self.metadata.get('feature_names', [])

            # CORRECTION D√âFINITIVE: Ajouter toutes les colonnes manquantes avec 0.0
            for c in feature_cols:
                if c not in df_features.columns:
                    df_features[c] = 0.0

            # Supprimer les colonnes en trop et r√©ordonner exactement comme √† l'entra√Ænement
            df_features = df_features.loc[:, feature_cols]

            # V√©rification finale du nombre de features
            if df_features.shape[1] != len(feature_cols):
                self._log(f"‚ö†Ô∏è  Mismatch: {df_features.shape[1]} features vs {len(feature_cols)} attendues", "ERROR")
                scores.append((grid, 0.0))
                continue

            X_eval = df_features.fillna(0)
            X_eval = X_eval.select_dtypes(include=[np.number])

            # Normalisation des features
            from sklearn.preprocessing import StandardScaler
            scaler = StandardScaler()
            X_eval_scaled = scaler.fit_transform(X_eval)

            # Pr√©diction des probabilit√©s pour tous les num√©ros
            probabilities = model.predict_proba(X_eval_scaled)

            # Calcul du score moyen de la grille
            grid_score = 0.0
            count = 0
            for num in grid:
                if 1 <= num <= KENO_PARAMS['total_numbers']:
                    idx = num - 1  # Index 0-69 pour num√©ros 1-70
                    if idx < len(probabilities):
                        if len(probabilities[idx]) > 0 and len(probabilities[idx][0]) > 1:
                            prob = probabilities[idx][0][1]
                            grid_score += prob
                            count += 1
            
            if count > 0:
                grid_score /= count
            
            scores.append((grid, grid_score))

        return scores

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="G√©n√©rateur avanc√© de grilles Keno avec apprentissage incr√©mental")
    parser.add_argument("--n", type=int, default=10, help="Nombre de grilles √† g√©n√©rer")
    parser.add_argument("--grids", type=int, help="Alias pour --n (nombre de grilles √† g√©n√©rer)")
    parser.add_argument("--size", type=int, default=10, choices=[7, 8, 9, 10], help="Taille des grilles (7 √† 10 num√©ros)")
    parser.add_argument("--profile", type=str, default="balanced", help="Profil d'entra√Ænement ML (quick, balanced, comprehensive, intensive)")
    parser.add_argument("--data", type=str, default=None, help="Chemin du fichier de donn√©es Keno")
    parser.add_argument("--silent", action="store_true", help="Mode silencieux")
    parser.add_argument("--retrain", action="store_true", help="Forcer le r√©entra√Ænement du mod√®le ML")
    parser.add_argument("--save-top30-ml", action="store_true", help="Sauvegarder le TOP 30 ML dans un CSV")
    parser.add_argument("--test-grids", action="store_true", help="√âvaluer les grilles g√©n√©r√©es avec le mod√®le ML")
    
    # Nouvelles options pour l'apprentissage incr√©mental
    parser.add_argument("--learning-report", action="store_true", help="Afficher le rapport d'apprentissage incr√©mental")
    parser.add_argument("--simulate-learning", type=int, metavar="N", help="Simuler N cycles d'apprentissage incr√©mental")
    parser.add_argument("--add-feedback", nargs=2, metavar=("PREDICTED", "ACTUAL"), 
                       help="Ajouter un feedback (format: 'num1,num2,...' 'num1,num2,...')")
    parser.add_argument("--retrain-incremental", action="store_true", help="R√©entra√Æner avec les donn√©es incr√©mentales")
    parser.add_argument("--demo-data", action="store_true", help="G√©n√©rer des donn√©es de d√©monstration")
    
    args = parser.parse_args()

    # Gestion de l'alias --grids
    num_grids = args.grids if args.grids is not None else args.n
    
    # G√©n√©ration de donn√©es de d√©monstration si demand√©e
    if args.demo_data:
        from pathlib import Path
        demo_script = Path(__file__).parent / "generate_demo_data.py"
        if demo_script.exists():
            import subprocess
            subprocess.run([sys.executable, str(demo_script), "--draws", "1000"])
        else:
            print("‚ùå Script de g√©n√©ration de donn√©es de d√©monstration non trouv√©")
        sys.exit(0)

    generator = KenoGeneratorAdvanced(
        data_path=args.data,
        silent=args.silent,
        training_profile=args.profile,
        grid_size=args.size
    )
    
    # Gestion des commandes sp√©ciales d'apprentissage incr√©mental
    if args.learning_report:
        print(generator.get_learning_report())
        sys.exit(0)
    
    if args.simulate_learning:
        if not generator.load_data():
            print("‚ùå Impossible de charger les donn√©es pour la simulation")
            sys.exit(1)
        generator.stats = generator.analyze_patterns()
        generator.train_xgboost_models(retrain=False)
        generator.load_ml_models()
        generator.simulate_learning_improvement(args.simulate_learning)
        sys.exit(0)
    
    if args.add_feedback:
        try:
            predicted = [int(x) for x in args.add_feedback[0].split(',')]
            actual = [int(x) for x in args.add_feedback[1].split(',')]
            generator.add_feedback(predicted, actual)
            print("‚úÖ Feedback ajout√© avec succ√®s")
            print(generator.get_learning_report())
        except ValueError:
            print("‚ùå Format de feedback invalide. Utilisez: --add-feedback '1,2,3,...' '4,5,6,...'")
        sys.exit(0)
    
    if args.retrain_incremental:
        if not generator.load_data():
            print("‚ùå Impossible de charger les donn√©es pour le r√©entra√Ænement")
            sys.exit(1)
        generator.stats = generator.analyze_patterns()
        generator.retrain_with_incremental_data()
        sys.exit(0)

    # Pipeline principal
    if args.retrain:
        generator.update_and_retrain()
    else:
        generator.run_full_pipeline(num_grids=num_grids, profile=args.profile)

    grids = generator.generate_optimized_grids(num_grids=num_grids)
    generator.save_results(grids)

    if args.save_top30_ml:
        generator.save_top30_ml_csv()

    if args.test_grids:
        grid_scores = generator.evaluate_grids_with_model(grids)
        print("\nüìä Scores des grilles g√©n√©r√©es :")
        for i, (grid, score) in enumerate(grid_scores, 1):
            print(f"Grille {i}: {grid} (Score: {score:.4f})")

    print("\n" + generator.generate_report(grids))
    
    # Affichage du rapport d'apprentissage si des performances ont √©t√© collect√©es
    if len(generator.incremental_state.performance_history) > 0:
        print("\n" + "="*60)
        print("üìä RAPPORT D'APPRENTISSAGE INCR√âMENTAL")
        print("="*60)
        print(generator.get_learning_report())
