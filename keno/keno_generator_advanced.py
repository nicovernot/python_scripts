#!/usr/bin/env python3
"""
============================================================================
üé≤ G√âN√âRATEUR INTELLIGENT DE GRILLES KENO - VERSION AVANC√âE üé≤
============================================================================

G√©n√©rateur de grilles Keno utilisant l'apprentissage automatique (XGBoost) 
et l'analyse statistique pour optimiser les combinaisons.

Caract√©ristiques:
- Machine Learning avec XGBoost pour pr√©dire les num√©ros probables
- Strat√©gie adaptative avec pond√©ration dynamique ML/Fr√©quence  
- Analyse de patterns temporels et cycliques
- Optimisation des combinaisons selon crit√®res statistiques
- G√©n√©ration de rapports d√©taill√©s avec visualisations

Auteur: Assistant IA
Version: 2.0
Date: Ao√ªt 2025
============================================================================
"""

import pandas as pd
import numpy as np
import random
import argparse
import sys
import time
import warnings
from datetime import datetime, timedelta
from pathlib import Path
import os
import json
import pickle
from typing import List, Tuple, Dict, Optional, Any
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
import multiprocessing as mp
from dataclasses import dataclass
from tqdm import tqdm
import logging

# DuckDB pour optimiser les requ√™tes
try:
    import duckdb
    HAS_DUCKDB = True
except ImportError:
    HAS_DUCKDB = False
    print("‚ö†Ô∏è  DuckDB non disponible. Utilisation de Pandas uniquement.")

# ML et analyse
try:
    import xgboost as xgb
    from sklearn.model_selection import train_test_split, cross_val_score
    from sklearn.metrics import accuracy_score, classification_report
    from sklearn.preprocessing import StandardScaler
    HAS_ML = True
except ImportError:
    HAS_ML = False
    print("‚ö†Ô∏è  Modules ML non disponibles. Mode fr√©quence uniquement.")

# Visualisation et analyse
try:
    import matplotlib.pyplot as plt
    import seaborn as sns
    from scipy import stats
    from scipy.fft import fft, fftfreq
    from scipy.signal import find_peaks
    from statsmodels.tsa.seasonal import STL
    from statsmodels.tsa.stattools import acf
    HAS_VIZ = True
except ImportError:
    HAS_VIZ = False
    print("‚ö†Ô∏è  Modules de visualisation non disponibles.")

# Configuration des warnings
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)

# ==============================================================================
# üîß CONFIGURATION ET CONSTANTES
# ==============================================================================

# Param√®tres sp√©cifiques au Keno
KENO_PARAMS = {
    'total_numbers': 70,        # Num√©ros de 1 √† 70
    'numbers_per_draw': 20,     # 20 num√©ros tir√©s par tirage
    'player_selection': 10,     # Le joueur s√©lectionne typiquement 10 num√©ros
    'min_selection': 7,         # Minimum de num√©ros s√©lectionnables (modifi√©)
    'max_selection': 10,        # Maximum de num√©ros s√©lectionnables
}

# Chemins des fichiers
BASE_DIR = Path(__file__).parent
DATA_DIR = BASE_DIR / "keno_data"
MODELS_DIR = BASE_DIR.parent / "keno_models"
OUTPUT_DIR = BASE_DIR.parent / "keno_output"

# Configuration ML
ML_CONFIG = {
    'n_estimators': 100,
    'max_depth': 6,
    'learning_rate': 0.1,
    'random_state': 42,
    'n_jobs': -1,
    'objective': 'binary:logistic'
}

def get_training_params(profile: str) -> dict:
    """
    Retourne les param√®tres d'entra√Ænement selon le profil s√©lectionn√©
    
    Args:
        profile: Profil d'entra√Ænement ('quick', 'balanced', 'comprehensive', 'intensive')
    
    Returns:
        dict: Param√®tres d'entra√Ænement pour RandomForest
    """
    if profile == "quick":
        return {
            'n_estimators': 50,        # Arbres r√©duits pour la vitesse
            'max_depth': 8,            # Profondeur mod√©r√©e
            'min_samples_split': 10,   # √âviter l'overfitting
            'min_samples_leaf': 5,     # √âviter l'overfitting
            'max_features': 'sqrt',    # Features r√©duites
            'random_state': 42,
            'n_jobs': -1,
            'class_weight': 'balanced'
        }
    elif profile == "balanced":
        return {
            'n_estimators': 100,       # Standard
            'max_depth': 12,           # Profondeur √©quilibr√©e
            'min_samples_split': 5,    # Standard
            'min_samples_leaf': 3,     # Standard
            'max_features': 'sqrt',    # Standard
            'random_state': 42,
            'n_jobs': -1,
            'class_weight': 'balanced'
        }
    elif profile == "comprehensive":
        return {
            'n_estimators': 200,       # Plus d'arbres pour meilleure pr√©cision
            'max_depth': 15,           # Profondeur √©lev√©e
            'min_samples_split': 4,    # Plus de splits
            'min_samples_leaf': 2,     # Feuilles plus petites
            'max_features': 'log2',    # Plus de features
            'random_state': 42,
            'n_jobs': -1,
            'class_weight': 'balanced'
        }
    elif profile == "intensive":
        return {
            'n_estimators': 300,       # Maximum d'arbres
            'max_depth': 20,           # Profondeur maximale
            'min_samples_split': 3,    # Splits agressifs
            'min_samples_leaf': 1,     # Feuilles minimales
            'max_features': None,      # Toutes les features
            'random_state': 42,
            'n_jobs': -1,
            'class_weight': 'balanced'
        }
    else:
        # Fallback sur balanced
        return get_training_params("balanced")

@dataclass
class KenoStats:
    """Structure pour stocker les statistiques Keno compl√®tes"""
    # 1. Fr√©quences d'apparition des num√©ros
    frequences: Dict[int, int]                           # Fr√©quence globale
    frequences_recentes: Dict[int, int]                  # Fr√©quence sur 100 derniers tirages
    frequences_50: Dict[int, int]                        # Fr√©quence sur 50 derniers tirages
    frequences_20: Dict[int, int]                        # Fr√©quence sur 20 derniers tirages
    
    # 2. Num√©ros en retard (overdue numbers)
    retards: Dict[int, int]                              # Retard actuel de chaque num√©ro
    retards_historiques: Dict[int, List[int]]            # Historique des retards
    
    # 3. Combinaisons et patterns r√©currents
    paires_freq: Dict[Tuple[int, int], int]              # Paires fr√©quentes
    trios_freq: Dict[Tuple[int, int, int], int]          # Trios fr√©quents
    patterns_parit√©: Dict[str, int]                      # Distribution pair/impair
    patterns_sommes: Dict[int, int]                      # Distribution des sommes
    patterns_zones: Dict[str, int]                       # R√©partition par zones/dizaines
    
    # 4. Analyse par p√©riode
    tendances_10: Dict[int, float]                       # Tendances sur 10 tirages
    tendances_50: Dict[int, float]                       # Tendances sur 50 tirages  
    tendances_100: Dict[int, float]                      # Tendances sur 100 tirages
    
    # Donn√©es brutes pour analyses avanc√©es
    zones_freq: Dict[str, int]                           # Ancien format maintenu pour compatibilit√©
    derniers_tirages: List[List[int]]                    # Derniers tirages
    tous_tirages: List[List[int]]                        # Tous les tirages pour DuckDB

# ==============================================================================
# üéØ CLASSE PRINCIPALE GENERATEUR KENO
# ==============================================================================

class KenoGeneratorAdvanced:
    """G√©n√©rateur avanc√© de grilles Keno avec ML et analyse statistique"""
    
    def __init__(self, data_path: str = None, silent: bool = False, training_profile: str = "balanced", grid_size: int = 10):
        """
        Initialise le g√©n√©rateur Keno
        
        Args:
            data_path: Chemin vers les donn√©es historiques
            silent: Mode silencieux pour r√©duire les sorties
            training_profile: Profil d'entra√Ænement ('quick', 'balanced', 'comprehensive', 'intensive')
            grid_size: Taille des grilles (7 √† 10 num√©ros)
        """
        self.silent = silent
        self.data_path = data_path or str(DATA_DIR / "keno_202010.parquet")
        self.models_dir = MODELS_DIR
        self.output_dir = OUTPUT_DIR
        self.training_profile = training_profile
        
        # Validation de la taille de grille
        if not (7 <= grid_size <= 10):
            self._log(f"‚ö†Ô∏è  Taille de grille invalide ({grid_size}). Utilisation de 10 par d√©faut.", "ERROR")
            grid_size = 10
        self.grid_size = grid_size
        
        # Cr√©er les r√©pertoires n√©cessaires
        self.models_dir.mkdir(exist_ok=True)
        self.output_dir.mkdir(exist_ok=True)
        
        # Initialisation des composants
        self.data = None
        self.stats = None
        self.ml_models = {}
        self.metadata = {}
        self.cache = {}
        
        # Strat√©gie adaptative
        self.adaptive_weights = {
            'ml_weight': 0.6,      # 60% ML par d√©faut
            'freq_weight': 0.4,    # 40% fr√©quence par d√©faut  
            'performance_history': [],
            'last_update': datetime.now()
        }
        
        self._log(f"üé≤ G√©n√©rateur Keno Avanc√© v2.0 initialis√© (grilles de {self.grid_size} num√©ros)")

    def _log(self, message: str, level: str = "INFO"):
        """Syst√®me de logging configur√©"""
        if not self.silent or level == "ERROR":
            print(f"{message}")
    
    def _ensure_zone_freq_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Ajoute les colonnes zoneX_freq manquantes pour compatibilit√© avec le mod√®le ML.
        """
        for freq_col in ['zone1_freq', 'zone2_freq', 'zone3_freq', 'zone4_freq']:
            if freq_col not in df.columns:
                df[freq_col] = 0
        return df

    
    def load_data(self) -> bool:
        """
        Charge les donn√©es historiques des tirages Keno
        
        Returns:
            bool: True si les donn√©es sont charg√©es avec succ√®s
        """
        try:
            self._log("üìä Chargement des donn√©es historiques Keno...")
            
            if not Path(self.data_path).exists():
                self._log(f"‚ùå Fichier non trouv√©: {self.data_path}", "ERROR")
                return False
                
            # Chargement depuis Parquet
            self.data = pd.read_parquet(self.data_path)
            
            # Validation des colonnes requises
            required_cols = ['date_de_tirage'] + [f'boule{i}' for i in range(1, 21)]
            missing_cols = [col for col in required_cols if col not in self.data.columns]
            
            if missing_cols:
                self._log(f"‚ùå Colonnes manquantes: {missing_cols}", "ERROR")
                return False
            
            # Tri par date
            self.data = self.data.sort_values('date_de_tirage').reset_index(drop=True)
            
            self._log(f"‚úÖ {len(self.data)} tirages Keno charg√©s ({self.data['date_de_tirage'].min()} √† {self.data['date_de_tirage'].max()})")
            return True
            
        except Exception as e:
            self._log(f"‚ùå Erreur lors du chargement des donn√©es: {e}", "ERROR")
            return False
    
    def analyze_patterns(self) -> KenoStats:
        """
        Analyse compl√®te des patterns et statistiques Keno avec DuckDB
        
        Returns:
            KenoStats: Statistiques compl√®tes calcul√©es
        """
        self._log("üîç Analyse compl√®te des patterns Keno avec DuckDB...")
        
        # Extraction des num√©ros de tous les tirages
        all_draws = []
        for _, row in self.data.iterrows():
            # Support des diff√©rents formats de colonnes
            if 'b1' in self.data.columns:
                draw = [int(row[f'boule{i}']) for i in range(1, 21)]
            else:
                draw = [int(row[f'boule{i}']) for i in range(1, 21)]
            all_draws.append(sorted(draw))
        
        self._log(f"üìä Analyse de {len(all_draws)} tirages")
        
        if HAS_DUCKDB:
            return self._analyze_with_duckdb(all_draws)
        else:
            return self._analyze_with_pandas(all_draws)
    
    def _analyze_with_duckdb(self, all_draws: List[List[int]]) -> KenoStats:
        """Analyse optimis√©e avec DuckDB"""
        self._log("üöÄ Utilisation de DuckDB pour l'analyse optimis√©e")
        
        # Cr√©er une connexion DuckDB
        conn = duckdb.connect(':memory:')
        
        # Pr√©parer les donn√©es pour DuckDB
        draws_data = []
        for i, draw in enumerate(all_draws):
            for num in draw:
                draws_data.append({'tirage_id': i, 'numero': num, 'position': draw.index(num)})
        
        draws_df = pd.DataFrame(draws_data)
        
        # Cr√©er la table dans DuckDB
        conn.register('tirages', draws_df)
        
        # 1. FR√âQUENCES D'APPARITION
        self._log("üìä Calcul des fr√©quences d'apparition...")
        
        # Fr√©quence globale
        freq_global = conn.execute("""
            SELECT numero, COUNT(*) as freq 
            FROM tirages 
            GROUP BY numero 
            ORDER BY numero
        """).fetchdf()
        frequences = dict(zip(freq_global['numero'], freq_global['freq']))
        
        # Fr√©quences r√©centes (100, 50, 20 derniers tirages)
        max_tirage = len(all_draws) - 1
        
        freq_100 = conn.execute(f"""
            SELECT numero, COUNT(*) as freq 
            FROM tirages 
            WHERE tirage_id >= {max(0, max_tirage - 99)}
            GROUP BY numero
        """).fetchdf()
        frequences_100 = dict(zip(freq_100['numero'], freq_100['freq']))
        
        freq_50 = conn.execute(f"""
            SELECT numero, COUNT(*) as freq 
            FROM tirages 
            WHERE tirage_id >= {max(0, max_tirage - 49)}
            GROUP BY numero
        """).fetchdf()
        frequences_50 = dict(zip(freq_50['numero'], freq_50['freq']))
        
        freq_20 = conn.execute(f"""
            SELECT numero, COUNT(*) as freq 
            FROM tirages 
            WHERE tirage_id >= {max(0, max_tirage - 19)}
            GROUP BY numero
        """).fetchdf()
        frequences_20 = dict(zip(freq_20['numero'], freq_20['freq']))
        
        # 2. CALCUL DES RETARDS
        self._log("‚è∞ Calcul des retards des num√©ros...")
        retards = {}
        retards_historiques = {}
        
        for num in range(1, KENO_PARAMS['total_numbers'] + 1):
            # Retard actuel
            retard = 0
            for draw in reversed(all_draws):
                if num in draw:
                    break
                retard += 1
            retards[num] = retard
            
            # Historique des retards
            historique = []
            dernier_tirage = -1
            for i, draw in enumerate(all_draws):
                if num in draw:
                    if dernier_tirage >= 0:
                        historique.append(i - dernier_tirage - 1)
                    dernier_tirage = i
            retards_historiques[num] = historique
        
        # 3. PATTERNS ET COMBINAISONS
        self._log("üîó Analyse des patterns et combinaisons...")
        
        # Paires fr√©quentes
        paires_freq = {}
        for draw in all_draws:
            for i in range(len(draw)):
                for j in range(i + 1, len(draw)):
                    pair = tuple(sorted([draw[i], draw[j]]))
                    paires_freq[pair] = paires_freq.get(pair, 0) + 1
        
        # Trios fr√©quents (limit√© aux plus fr√©quents pour performance)
        trios_freq = {}
        for draw in all_draws[-1000:]:  # Seulement les 1000 derniers pour performance
            for i in range(len(draw)):
                for j in range(i + 1, len(draw)):
                    for k in range(j + 1, len(draw)):
                        trio = tuple(sorted([draw[i], draw[j], draw[k]]))
                        trios_freq[trio] = trios_freq.get(trio, 0) + 1
        
        # Patterns de parit√©
        patterns_parit√© = {"tout_pair": 0, "tout_impair": 0, "mixte": 0}
        for draw in all_draws:
            pairs = sum(1 for num in draw if num % 2 == 0)
            if pairs == len(draw):
                patterns_parit√©["tout_pair"] += 1
            elif pairs == 0:
                patterns_parit√©["tout_impair"] += 1
            else:
                patterns_parit√©["mixte"] += 1
        
        # Patterns de sommes
        patterns_sommes = {}
        for draw in all_draws:
            somme = sum(draw)
            patterns_sommes[somme] = patterns_sommes.get(somme, 0) + 1
        
        # Patterns de zones/dizaines
        patterns_zones = {
            "zone_1_17": 0, "zone_18_35": 0, "zone_36_52": 0, "zone_53_70": 0,
            "dizaine_1_10": 0, "dizaine_11_20": 0, "dizaine_21_30": 0, 
            "dizaine_31_40": 0, "dizaine_41_50": 0, "dizaine_51_60": 0, "dizaine_61_70": 0
        }
        
        for draw in all_draws:
            # Comptage par zones
            for num in draw:
                if 1 <= num <= 17:
                    patterns_zones["zone_1_17"] += 1
                elif 18 <= num <= 35:
                    patterns_zones["zone_18_35"] += 1
                elif 36 <= num <= 52:
                    patterns_zones["zone_36_52"] += 1
                else:  # 53-70
                    patterns_zones["zone_53_70"] += 1
                
                # Comptage par dizaines
                dizaine = (num - 1) // 10 + 1
                if dizaine <= 7:
                    patterns_zones[f"dizaine_{(dizaine-1)*10+1}_{dizaine*10}"] += 1
        
        # 4. ANALYSE PAR P√âRIODE ET TENDANCES
        self._log("üìà Calcul des tendances par p√©riode...")
        
        tendances_10 = self._calculer_tendances(all_draws, 10)
        tendances_50 = self._calculer_tendances(all_draws, 50)
        tendances_100 = self._calculer_tendances(all_draws, 100)
        
        # Zones compatibilit√© (ancien format)
        zones_freq = {
            "zone1_17": patterns_zones["zone_1_17"],
            "zone18_35": patterns_zones["zone_18_35"], 
            "zone36_52": patterns_zones["zone_36_52"],
            "zone53_70": patterns_zones["zone_53_70"]
        }
        
        # Garder les derniers tirages
        derniers_tirages = all_draws[-50:] if len(all_draws) >= 50 else all_draws
        
        conn.close()
        
        # Initialiser avec des valeurs par d√©faut pour les champs manquants
        for num in range(1, KENO_PARAMS['total_numbers'] + 1):
            if num not in frequences:
                frequences[num] = 0
            if num not in frequences_100:
                frequences_100[num] = 0
            if num not in frequences_50:
                frequences_50[num] = 0
            if num not in frequences_20:
                frequences_20[num] = 0
        
        self.stats = KenoStats(
            frequences=frequences,
            frequences_recentes=frequences_100,
            frequences_50=frequences_50,
            frequences_20=frequences_20,
            retards=retards,
            retards_historiques=retards_historiques,
            paires_freq=paires_freq,
            trios_freq=trios_freq,
            patterns_parit√©=patterns_parit√©,
            patterns_sommes=patterns_sommes,
            patterns_zones=patterns_zones,
            tendances_10=tendances_10,
            tendances_50=tendances_50,
            tendances_100=tendances_100,
            zones_freq=zones_freq,
            derniers_tirages=derniers_tirages,
            tous_tirages=all_draws
        )
        
        self._log(f"‚úÖ Analyse DuckDB termin√©e - {len(all_draws)} tirages analys√©s")
        return self.stats
    
    def _analyze_with_pandas(self, all_draws: List[List[int]]) -> KenoStats:
        """Analyse de fallback avec Pandas seulement"""
        self._log("‚ö†Ô∏è  Analyse de fallback avec Pandas (DuckDB non disponible)")
        
        # Impl√©mentation simplifi√©e pour compatibilit√©
        frequences = {i: 0 for i in range(1, KENO_PARAMS['total_numbers'] + 1)}
        retards = {i: 0 for i in range(1, KENO_PARAMS['total_numbers'] + 1)}
        paires_freq = {}
        zones_freq = {"zone1_17": 0, "zone18_35": 0, "zone36_52": 0, "zone53_70": 0}
        
        # Comptage basique des fr√©quences
        for draw in all_draws:
            for num in draw:
                frequences[num] += 1
                
                # Zones
                if 1 <= num <= 17:
                    zones_freq["zone1_17"] += 1
                elif 18 <= num <= 35:
                    zones_freq["zone18_35"] += 1
                elif 36 <= num <= 52:
                    zones_freq["zone36_52"] += 1
                else:
                    zones_freq["zone53_70"] += 1
        
        # Calcul des retards
        for num in range(1, KENO_PARAMS['total_numbers'] + 1):
            retard = 0
            for draw in reversed(all_draws):
                if num in draw:
                    break
                retard += 1
            retards[num] = retard
        
        # Paires basiques
        for draw in all_draws:
            for i in range(len(draw)):
                for j in range(i + 1, len(draw)):
                    pair = tuple(sorted([draw[i], draw[j]]))
                    paires_freq[pair] = paires_freq.get(pair, 0) + 1
        
        derniers_tirages = all_draws[-50:] if len(all_draws) >= 50 else all_draws
        
        # Initialiser avec des valeurs par d√©faut
        self.stats = KenoStats(
            frequences=frequences,
            frequences_recentes=frequences.copy(),
            frequences_50=frequences.copy(),
            frequences_20=frequences.copy(),
            retards=retards,
            retards_historiques={i: [] for i in range(1, 71)},
            paires_freq=paires_freq,
            trios_freq={},
            patterns_parit√©={"tout_pair": 0, "tout_impair": 0, "mixte": len(all_draws)},
            patterns_sommes={},
            patterns_zones={},
            tendances_10={i: 0.0 for i in range(1, 71)},
            tendances_50={i: 0.0 for i in range(1, 71)},
            tendances_100={i: 0.0 for i in range(1, 71)},
            zones_freq=zones_freq,
            derniers_tirages=derniers_tirages,
            tous_tirages=all_draws
        )
        
        self._log(f"‚úÖ Analyse Pandas termin√©e - {len(all_draws)} tirages analys√©s")
        return self.stats
    
    def _calculer_tendances(self, all_draws: List[List[int]], periode: int) -> Dict[int, float]:
        """Calcule les tendances d'apparition sur une p√©riode donn√©e"""
        if len(all_draws) < periode:
            return {i: 0.0 for i in range(1, KENO_PARAMS['total_numbers'] + 1)}
        
        tendances = {}
        for num in range(1, KENO_PARAMS['total_numbers'] + 1):
            # Fr√©quence r√©cente vs fr√©quence historique
            recent_draws = all_draws[-periode:]
            freq_recent = sum(1 for draw in recent_draws if num in draw)
            freq_moyenne = freq_recent / periode
            
            # Fr√©quence historique
            freq_historique = sum(1 for draw in all_draws[:-periode] if num in draw)
            if len(all_draws) > periode:
                freq_historique_moyenne = freq_historique / (len(all_draws) - periode)
            else:
                freq_historique_moyenne = freq_moyenne
            
            # Tendance = ratio r√©cent/historique
            if freq_historique_moyenne > 0:
                tendances[num] = freq_moyenne / freq_historique_moyenne
            else:
                tendances[num] = freq_moyenne * 2  # Bonus si nouveau num√©ro
        
        return tendances
    
    def add_cyclic_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Ajoute des features cycliques pour l'apprentissage automatique
        
        Args:
            df: DataFrame avec les donn√©es
            
        Returns:
            DataFrame avec les features ajout√©es
        """
        df = df.copy()
        
        # Features temporelles cycliques
        df['day_sin'] = np.sin(2 * np.pi * df['date_de_tirage'].dt.day / 31)
        df['day_cos'] = np.cos(2 * np.pi * df['date_de_tirage'].dt.day / 31)
        df['month_sin'] = np.sin(2 * np.pi * df['date_de_tirage'].dt.month / 12)  
        df['month_cos'] = np.cos(2 * np.pi * df['date_de_tirage'].dt.month / 12)
        
        # Features cycliques pour chaque num√©ro (1-70 pour Keno)
        for i in range(1, 11):  # Pour les 10 premi√®res boules principales
            if f'boule{i}' in df.columns:
                df[f'boule{i}_sin'] = np.sin(2 * np.pi * df[f'boule{i}'] / KENO_PARAMS['total_numbers'])
                df[f'boule{i}_cos'] = np.cos(2 * np.pi * df[f'boule{i}'] / KENO_PARAMS['total_numbers'])
        
        return df
    
    def enrich_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Ajoute des features avanc√©es pour le ML : s√©quences, clusters, dispersion, stats paires/trios, features cycliques, etc.
        """
        df = df.copy()
        # Features cycliques d√©j√† pr√©sentes
        # Ajout de la dispersion (√©cart-type des num√©ros tir√©s)
        df['dispersion'] = df[[f'boule{i}' for i in range(1, 21)]].std(axis=1)
        # Ajout du nombre de paires et trios fr√©quents dans le tirage
        if hasattr(self, 'stats') and self.stats:
            def count_pairs(row):
                nums = [int(row[f'boule{i}']) for i in range(1, 21)]
                return sum(1 for i in range(len(nums)) for j in range(i+1, len(nums)) if tuple(sorted([nums[i], nums[j]])) in self.stats.paires_freq)
            def count_trios(row):
                nums = [int(row[f'boule{i}']) for i in range(1, 21)]
                return sum(1 for i in range(len(nums)) for j in range(i+1, len(nums)) for k in range(j+1, len(nums)) if tuple(sorted([nums[i], nums[j], nums[k]])) in self.stats.trios_freq)
            df['nb_paires_freq'] = df.apply(count_pairs, axis=1)
            df['nb_trios_freq'] = df.apply(count_trios, axis=1)
        else:
            df['nb_paires_freq'] = 0
            df['nb_trios_freq'] = 0
        # Ajout de la somme des num√©ros tir√©s
        df['somme_tirage'] = df[[f'boule{i}' for i in range(1, 21)]].sum(axis=1)
        # Ajout de la parit√© (nombre de pairs)
        df['nb_pairs'] = df[[f'boule{i}' for i in range(1, 21)]].apply(lambda x: sum(1 for n in x if n % 2 == 0), axis=1)
        # Ajout de la feature cluster (nombre de groupes de num√©ros cons√©cutifs)
        def count_clusters(row):
            nums = sorted([int(row[f'boule{i}']) for i in range(1, 21)])
            clusters = 1
            for i in range(1, len(nums)):
                if nums[i] - nums[i-1] > 1:
                    clusters += 1
            return clusters
        df['nb_clusters'] = df.apply(count_clusters, axis=1)
        return df
    
    def train_xgboost_models(self, retrain: bool = False) -> bool:
        """
        Entra√Æne un mod√®le XGBoost multi-label pour pr√©dire les num√©ros Keno
        
        Args:
            retrain: Force le r√©entra√Ænement m√™me si le mod√®le existe
            
        Returns:
            bool: True si l'entra√Ænement est r√©ussi
        """
        if not HAS_ML:
            self._log("‚ùå Modules ML non disponibles", "ERROR")
            return False
        
        # V√©rification du mod√®le existant (un seul mod√®le multi-label)
        model_file = self.models_dir / "xgb_keno_multilabel.pkl"
        if not retrain and model_file.exists():
            self._log("‚úÖ Mod√®le XGBoost Keno multi-label existant trouv√©")
            return self.load_ml_models()
        
        self._log("ü§ñ Entra√Ænement du mod√®le XGBoost Keno multi-label...")
        self._log("   üìä Strat√©gie: 1 mod√®le multi-label pour apprendre les corr√©lations")
        
        try:
            # Pr√©paration des donn√©es avec features enrichies (optimis√© pour √©viter la fragmentation)
            df_features = self.add_cyclic_features(self.data)
            df_features = self.enrich_features(df_features)
            
            # Features temporelles
            feature_cols = ['day_sin', 'day_cos', 'month_sin', 'month_cos']
            
            # Pr√©paration des features d'historique avec pd.concat pour √©viter la fragmentation
            self._log("   üìù Cr√©ation des features d'historique...")
            lag_features = {}
            
            for lag in range(1, 6):
                for ball_num in range(1, 21):
                    col_name = f'lag{lag}_boule{ball_num}'
                    if lag < len(df_features):
                        lag_features[col_name] = df_features[f'boule{ball_num}'].shift(lag).fillna(0)
                    else:
                        lag_features[col_name] = pd.Series([0] * len(df_features))
                    feature_cols.append(col_name)
            
            # Ajout des features d'historique en une seule fois
            lag_df = pd.DataFrame(lag_features, index=df_features.index)
            df_features = pd.concat([df_features, lag_df], axis=1)
            
            # Features de fr√©quence par zone (calcul√©es efficacement)
            self._log("   üìù Calcul des features de zones...")
            zone_features = {
                'zone1_count': [],
                'zone2_count': [],
                'zone3_count': [],
                'zone4_count': [],
                'zone1_freq': [],
                'zone2_freq': [],
                'zone3_freq': [],
                'zone4_freq': []
            }
            
            for idx, row in df_features.iterrows():
                draw_numbers = [int(row[f'boule{i}']) for i in range(1, 21)]
                zone_features['zone1_count'].append(sum(1 for n in draw_numbers if 1 <= n <= 17))
                zone_features['zone2_count'].append(sum(1 for n in draw_numbers if 18 <= n <= 35))
                zone_features['zone3_count'].append(sum(1 for n in draw_numbers if 36 <= n <= 52))
                zone_features['zone4_count'].append(sum(1 for n in draw_numbers if 53 <= n <= 70))
            
            # Ajout des features de zones
            for zone_name, zone_values in zone_features.items():
                # Correction : si la liste est vide ou de mauvaise taille, remplis avec des z√©ros
                if len(zone_values) != len(df_features):
                    zone_values = [0] * len(df_features)
                df_features[zone_name] = zone_values
                feature_cols.append(zone_name)

            # AJOUT CRUCIAL: Features statistiques √©tendues (manquantes √† l'entra√Ænement)
            if self.stats:
                # Features fr√©quentielles pour chaque num√©ro (1-70)
                stats_data = {
                    f'freq_recent_{num}': [self.stats.frequences_recentes.get(num, 0)] * len(df_features)
                    for num in range(1, 71)
                }
                stats_data.update({
                    f'retard_{num}': [self.stats.retards.get(num, 0)] * len(df_features)
                    for num in range(1, 71)
                })
                stats_data.update({
                    f'tendance_{num}': [self.stats.tendances_50.get(num, 1.0)] * len(df_features)
                    for num in range(1, 71)
                })
                
                # Ajouter toutes ces features au DataFrame
                for feature_name, feature_values in stats_data.items():
                    df_features[feature_name] = feature_values
                    feature_cols.append(feature_name)
            else:
                # Cr√©er des valeurs par d√©faut pour toutes les features statistiques
                for num in range(1, 71):
                    df_features[f'freq_recent_{num}'] = [0] * len(df_features)
                    df_features[f'retard_{num}'] = [0] * len(df_features)
                    df_features[f'tendance_{num}'] = [1.0] * len(df_features)
                    feature_cols.extend([f'freq_recent_{num}', f'retard_{num}', f'tendance_{num}'])
            
            X = df_features[feature_cols].fillna(0)
            
            # Cr√©ation du target multi-label (matrice 70 colonnes, une par num√©ro)
            y = np.zeros((len(df_features), KENO_PARAMS['total_numbers']))
            
            for idx, row in df_features.iterrows():
                draw_numbers = [int(row[f'boule{i}']) for i in range(1, 21)]
                for num in draw_numbers:
                    if 1 <= num <= KENO_PARAMS['total_numbers']:
                        y[idx, num - 1] = 1  # Index 0-69 pour num√©ros 1-70
            
            self._log(f"   üìù Donn√©es pr√©par√©es: {X.shape[0]} tirages, {X.shape[1]} features")
            self._log(f"   üìù Target multi-label: {y.shape[1]} num√©ros √† pr√©dire")
            
            # Split train/test
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42
            )
            
            # Configuration sp√©ciale pour multi-label
            ml_config = ML_CONFIG.copy()
            ml_config['objective'] = 'multi:logistic'
            ml_config['num_class'] = 2  # Binaire pour chaque label
            
            # Entra√Ænement du mod√®le multi-label avec RandomForest (meilleur pour les corr√©lations)
            from sklearn.ensemble import RandomForestClassifier
            from sklearn.multioutput import MultiOutputClassifier
            
            # Obtenir les param√®tres selon le profil d'entra√Ænement
            rf_params = get_training_params(self.training_profile)
            
            # Afficher le profil utilis√©
            profile_names = {
                'quick': 'Ultra-rapide',
                'balanced': '√âquilibr√©', 
                'comprehensive': 'Complet',
                'intensive': 'Intensif'
            }
            profile_name = profile_names.get(self.training_profile, '√âquilibr√©')
            self._log(f"üìä Profil d'entra√Ænement: {profile_name} ({self.training_profile})")
            self._log(f"   ‚Ä¢ Arbres: {rf_params['n_estimators']}")
            self._log(f"   ‚Ä¢ Profondeur max: {rf_params['max_depth']}")
            self._log(f"   ‚Ä¢ Features: {rf_params['max_features']}")
            
            # RandomForest configur√© selon le profil
            base_model = RandomForestClassifier(**rf_params)
            
            # Alternative: Essayer aussi XGBoost avec MultiOutputClassifier
            # base_model = xgb.XGBClassifier(
            #     objective='binary:logistic',
            #     n_estimators=150,
            #     max_depth=8,
            #     learning_rate=0.05,
            #     random_state=42,
            #     n_jobs=-1
            # )
            
            # Mod√®le multi-output
            model = MultiOutputClassifier(base_model, n_jobs=-1)
            
            self._log("   üîÑ Entra√Ænement du mod√®le RandomForest multi-label...")
            model.fit(X_train, y_train)
            
            # √âvaluation rapide
            y_pred = model.predict(X_test)
            
            # Calcul de l'accuracy moyenne sur tous les labels
            accuracies = []
            for i in range(y.shape[1]):
                acc = accuracy_score(y_test[:, i], y_pred[:, i])
                accuracies.append(acc)
            
            mean_accuracy = np.mean(accuracies)
            self._log(f"   üìä Accuracy moyenne: {mean_accuracy:.4f}")
            
            # Validation crois√©e
            from sklearn.model_selection import cross_val_score
            cv_scores = cross_val_score(base_model, X_train, y_train, cv=5, n_jobs=-1)
            self._log(f"   üìä Validation crois√©e (CV=5): Score moyen = {np.mean(cv_scores):.4f}")
            
            # Sauvegarde du mod√®le
            with open(model_file, 'wb') as f:
                pickle.dump(model, f)
            
            self.ml_models['multilabel'] = model
            
            # Sauvegarde des m√©tadonn√©es
            self.metadata = {
                'features_count': len(feature_cols),
                'model_type': 'xgboost_keno_multilabel',
                'created_date': datetime.now().strftime('%Y-%m-%d'),
                'version': '3.0',  # Version avec multi-label
                'keno_params': KENO_PARAMS,
                'mean_accuracy': mean_accuracy,
                'feature_names': feature_cols
            }
            
            metadata_path = self.models_dir / "metadata.json"
            with open(metadata_path, 'w') as f:
                json.dump(self.metadata, f, indent=2)
            
            self._log("‚úÖ Entra√Ænement du mod√®le multi-label Keno termin√©")
            return True
            
        except Exception as e:
            self._log(f"‚ùå Erreur lors de l'entra√Ænement: {e}", "ERROR")
            import traceback
            self._log(f"D√©tails: {traceback.format_exc()}")
            return False
    
    def load_ml_models(self) -> bool:
        """Charge le mod√®le ML multi-label pr√©-entra√Æn√©"""
        try:
            self._log("üì• Chargement du mod√®le ML Keno multi-label...")
            
            # Chargement des m√©tadonn√©es
            metadata_path = self.models_dir / "metadata.json"
            if metadata_path.exists():
                with open(metadata_path, 'r') as f:
                    self.metadata = json.load(f)
            
            # Chargement du mod√®le multi-label
            model_path = self.models_dir / "xgb_keno_multilabel.pkl"
            if model_path.exists():
                with open(model_path, 'rb') as f:
                    self.ml_models['multilabel'] = pickle.load(f)
                
                self._log("‚úÖ Mod√®le multi-label Keno charg√© avec succ√®s")
                if 'mean_accuracy' in self.metadata:
                    self._log(f"   üìä Accuracy du mod√®le: {self.metadata['mean_accuracy']:.4f}")
                return True
            else:
                self._log("‚ùå Mod√®le multi-label non trouv√©", "ERROR")
                return False
                
        except Exception as e:
            self._log(f"‚ùå Erreur lors du chargement du mod√®le: {e}", "ERROR")
            return False
    
    def predict_numbers_ml(self, num_grids: int = 10) -> List[List[int]]:
        """
        Pr√©dit les num√©ros avec le mod√®le ML multi-label
        
        Args:
            num_grids: Nombre de grilles √† g√©n√©rer
            
        Returns:
            List[List[int]]: Liste des grilles pr√©dites
        """
        if 'multilabel' not in self.ml_models:
            self._log("‚ùå Mod√®le ML multi-label non disponible", "ERROR")
            return []
        
        try:
            model = self.ml_models['multilabel']
            
            # Pr√©paration des features pour la pr√©diction
            current_date = pd.Timestamp.now()
            df_predict = pd.DataFrame({
                'date_de_tirage': [current_date],
                **{f'boule{i}': [0] for i in range(1, 21)}  # Valeurs dummy
            })
            
            # Ajout des features temporelles ET enrichies (comme √† l'entra√Ænement)
            df_features = self.add_cyclic_features(df_predict)
            df_features = self.enrich_features(df_features)
            
            # Features d'historique
            lag_features = {}
            if self.stats and self.stats.derniers_tirages:
                for lag in range(1, 6):
                    for ball_num in range(1, 21):
                        col_name = f'lag{lag}_boule{ball_num}'
                        if lag <= len(self.stats.derniers_tirages):
                            last_draw = self.stats.derniers_tirages[-(lag)]
                            lag_features[col_name] = last_draw[ball_num - 1] if ball_num <= len(last_draw) else 0
                        else:
                            lag_features[col_name] = 0
                lag_df = pd.DataFrame([lag_features], index=df_features.index)
            else:
                # Valeurs par d√©faut si pas d'historique
                lag_features = {f'lag{lag}_boule{ball_num}': 0 for lag in range(1, 6) for ball_num in range(1, 21)}
                lag_df = pd.DataFrame([lag_features], index=df_features.index)

            df_features = pd.concat([df_features, lag_df], axis=1)

            # Features de fr√©quence par zone
            zone_features = {
                'zone1_count': [0],
                'zone2_count': [0],
                'zone3_count': [0],
                'zone4_count': [0],
                'zone1_freq': [0],
                'zone2_freq': [0],
                'zone3_freq': [0],
                'zone4_freq': [0]
            }
            
            for zone_name, zone_values in zone_features.items():
                df_features[zone_name] = zone_values

            # AJOUT CRUCIAL: Features statistiques √©tendues pour TOUS les num√©ros (1-70)
            if self.stats:
                stats_data = {
                    f'freq_recent_{num}': self.stats.frequences_recentes.get(num, 0)
                    for num in range(1, 71)
                }
                stats_data.update({
                    f'retard_{num}': self.stats.retards.get(num, 0)
                    for num in range(1, 71)
                })
                stats_data.update({
                    f'tendance_{num}': self.stats.tendances_50.get(num, 1.0)
                    for num in range(1, 71)
                })
                stats_df = pd.DataFrame([stats_data], index=df_features.index)
                df_features = pd.concat([df_features, stats_df], axis=1)
            else:
                # Cr√©er des valeurs par d√©faut pour toutes les features statistiques
                stats_data = {}
                for num in range(1, 71):
                    stats_data[f'freq_recent_{num}'] = 0
                    stats_data[f'retard_{num}'] = 0
                    stats_data[f'tendance_{num}'] = 1.0
                stats_df = pd.DataFrame([stats_data], index=df_features.index)
                df_features = pd.concat([df_features, stats_df], axis=1)

            # Utiliser la liste de features de l'entra√Ænement
            feature_cols = self.metadata.get('feature_names', [])

            # CORRECTION D√âFINITIVE: Garantir exactement les m√™mes features
            for c in feature_cols:
                if c not in df_features.columns:
                    df_features[c] = 0.0

            # Supprimer les colonnes en trop et r√©ordonner exactement comme √† l'entra√Ænement
            df_features = df_features.loc[:, feature_cols]

            X_pred = df_features.fillna(0)
            X_pred = X_pred.select_dtypes(include=[np.number])

            # V√©rification du nombre de features
            if X_pred.shape[1] != len(feature_cols):
                self._log(f"‚ö†Ô∏è  Mismatch: {X_pred.shape[1]} features vs {len(feature_cols)} attendues", "ERROR")
                return []

            # Normalisation des features
            from sklearn.preprocessing import StandardScaler
            scaler = StandardScaler()
            X_pred_scaled = scaler.fit_transform(X_pred)

            # Pr√©diction des probabilit√©s pour tous les num√©ros
            probabilities = model.predict_proba(X_pred_scaled)

            number_probs = []
            for i in range(min(KENO_PARAMS['total_numbers'], len(probabilities))):
                num = i + 1
                if len(probabilities[i][0]) > 1:
                    prob = probabilities[i][0][1]
                else:
                    prob = 0.5
                number_probs.append((num, prob))

            number_probs.sort(key=lambda x: x[1], reverse=True)
            top30 = number_probs[:30]

            self._log(f"‚úÖ TOP 30 ML calcul√© - Probabilit√© moyenne: {np.mean([prob for _, prob in top30]):.4f}")
            return top30

        except Exception as e:
            self._log(f"‚ùå Erreur lors du calcul TOP 30 ML: {e}", "ERROR")
            import traceback
            self._log(f"D√©tails: {traceback.format_exc()}")
            return []

    def get_top30_numbers_advanced(self) -> list:
        """
        S√©lectionne les 30 meilleurs num√©ros selon un score composite avanc√©
        (fr√©quence globale, fr√©quence r√©cente, retard, tendance, score ML, paires/trios)
        """
        if not self.stats:
            self._log("Stats non disponibles, impossible de calculer le TOP 30.", "ERROR")
            return []
        scores = {}
        max_freq = max(self.stats.frequences.values()) if self.stats.frequences else 1
        max_freq_recent = max(self.stats.frequences_recentes.values()) if self.stats.frequences_recentes else 1
        max_retard = max(self.stats.retards.values()) if self.stats.retards else 1
        max_tendance = max(self.stats.tendances_50.values()) if self.stats.tendances_50 else 1.0

        # Pond√©rations dynamiques (exemple)
        ml_weight = 0.15
        freq_weight = 0.25
        recent_weight = 0.20
        retard_weight = 0.15
        tendance_weight = 0.15
        pair_weight = 0.05
        trio_weight = 0.05
        lift_weight = 0.10
        diversity_penalty = 0.10

        # Calcul du lift des paires
        def lift(i, j):
            cofreq = self.stats.paires_freq.get(tuple(sorted([i, j])), 0)
            freq_i = self.stats.frequences.get(i, 1)
            freq_j = self.stats.frequences.get(j, 1)
            return cofreq / (freq_i * freq_j) if freq_i and freq_j else 0

        for num in range(1, KENO_PARAMS['total_numbers'] + 1):
            score = 0.0
            score += (self.stats.frequences.get(num, 0) / max_freq) * freq_weight
            score += (self.stats.frequences_recentes.get(num, 0) / max_freq_recent) * recent_weight
            score += (1 - self.stats.retards.get(num, 0) / max_retard) * retard_weight
            score += (self.stats.tendances_50.get(num, 1.0) / max_tendance) * tendance_weight
            if hasattr(self, 'ml_scores') and self.ml_scores and num in self.ml_scores:
                score += self.ml_scores[num] * ml_weight

            # Bonus lift sur les paires
            lift_score = 0
            for other in range(1, 71):
                if other != num:
                    lift_score += lift(num, other)
            score += (lift_score / 69) * lift_weight

            # Bonus pour les paires/trios fr√©quents
            pair_bonus = sum([self.stats.paires_freq.get(tuple(sorted([num, other])), 0) for other in range(1, 71) if other != num])
            score += (pair_bonus / 1000) * pair_weight
            trio_bonus = sum([self.stats.trios_freq.get(tuple(sorted([num, other1, other2]),), 0)
                              for other1 in range(1, 71) for other2 in range(other1+1, 71)
                              if other1 != num and other2 != num])
            score += (trio_bonus / 5000) * trio_weight

            # Diversit√©‚ÄØ: p√©nalit√© si trop de num√©ros dans la m√™me dizaine ou m√™me parit√©
            dizaine = (num - 1) // 10
            parite = num % 2
            same_dizaine = sum(1 for other in range(1, 71) if other != num and (other - 1) // 10 == dizaine)
            same_parite = sum(1 for other in range(1, 71) if other != num and other % 2 == parite)
            score -= ((same_dizaine / 69) + (same_parite / 69)) * diversity_penalty

            scores[num] = score

        top30 = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:30]
        return [num for num, score in top30]
        
    def run_full_pipeline(self, num_grids: int = 40, profile: str = "balanced"):
        """
        Pipeline complet : chargement des donn√©es, analyse, entra√Ænement ML, g√©n√©ration des grilles.
        """
        self._log("üöÄ D√©marrage du pipeline complet Keno...")
        if not self.load_data():
            self._log("‚ùå Chargement des donn√©es impossible.", "ERROR")
            return
        self.stats = self.analyze_patterns()
        self.train_xgboost_models(retrain=False)
        self.load_ml_models()
        self._log("‚úÖ Pipeline complet termin√©.")

    def generate_optimized_grids(self, num_grids: int = 40) -> list:
        """
        G√©n√®re des grilles optimis√©es en privil√©giant le TOP 30 ML.
        """
        self._log(f"üéØ G√©n√©ration de {num_grids} grilles Keno optimis√©es (TOP 30 ML privil√©gi√©, {self.grid_size} num√©ros/grille)...")
        top30_ml = [num for num, _ in self.predict_numbers_ml()]
        if not top30_ml or len(top30_ml) < self.grid_size:
            self._log("‚ùå TOP 30 ML indisponible, g√©n√©ration al√©atoire.", "ERROR")
            # Fallback : g√©n√©ration al√©atoire
            return [sorted(random.sample(range(1, 71), self.grid_size)) for _ in range(num_grids)]
        grids = []
        for _ in range(num_grids):
            grid = sorted(random.sample(top30_ml, self.grid_size))
            grids.append(grid)
        self._log(f"‚úÖ {len(grids)} grilles de {self.grid_size} num√©ros g√©n√©r√©es √† partir du TOP 30 ML")
        return grids

    def save_results(self, grids: list):
        """Sauvegarde les grilles g√©n√©r√©es dans un fichier CSV avec colonnes adapt√©es √† la taille"""
        output_path = self.output_dir / "grilles_keno.csv"
        columns = [f"numero_{i}" for i in range(1, self.grid_size + 1)]
        df = pd.DataFrame(grids, columns=columns)
        df.to_csv(output_path, index=False)
        self._log(f"üíæ Grilles de {self.grid_size} num√©ros sauvegard√©es dans {output_path}")

    def save_top30_ml_csv(self):
        """Sauvegarde le TOP 30 ML dans un fichier CSV"""
        top30_ml = [num for num, _ in self.predict_numbers_ml()]
        output_path = self.output_dir / "top30_ml.csv"
        df = pd.DataFrame(top30_ml, columns=["Num√©ro"])
        df.to_csv(output_path, index=False)
        self._log(f"üíæ TOP 30 ML sauvegard√© dans {output_path}")

    def generate_report(self, grids: list) -> str:
        """
        G√©n√®re un rapport d√©taill√© sur les grilles produites.
        """
        report = "# Rapport d√©taill√© des grilles Keno\n\n"
        report += f"Nombre de grilles g√©n√©r√©es : {len(grids)}\n\n"
        report += "## Grilles\n"
        for idx, grid in enumerate(grids, 1):
            report += f"- Grille {idx}: {grid}\n"
        report += "\n"
        # Statistiques sur la diversit√© des num√©ros
        all_numbers = [num for grid in grids for num in grid]
        unique_numbers = set(all_numbers)
        report += f"Nombre total de num√©ros uniques utilis√©s : {len(unique_numbers)}\n"
        report += f"Liste des num√©ros uniques : {sorted(unique_numbers)}\n"
        return report

    def update_and_retrain(self):
        """
        Recharge les donn√©es, analyse, et r√©entra√Æne le mod√®le ML sur tous les tirages.
        """
        self._log("üîÑ Mise √† jour des donn√©es et r√©entra√Ænement du mod√®le ML...")
        self.load_data()
        self.stats = self.analyze_patterns()
        self.train_xgboost_models(retrain=True)
        self.load_ml_models()
        self._log("‚úÖ Mod√®le ML r√©entra√Æn√© avec les nouveaux tirages.")

    def evaluate_grids_with_model(self, grids: list) -> list:
        """
        √âvalue chaque grille g√©n√©r√©e avec le mod√®le ML et retourne les scores/probabilit√©s.
        """
        if 'multilabel' not in self.ml_models:
            self._log("‚ùå Mod√®le ML non disponible pour l'√©valuation.", "ERROR")
            return []
        model = self.ml_models['multilabel']
        scores = []
        for grid in grids:
            current_date = pd.Timestamp.now()
            # Adapter le DataFrame aux grilles de taille variable
            df_data = {'date_de_tirage': [current_date]}
            for i in range(1, 21):
                if i <= len(grid):
                    df_data[f'boule{i}'] = [grid[i-1]]
                else:
                    df_data[f'boule{i}'] = [0]  # Padding avec des z√©ros
            
            df = pd.DataFrame(df_data)

            # Ajout des features temporelles ET enrichies (comme √† l'entra√Ænement)
            df_features = self.add_cyclic_features(df)
            df_features = self.enrich_features(df_features)

            # Reconstruction des features d'historique - EXACTEMENT comme √† l'entra√Ænement
            lag_features = {}
            if self.stats and self.stats.derniers_tirages:
                for lag in range(1, 6):
                    for ball_num in range(1, 21):
                        col_name = f'lag{lag}_boule{ball_num}'
                        if lag <= len(self.stats.derniers_tirages):
                            last_draw = self.stats.derniers_tirages[-(lag)]
                            lag_features[col_name] = last_draw[ball_num - 1] if ball_num <= len(last_draw) else 0
                        else:
                            lag_features[col_name] = 0
                lag_df = pd.DataFrame([lag_features], index=df_features.index)
            else:
                # Valeurs par d√©faut si pas d'historique
                lag_features = {f'lag{lag}_boule{ball_num}': 0 for lag in range(1, 6) for ball_num in range(1, 21)}
                lag_df = pd.DataFrame([lag_features], index=df_features.index)

            df_features = pd.concat([df_features, lag_df], axis=1)

            # Features de fr√©quence par zone - EXACTEMENT comme √† l'entra√Ænement
            zone_features = {
                'zone1_count': [],
                'zone2_count': [],
                'zone3_count': [],
                'zone4_count': [],
                'zone1_freq': [],
                'zone2_freq': [],
                'zone3_freq': [],
                'zone4_freq': []
            }
            
            for idx, row in df_features.iterrows():
                draw_numbers = [int(row[f'boule{i}']) for i in range(1, 21) if row[f'boule{i}'] > 0]
                zone_features['zone1_count'].append(sum(1 for n in draw_numbers if 1 <= n <= 17))
                zone_features['zone2_count'].append(sum(1 for n in draw_numbers if 18 <= n <= 35))
                zone_features['zone3_count'].append(sum(1 for n in draw_numbers if 36 <= n <= 52))
                zone_features['zone4_count'].append(sum(1 for n in draw_numbers if 53 <= n <= 70))
                zone_features['zone1_freq'].append(0)
                zone_features['zone2_freq'].append(0)
                zone_features['zone3_freq'].append(0)
                zone_features['zone4_freq'].append(0)

            # Ajout des features de zones
            for zone_name, zone_values in zone_features.items():
                if len(zone_values) != len(df_features):
                    zone_values = [0] * len(df_features)
                df_features[zone_name] = zone_values

            # AJOUT CRUCIAL: Features statistiques √©tendues comme √† l'entra√Ænement
            if self.stats:
                stats_data = {
                    f'freq_recent_{num}': self.stats.frequences_recentes.get(num, 0)
                    for num in range(1, 71)
                }
                stats_data.update({
                    f'retard_{num}': self.stats.retards.get(num, 0)
                    for num in range(1, 71)
                })
                stats_data.update({
                    f'tendance_{num}': self.stats.tendances_50.get(num, 1.0)
                    for num in range(1, 71)
                })
                stats_df = pd.DataFrame([stats_data], index=df_features.index)
                df_features = pd.concat([df_features, stats_df], axis=1)
            else:
                # Cr√©er des valeurs par d√©faut pour toutes les features statistiques
                stats_data = {}
                for num in range(1, 71):
                    stats_data[f'freq_recent_{num}'] = 0
                    stats_data[f'retard_{num}'] = 0
                    stats_data[f'tendance_{num}'] = 1.0
                stats_df = pd.DataFrame([stats_data], index=df_features.index)
                df_features = pd.concat([df_features, stats_df], axis=1)

            # Utiliser la liste de features de l'entra√Ænement
            feature_cols = self.metadata.get('feature_names', [])

            # CORRECTION D√âFINITIVE: Ajouter toutes les colonnes manquantes avec 0.0
            for c in feature_cols:
                if c not in df_features.columns:
                    df_features[c] = 0.0

            # Supprimer les colonnes en trop et r√©ordonner exactement comme √† l'entra√Ænement
            df_features = df_features.loc[:, feature_cols]

            # V√©rification finale du nombre de features
            if df_features.shape[1] != len(feature_cols):
                self._log(f"‚ö†Ô∏è  Mismatch: {df_features.shape[1]} features vs {len(feature_cols)} attendues", "ERROR")
                scores.append((grid, 0.0))
                continue

            X_eval = df_features.fillna(0)
            X_eval = X_eval.select_dtypes(include=[np.number])

            # Normalisation des features
            from sklearn.preprocessing import StandardScaler
            scaler = StandardScaler()
            X_eval_scaled = scaler.fit_transform(X_eval)

            # Pr√©diction des probabilit√©s pour tous les num√©ros
            probabilities = model.predict_proba(X_eval_scaled)

            # Calcul du score moyen de la grille
            grid_score = 0.0
            count = 0
            for num in grid:
                if 1 <= num <= KENO_PARAMS['total_numbers']:
                    idx = num - 1  # Index 0-69 pour num√©ros 1-70
                    if idx < len(probabilities):
                        if len(probabilities[idx]) > 0 and len(probabilities[idx][0]) > 1:
                            prob = probabilities[idx][0][1]
                            grid_score += prob
                            count += 1
            
            if count > 0:
                grid_score /= count
            
            scores.append((grid, grid_score))

        return scores

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="G√©n√©rateur avanc√© de grilles Keno")
    parser.add_argument("--n", type=int, default=10, help="Nombre de grilles √† g√©n√©rer")
    parser.add_argument("--grids", type=int, help="Alias pour --n (nombre de grilles √† g√©n√©rer)")
    parser.add_argument("--size", type=int, default=10, choices=[7, 8, 9, 10], help="Taille des grilles (7 √† 10 num√©ros)")
    parser.add_argument("--profile", type=str, default="balanced", help="Profil d'entra√Ænement ML (quick, balanced, comprehensive, intensive)")
    parser.add_argument("--data", type=str, default=None, help="Chemin du fichier de donn√©es Keno")
    parser.add_argument("--silent", action="store_true", help="Mode silencieux")
    parser.add_argument("--retrain", action="store_true", help="Forcer le r√©entra√Ænement du mod√®le ML")
    parser.add_argument("--save-top30-ml", action="store_true", help="Sauvegarder le TOP 30 ML dans un CSV")
    parser.add_argument("--test-grids", action="store_true", help="√âvaluer les grilles g√©n√©r√©es avec le mod√®le ML")
    args = parser.parse_args()

    # Gestion de l'alias --grids
    num_grids = args.grids if args.grids is not None else args.n

    generator = KenoGeneratorAdvanced(
        data_path=args.data,
        silent=args.silent,
        training_profile=args.profile,
        grid_size=args.size
    )

    if args.retrain:
        generator.update_and_retrain()
    else:
        generator.run_full_pipeline(num_grids=num_grids, profile=args.profile)

    grids = generator.generate_optimized_grids(num_grids=num_grids)
    generator.save_results(grids)

    if args.save_top30_ml:
        generator.save_top30_ml_csv()

    if args.test_grids:
        grid_scores = generator.evaluate_grids_with_model(grids)
        print("Scores des grilles g√©n√©r√©es :")
        for i, score in enumerate(grid_scores, 1):
            print(f"Grille {i}: {score}")

    print(generator.generate_report(grids))
