#!/usr/bin/env python3
"""
============================================================================
üé≤ G√âN√âRATEUR INTELLIGENT DE GRILLES KENO - VERSION AVANC√âE üé≤
============================================================================

G√©n√©rateur de grilles Keno utilisant l'apprentissage automatique (XGBoost) 
et l'analyse statistique pour optimiser les combinaisons.

Caract√©ristiques:
- Machine Learning avec XGBoost pour pr√©dire les num√©ros probables
- Strat√©gie adaptative avec pond√©ration dynamique ML/Fr√©quence  
- Analyse de patterns temporels et cycliques
- Optimisation des combinaisons selon crit√®res statistiques
- G√©n√©ration de rapports d√©taill√©s avec visualisations

Auteur: Assistant IA
Version: 2.0
Date: Ao√ªt 2025
============================================================================
"""

import pandas as pd
import numpy as np
import random
import argparse
import sys
import time
import warnings
from datetime import datetime, timedelta
from pathlib import Path
import os
import json
import pickle
from typing import List, Tuple, Dict, Optional, Any
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
import multiprocessing as mp
from dataclasses import dataclass
from tqdm import tqdm
import logging

# DuckDB pour optimiser les requ√™tes
try:
    import duckdb
    HAS_DUCKDB = True
except ImportError:
    HAS_DUCKDB = False
    print("‚ö†Ô∏è  DuckDB non disponible. Utilisation de Pandas uniquement.")

# ML et analyse
try:
    import xgboost as xgb
    from sklearn.model_selection import train_test_split, cross_val_score
    from sklearn.metrics import accuracy_score, classification_report
    from sklearn.preprocessing import StandardScaler
    HAS_ML = True
except ImportError:
    HAS_ML = False
    print("‚ö†Ô∏è  Modules ML non disponibles. Mode fr√©quence uniquement.")

# Visualisation et analyse
try:
    import matplotlib.pyplot as plt
    import seaborn as sns
    from scipy import stats
    from scipy.fft import fft, fftfreq
    from scipy.signal import find_peaks
    from statsmodels.tsa.seasonal import STL
    from statsmodels.tsa.stattools import acf
    HAS_VIZ = True
except ImportError:
    HAS_VIZ = False
    print("‚ö†Ô∏è  Modules de visualisation non disponibles.")

# Configuration des warnings
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)

# ==============================================================================
# üîß CONFIGURATION ET CONSTANTES
# ==============================================================================

# Param√®tres sp√©cifiques au Keno
KENO_PARAMS = {
    'total_numbers': 70,        # Num√©ros de 1 √† 70
    'numbers_per_draw': 20,     # 20 num√©ros tir√©s par tirage
    'player_selection': 10,     # Le joueur s√©lectionne typiquement 10 num√©ros
    'min_selection': 2,         # Minimum de num√©ros s√©lectionnables
    'max_selection': 10,        # Maximum de num√©ros s√©lectionnables
}

# Chemins des fichiers
BASE_DIR = Path(__file__).parent
DATA_DIR = BASE_DIR / "keno_data"
MODELS_DIR = BASE_DIR.parent / "keno_models"
OUTPUT_DIR = BASE_DIR.parent / "keno_output"

# Configuration ML
ML_CONFIG = {
    'n_estimators': 100,
    'max_depth': 6,
    'learning_rate': 0.1,
    'random_state': 42,
    'n_jobs': -1,
    'objective': 'binary:logistic'
}

def get_training_params(profile: str) -> dict:
    """
    Retourne les param√®tres d'entra√Ænement selon le profil s√©lectionn√©
    
    Args:
        profile: Profil d'entra√Ænement ('quick', 'balanced', 'comprehensive', 'intensive')
    
    Returns:
        dict: Param√®tres d'entra√Ænement pour RandomForest
    """
    if profile == "quick":
        return {
            'n_estimators': 50,        # Arbres r√©duits pour la vitesse
            'max_depth': 8,            # Profondeur mod√©r√©e
            'min_samples_split': 10,   # √âviter l'overfitting
            'min_samples_leaf': 5,     # √âviter l'overfitting
            'max_features': 'sqrt',    # Features r√©duites
            'random_state': 42,
            'n_jobs': -1,
            'class_weight': 'balanced'
        }
    elif profile == "balanced":
        return {
            'n_estimators': 100,       # Standard
            'max_depth': 12,           # Profondeur √©quilibr√©e
            'min_samples_split': 5,    # Standard
            'min_samples_leaf': 3,     # Standard
            'max_features': 'sqrt',    # Standard
            'random_state': 42,
            'n_jobs': -1,
            'class_weight': 'balanced'
        }
    elif profile == "comprehensive":
        return {
            'n_estimators': 200,       # Plus d'arbres pour meilleure pr√©cision
            'max_depth': 15,           # Profondeur √©lev√©e
            'min_samples_split': 4,    # Plus de splits
            'min_samples_leaf': 2,     # Feuilles plus petites
            'max_features': 'log2',    # Plus de features
            'random_state': 42,
            'n_jobs': -1,
            'class_weight': 'balanced'
        }
    elif profile == "intensive":
        return {
            'n_estimators': 300,       # Maximum d'arbres
            'max_depth': 20,           # Profondeur maximale
            'min_samples_split': 3,    # Splits agressifs
            'min_samples_leaf': 1,     # Feuilles minimales
            'max_features': None,      # Toutes les features
            'random_state': 42,
            'n_jobs': -1,
            'class_weight': 'balanced'
        }
    else:
        # Fallback sur balanced
        return get_training_params("balanced")

@dataclass
class KenoStats:
    """Structure pour stocker les statistiques Keno compl√®tes"""
    # 1. Fr√©quences d'apparition des num√©ros
    frequences: Dict[int, int]                           # Fr√©quence globale
    frequences_recentes: Dict[int, int]                  # Fr√©quence sur 100 derniers tirages
    frequences_50: Dict[int, int]                        # Fr√©quence sur 50 derniers tirages
    frequences_20: Dict[int, int]                        # Fr√©quence sur 20 derniers tirages
    
    # 2. Num√©ros en retard (overdue numbers)
    retards: Dict[int, int]                              # Retard actuel de chaque num√©ro
    retards_historiques: Dict[int, List[int]]            # Historique des retards
    
    # 3. Combinaisons et patterns r√©currents
    paires_freq: Dict[Tuple[int, int], int]              # Paires fr√©quentes
    trios_freq: Dict[Tuple[int, int, int], int]          # Trios fr√©quents
    patterns_parit√©: Dict[str, int]                      # Distribution pair/impair
    patterns_sommes: Dict[int, int]                      # Distribution des sommes
    patterns_zones: Dict[str, int]                       # R√©partition par zones/dizaines
    
    # 4. Analyse par p√©riode
    tendances_10: Dict[int, float]                       # Tendances sur 10 tirages
    tendances_50: Dict[int, float]                       # Tendances sur 50 tirages  
    tendances_100: Dict[int, float]                      # Tendances sur 100 tirages
    
    # Donn√©es brutes pour analyses avanc√©es
    zones_freq: Dict[str, int]                           # Ancien format maintenu pour compatibilit√©
    derniers_tirages: List[List[int]]                    # Derniers tirages
    tous_tirages: List[List[int]]                        # Tous les tirages pour DuckDB

# ==============================================================================
# üéØ CLASSE PRINCIPALE GENERATEUR KENO
# ==============================================================================

class KenoGeneratorAdvanced:
    """G√©n√©rateur avanc√© de grilles Keno avec ML et analyse statistique"""
    
    def __init__(self, data_path: str = None, silent: bool = False, training_profile: str = "balanced"):
        """
        Initialise le g√©n√©rateur Keno
        
        Args:
            data_path: Chemin vers les donn√©es historiques
            silent: Mode silencieux pour r√©duire les sorties
            training_profile: Profil d'entra√Ænement ('quick', 'balanced', 'comprehensive', 'intensive')
        """
        self.silent = silent
        self.data_path = data_path or str(DATA_DIR / "keno_202010.parquet")
        self.models_dir = MODELS_DIR
        self.output_dir = OUTPUT_DIR
        self.training_profile = training_profile
        
        # Cr√©er les r√©pertoires n√©cessaires
        self.models_dir.mkdir(exist_ok=True)
        self.output_dir.mkdir(exist_ok=True)
        
        # Initialisation des composants
        self.data = None
        self.stats = None
        self.ml_models = {}
        self.metadata = {}
        self.cache = {}
        
        # Strat√©gie adaptative
        self.adaptive_weights = {
            'ml_weight': 0.6,      # 60% ML par d√©faut
            'freq_weight': 0.4,    # 40% fr√©quence par d√©faut  
            'performance_history': [],
            'last_update': datetime.now()
        }
        
        self._log("üé≤ G√©n√©rateur Keno Avanc√© v2.0 initialis√©")
        
    def _log(self, message: str, level: str = "INFO"):
        """Syst√®me de logging configur√©"""
        if not self.silent or level == "ERROR":
            print(f"{message}")
    
    def load_data(self) -> bool:
        """
        Charge les donn√©es historiques des tirages Keno
        
        Returns:
            bool: True si les donn√©es sont charg√©es avec succ√®s
        """
        try:
            self._log("üìä Chargement des donn√©es historiques Keno...")
            
            if not Path(self.data_path).exists():
                self._log(f"‚ùå Fichier non trouv√©: {self.data_path}", "ERROR")
                return False
                
            # Chargement depuis Parquet
            self.data = pd.read_parquet(self.data_path)
            
            # Validation des colonnes requises
            required_cols = ['date_de_tirage'] + [f'boule{i}' for i in range(1, 21)]
            missing_cols = [col for col in required_cols if col not in self.data.columns]
            
            if missing_cols:
                self._log(f"‚ùå Colonnes manquantes: {missing_cols}", "ERROR")
                return False
            
            # Tri par date
            self.data = self.data.sort_values('date_de_tirage').reset_index(drop=True)
            
            self._log(f"‚úÖ {len(self.data)} tirages Keno charg√©s ({self.data['date_de_tirage'].min()} √† {self.data['date_de_tirage'].max()})")
            return True
            
        except Exception as e:
            self._log(f"‚ùå Erreur lors du chargement des donn√©es: {e}", "ERROR")
            return False
    
    def analyze_patterns(self) -> KenoStats:
        """
        Analyse compl√®te des patterns et statistiques Keno avec DuckDB
        
        Returns:
            KenoStats: Statistiques compl√®tes calcul√©es
        """
        self._log("üîç Analyse compl√®te des patterns Keno avec DuckDB...")
        
        # Extraction des num√©ros de tous les tirages
        all_draws = []
        for _, row in self.data.iterrows():
            # Support des diff√©rents formats de colonnes
            if 'b1' in self.data.columns:
                draw = [int(row[f'b{i}']) for i in range(1, 21)]
            else:
                draw = [int(row[f'boule{i}']) for i in range(1, 21)]
            all_draws.append(sorted(draw))
        
        self._log(f"üìä Analyse de {len(all_draws)} tirages")
        
        if HAS_DUCKDB:
            return self._analyze_with_duckdb(all_draws)
        else:
            return self._analyze_with_pandas(all_draws)
    
    def _analyze_with_duckdb(self, all_draws: List[List[int]]) -> KenoStats:
        """Analyse optimis√©e avec DuckDB"""
        self._log("üöÄ Utilisation de DuckDB pour l'analyse optimis√©e")
        
        # Cr√©er une connexion DuckDB
        conn = duckdb.connect(':memory:')
        
        # Pr√©parer les donn√©es pour DuckDB
        draws_data = []
        for i, draw in enumerate(all_draws):
            for num in draw:
                draws_data.append({'tirage_id': i, 'numero': num, 'position': draw.index(num)})
        
        draws_df = pd.DataFrame(draws_data)
        
        # Cr√©er la table dans DuckDB
        conn.register('tirages', draws_df)
        
        # 1. FR√âQUENCES D'APPARITION
        self._log("üìä Calcul des fr√©quences d'apparition...")
        
        # Fr√©quence globale
        freq_global = conn.execute("""
            SELECT numero, COUNT(*) as freq 
            FROM tirages 
            GROUP BY numero 
            ORDER BY numero
        """).fetchdf()
        frequences = dict(zip(freq_global['numero'], freq_global['freq']))
        
        # Fr√©quences r√©centes (100, 50, 20 derniers tirages)
        max_tirage = len(all_draws) - 1
        
        freq_100 = conn.execute(f"""
            SELECT numero, COUNT(*) as freq 
            FROM tirages 
            WHERE tirage_id >= {max(0, max_tirage - 99)}
            GROUP BY numero
        """).fetchdf()
        frequences_100 = dict(zip(freq_100['numero'], freq_100['freq']))
        
        freq_50 = conn.execute(f"""
            SELECT numero, COUNT(*) as freq 
            FROM tirages 
            WHERE tirage_id >= {max(0, max_tirage - 49)}
            GROUP BY numero
        """).fetchdf()
        frequences_50 = dict(zip(freq_50['numero'], freq_50['freq']))
        
        freq_20 = conn.execute(f"""
            SELECT numero, COUNT(*) as freq 
            FROM tirages 
            WHERE tirage_id >= {max(0, max_tirage - 19)}
            GROUP BY numero
        """).fetchdf()
        frequences_20 = dict(zip(freq_20['numero'], freq_20['freq']))
        
        # 2. CALCUL DES RETARDS
        self._log("‚è∞ Calcul des retards des num√©ros...")
        retards = {}
        retards_historiques = {}
        
        for num in range(1, KENO_PARAMS['total_numbers'] + 1):
            # Retard actuel
            retard = 0
            for draw in reversed(all_draws):
                if num in draw:
                    break
                retard += 1
            retards[num] = retard
            
            # Historique des retards
            historique = []
            dernier_tirage = -1
            for i, draw in enumerate(all_draws):
                if num in draw:
                    if dernier_tirage >= 0:
                        historique.append(i - dernier_tirage - 1)
                    dernier_tirage = i
            retards_historiques[num] = historique
        
        # 3. PATTERNS ET COMBINAISONS
        self._log("üîó Analyse des patterns et combinaisons...")
        
        # Paires fr√©quentes
        paires_freq = {}
        for draw in all_draws:
            for i in range(len(draw)):
                for j in range(i + 1, len(draw)):
                    pair = tuple(sorted([draw[i], draw[j]]))
                    paires_freq[pair] = paires_freq.get(pair, 0) + 1
        
        # Trios fr√©quents (limit√© aux plus fr√©quents pour performance)
        trios_freq = {}
        for draw in all_draws[-1000:]:  # Seulement les 1000 derniers pour performance
            for i in range(len(draw)):
                for j in range(i + 1, len(draw)):
                    for k in range(j + 1, len(draw)):
                        trio = tuple(sorted([draw[i], draw[j], draw[k]]))
                        trios_freq[trio] = trios_freq.get(trio, 0) + 1
        
        # Patterns de parit√©
        patterns_parit√© = {"tout_pair": 0, "tout_impair": 0, "mixte": 0}
        for draw in all_draws:
            pairs = sum(1 for num in draw if num % 2 == 0)
            if pairs == len(draw):
                patterns_parit√©["tout_pair"] += 1
            elif pairs == 0:
                patterns_parit√©["tout_impair"] += 1
            else:
                patterns_parit√©["mixte"] += 1
        
        # Patterns de sommes
        patterns_sommes = {}
        for draw in all_draws:
            somme = sum(draw)
            patterns_sommes[somme] = patterns_sommes.get(somme, 0) + 1
        
        # Patterns de zones/dizaines
        patterns_zones = {
            "zone_1_17": 0, "zone_18_35": 0, "zone_36_52": 0, "zone_53_70": 0,
            "dizaine_1_10": 0, "dizaine_11_20": 0, "dizaine_21_30": 0, 
            "dizaine_31_40": 0, "dizaine_41_50": 0, "dizaine_51_60": 0, "dizaine_61_70": 0
        }
        
        for draw in all_draws:
            # Comptage par zones
            for num in draw:
                if 1 <= num <= 17:
                    patterns_zones["zone_1_17"] += 1
                elif 18 <= num <= 35:
                    patterns_zones["zone_18_35"] += 1
                elif 36 <= num <= 52:
                    patterns_zones["zone_36_52"] += 1
                else:  # 53-70
                    patterns_zones["zone_53_70"] += 1
                
                # Comptage par dizaines
                dizaine = (num - 1) // 10 + 1
                if dizaine <= 7:
                    patterns_zones[f"dizaine_{(dizaine-1)*10+1}_{dizaine*10}"] += 1
        
        # 4. ANALYSE PAR P√âRIODE ET TENDANCES
        self._log("üìà Calcul des tendances par p√©riode...")
        
        tendances_10 = self._calculer_tendances(all_draws, 10)
        tendances_50 = self._calculer_tendances(all_draws, 50)
        tendances_100 = self._calculer_tendances(all_draws, 100)
        
        # Zones compatibilit√© (ancien format)
        zones_freq = {
            "zone1_17": patterns_zones["zone_1_17"],
            "zone18_35": patterns_zones["zone_18_35"], 
            "zone36_52": patterns_zones["zone_36_52"],
            "zone53_70": patterns_zones["zone_53_70"]
        }
        
        # Garder les derniers tirages
        derniers_tirages = all_draws[-50:] if len(all_draws) >= 50 else all_draws
        
        conn.close()
        
        # Initialiser avec des valeurs par d√©faut pour les champs manquants
        for num in range(1, KENO_PARAMS['total_numbers'] + 1):
            if num not in frequences:
                frequences[num] = 0
            if num not in frequences_100:
                frequences_100[num] = 0
            if num not in frequences_50:
                frequences_50[num] = 0
            if num not in frequences_20:
                frequences_20[num] = 0
        
        self.stats = KenoStats(
            frequences=frequences,
            frequences_recentes=frequences_100,
            frequences_50=frequences_50,
            frequences_20=frequences_20,
            retards=retards,
            retards_historiques=retards_historiques,
            paires_freq=paires_freq,
            trios_freq=trios_freq,
            patterns_parit√©=patterns_parit√©,
            patterns_sommes=patterns_sommes,
            patterns_zones=patterns_zones,
            tendances_10=tendances_10,
            tendances_50=tendances_50,
            tendances_100=tendances_100,
            zones_freq=zones_freq,
            derniers_tirages=derniers_tirages,
            tous_tirages=all_draws
        )
        
        self._log(f"‚úÖ Analyse DuckDB termin√©e - {len(all_draws)} tirages analys√©s")
        return self.stats
    
    def _analyze_with_pandas(self, all_draws: List[List[int]]) -> KenoStats:
        """Analyse de fallback avec Pandas seulement"""
        self._log("‚ö†Ô∏è  Analyse de fallback avec Pandas (DuckDB non disponible)")
        
        # Impl√©mentation simplifi√©e pour compatibilit√©
        frequences = {i: 0 for i in range(1, KENO_PARAMS['total_numbers'] + 1)}
        retards = {i: 0 for i in range(1, KENO_PARAMS['total_numbers'] + 1)}
        paires_freq = {}
        zones_freq = {"zone1_17": 0, "zone18_35": 0, "zone36_52": 0, "zone53_70": 0}
        
        # Comptage basique des fr√©quences
        for draw in all_draws:
            for num in draw:
                frequences[num] += 1
                
                # Zones
                if 1 <= num <= 17:
                    zones_freq["zone1_17"] += 1
                elif 18 <= num <= 35:
                    zones_freq["zone18_35"] += 1
                elif 36 <= num <= 52:
                    zones_freq["zone36_52"] += 1
                else:
                    zones_freq["zone53_70"] += 1
        
        # Calcul des retards
        for num in range(1, KENO_PARAMS['total_numbers'] + 1):
            retard = 0
            for draw in reversed(all_draws):
                if num in draw:
                    break
                retard += 1
            retards[num] = retard
        
        # Paires basiques
        for draw in all_draws:
            for i in range(len(draw)):
                for j in range(i + 1, len(draw)):
                    pair = tuple(sorted([draw[i], draw[j]]))
                    paires_freq[pair] = paires_freq.get(pair, 0) + 1
        
        derniers_tirages = all_draws[-50:] if len(all_draws) >= 50 else all_draws
        
        # Initialiser avec des valeurs par d√©faut
        self.stats = KenoStats(
            frequences=frequences,
            frequences_recentes=frequences.copy(),
            frequences_50=frequences.copy(),
            frequences_20=frequences.copy(),
            retards=retards,
            retards_historiques={i: [] for i in range(1, 71)},
            paires_freq=paires_freq,
            trios_freq={},
            patterns_parit√©={"tout_pair": 0, "tout_impair": 0, "mixte": len(all_draws)},
            patterns_sommes={},
            patterns_zones={},
            tendances_10={i: 0.0 for i in range(1, 71)},
            tendances_50={i: 0.0 for i in range(1, 71)},
            tendances_100={i: 0.0 for i in range(1, 71)},
            zones_freq=zones_freq,
            derniers_tirages=derniers_tirages,
            tous_tirages=all_draws
        )
        
        self._log(f"‚úÖ Analyse Pandas termin√©e - {len(all_draws)} tirages analys√©s")
        return self.stats
    
    def _calculer_tendances(self, all_draws: List[List[int]], periode: int) -> Dict[int, float]:
        """Calcule les tendances d'apparition sur une p√©riode donn√©e"""
        if len(all_draws) < periode:
            return {i: 0.0 for i in range(1, KENO_PARAMS['total_numbers'] + 1)}
        
        tendances = {}
        for num in range(1, KENO_PARAMS['total_numbers'] + 1):
            # Fr√©quence r√©cente vs fr√©quence historique
            recent_draws = all_draws[-periode:]
            freq_recent = sum(1 for draw in recent_draws if num in draw)
            freq_moyenne = freq_recent / periode
            
            # Fr√©quence historique
            freq_historique = sum(1 for draw in all_draws[:-periode] if num in draw)
            if len(all_draws) > periode:
                freq_historique_moyenne = freq_historique / (len(all_draws) - periode)
            else:
                freq_historique_moyenne = freq_moyenne
            
            # Tendance = ratio r√©cent/historique
            if freq_historique_moyenne > 0:
                tendances[num] = freq_moyenne / freq_historique_moyenne
            else:
                tendances[num] = freq_moyenne * 2  # Bonus si nouveau num√©ro
        
        return tendances
    
    def add_cyclic_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Ajoute des features cycliques pour l'apprentissage automatique
        
        Args:
            df: DataFrame avec les donn√©es
            
        Returns:
            DataFrame avec les features ajout√©es
        """
        df = df.copy()
        
        # Features temporelles cycliques
        df['day_sin'] = np.sin(2 * np.pi * df['date_de_tirage'].dt.day / 31)
        df['day_cos'] = np.cos(2 * np.pi * df['date_de_tirage'].dt.day / 31)
        df['month_sin'] = np.sin(2 * np.pi * df['date_de_tirage'].dt.month / 12)  
        df['month_cos'] = np.cos(2 * np.pi * df['date_de_tirage'].dt.month / 12)
        
        # Features cycliques pour chaque num√©ro (1-70 pour Keno)
        for i in range(1, 11):  # Pour les 10 premi√®res boules principales
            if f'boule{i}' in df.columns:
                df[f'boule{i}_sin'] = np.sin(2 * np.pi * df[f'boule{i}'] / KENO_PARAMS['total_numbers'])
                df[f'boule{i}_cos'] = np.cos(2 * np.pi * df[f'boule{i}'] / KENO_PARAMS['total_numbers'])
        
        return df
    
    def train_xgboost_models(self, retrain: bool = False) -> bool:
        """
        Entra√Æne un mod√®le XGBoost multi-label pour pr√©dire les num√©ros Keno
        
        Args:
            retrain: Force le r√©entra√Ænement m√™me si le mod√®le existe
            
        Returns:
            bool: True si l'entra√Ænement est r√©ussi
        """
        if not HAS_ML:
            self._log("‚ùå Modules ML non disponibles", "ERROR")
            return False
        
        # V√©rification du mod√®le existant (un seul mod√®le multi-label)
        model_file = self.models_dir / "xgb_keno_multilabel.pkl"
        if not retrain and model_file.exists():
            self._log("‚úÖ Mod√®le XGBoost Keno multi-label existant trouv√©")
            return self.load_ml_models()
        
        self._log("ü§ñ Entra√Ænement du mod√®le XGBoost Keno multi-label...")
        self._log("   üìä Strat√©gie: 1 mod√®le multi-label pour apprendre les corr√©lations")
        
        try:
            # Pr√©paration des donn√©es avec features enrichies (optimis√© pour √©viter la fragmentation)
            df_features = self.add_cyclic_features(self.data)
            
            # Features temporelles
            feature_cols = ['day_sin', 'day_cos', 'month_sin', 'month_cos']
            
            # Pr√©paration des features d'historique avec pd.concat pour √©viter la fragmentation
            self._log("   üìù Cr√©ation des features d'historique...")
            lag_features = {}
            
            for lag in range(1, 6):
                for ball_num in range(1, 21):
                    col_name = f'lag{lag}_boule{ball_num}'
                    if lag < len(df_features):
                        lag_features[col_name] = df_features[f'boule{ball_num}'].shift(lag).fillna(0)
                    else:
                        lag_features[col_name] = pd.Series([0] * len(df_features))
                    feature_cols.append(col_name)
            
            # Ajout des features d'historique en une seule fois
            lag_df = pd.DataFrame(lag_features, index=df_features.index)
            df_features = pd.concat([df_features, lag_df], axis=1)
            
            # Features de fr√©quence par zone (calcul√©es efficacement)
            self._log("   üìù Calcul des features de zones...")
            zone_features = {
                'zone1_count': [],
                'zone2_count': [],
                'zone3_count': [],
                'zone4_count': []
            }
            
            for idx, row in df_features.iterrows():
                draw_numbers = [int(row[f'boule{i}']) for i in range(1, 21)]
                zone_features['zone1_count'].append(sum(1 for n in draw_numbers if 1 <= n <= 17))
                zone_features['zone2_count'].append(sum(1 for n in draw_numbers if 18 <= n <= 35))
                zone_features['zone3_count'].append(sum(1 for n in draw_numbers if 36 <= n <= 52))
                zone_features['zone4_count'].append(sum(1 for n in draw_numbers if 53 <= n <= 70))
            
            # Ajout des features de zones
            for zone_name, zone_values in zone_features.items():
                df_features[zone_name] = zone_values
                feature_cols.append(zone_name)
            
            X = df_features[feature_cols].fillna(0)
            
            # Cr√©ation du target multi-label (matrice 70 colonnes, une par num√©ro)
            y = np.zeros((len(df_features), KENO_PARAMS['total_numbers']))
            
            for idx, row in df_features.iterrows():
                draw_numbers = [int(row[f'boule{i}']) for i in range(1, 21)]
                for num in draw_numbers:
                    if 1 <= num <= KENO_PARAMS['total_numbers']:
                        y[idx, num - 1] = 1  # Index 0-69 pour num√©ros 1-70
            
            self._log(f"   üìù Donn√©es pr√©par√©es: {X.shape[0]} tirages, {X.shape[1]} features")
            self._log(f"   üìù Target multi-label: {y.shape[1]} num√©ros √† pr√©dire")
            
            # Split train/test
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42
            )
            
            # Configuration sp√©ciale pour multi-label
            ml_config = ML_CONFIG.copy()
            ml_config['objective'] = 'multi:logistic'
            ml_config['num_class'] = 2  # Binaire pour chaque label
            
            # Entra√Ænement du mod√®le multi-label avec RandomForest (meilleur pour les corr√©lations)
            from sklearn.ensemble import RandomForestClassifier
            from sklearn.multioutput import MultiOutputClassifier
            
            # Obtenir les param√®tres selon le profil d'entra√Ænement
            rf_params = get_training_params(self.training_profile)
            
            # Afficher le profil utilis√©
            profile_names = {
                'quick': 'Ultra-rapide',
                'balanced': '√âquilibr√©', 
                'comprehensive': 'Complet',
                'intensive': 'Intensif'
            }
            profile_name = profile_names.get(self.training_profile, '√âquilibr√©')
            self._log(f"üìä Profil d'entra√Ænement: {profile_name} ({self.training_profile})")
            self._log(f"   ‚Ä¢ Arbres: {rf_params['n_estimators']}")
            self._log(f"   ‚Ä¢ Profondeur max: {rf_params['max_depth']}")
            self._log(f"   ‚Ä¢ Features: {rf_params['max_features']}")
            
            # RandomForest configur√© selon le profil
            base_model = RandomForestClassifier(**rf_params)
            
            # Alternative: Essayer aussi XGBoost avec MultiOutputClassifier
            # base_model = xgb.XGBClassifier(
            #     objective='binary:logistic',
            #     n_estimators=150,
            #     max_depth=8,
            #     learning_rate=0.05,
            #     random_state=42,
            #     n_jobs=-1
            # )
            
            # Mod√®le multi-output
            model = MultiOutputClassifier(base_model, n_jobs=-1)
            
            self._log("   üîÑ Entra√Ænement du mod√®le RandomForest multi-label...")
            model.fit(X_train, y_train)
            
            # √âvaluation rapide
            y_pred = model.predict(X_test)
            
            # Calcul de l'accuracy moyenne sur tous les labels
            accuracies = []
            for i in range(y.shape[1]):
                acc = accuracy_score(y_test[:, i], y_pred[:, i])
                accuracies.append(acc)
            
            mean_accuracy = np.mean(accuracies)
            self._log(f"   üìä Accuracy moyenne: {mean_accuracy:.4f}")
            
            # Sauvegarde du mod√®le
            with open(model_file, 'wb') as f:
                pickle.dump(model, f)
            
            self.ml_models['multilabel'] = model
            
            # Sauvegarde des m√©tadonn√©es
            self.metadata = {
                'features_count': len(feature_cols),
                'model_type': 'xgboost_keno_multilabel',
                'created_date': datetime.now().strftime('%Y-%m-%d'),
                'version': '3.0',  # Version avec multi-label
                'keno_params': KENO_PARAMS,
                'mean_accuracy': mean_accuracy,
                'feature_names': feature_cols
            }
            
            metadata_path = self.models_dir / "metadata.json"
            with open(metadata_path, 'w') as f:
                json.dump(self.metadata, f, indent=2)
            
            self._log("‚úÖ Entra√Ænement du mod√®le multi-label Keno termin√©")
            return True
            
        except Exception as e:
            self._log(f"‚ùå Erreur lors de l'entra√Ænement: {e}", "ERROR")
            import traceback
            self._log(f"D√©tails: {traceback.format_exc()}")
            return False
    
    def load_ml_models(self) -> bool:
        """Charge le mod√®le ML multi-label pr√©-entra√Æn√©"""
        try:
            self._log("üì• Chargement du mod√®le ML Keno multi-label...")
            
            # Chargement des m√©tadonn√©es
            metadata_path = self.models_dir / "metadata.json"
            if metadata_path.exists():
                with open(metadata_path, 'r') as f:
                    self.metadata = json.load(f)
            
            # Chargement du mod√®le multi-label
            model_path = self.models_dir / "xgb_keno_multilabel.pkl"
            if model_path.exists():
                with open(model_path, 'rb') as f:
                    self.ml_models['multilabel'] = pickle.load(f)
                
                self._log("‚úÖ Mod√®le multi-label Keno charg√© avec succ√®s")
                if 'mean_accuracy' in self.metadata:
                    self._log(f"   üìä Accuracy du mod√®le: {self.metadata['mean_accuracy']:.4f}")
                return True
            else:
                self._log("‚ùå Mod√®le multi-label non trouv√©", "ERROR")
                return False
                
        except Exception as e:
            self._log(f"‚ùå Erreur lors du chargement du mod√®le: {e}", "ERROR")
            return False
    
    def predict_numbers_ml(self, num_grids: int = 10) -> List[List[int]]:
        """
        Pr√©dit les num√©ros avec le mod√®le ML multi-label
        
        Args:
            num_grids: Nombre de grilles √† g√©n√©rer
            
        Returns:
            List[List[int]]: Liste des grilles pr√©dites
        """
        if 'multilabel' not in self.ml_models:
            self._log("‚ùå Mod√®le ML multi-label non disponible", "ERROR")
            return []
        
        try:
            predictions = []
            model = self.ml_models['multilabel']
            
            # Pr√©paration des features pour la pr√©diction
            current_date = pd.Timestamp.now()
            df_predict = pd.DataFrame({
                'date_de_tirage': [current_date],
                **{f'boule{i}': [0] for i in range(1, 21)}  # Valeurs dummy
            })
            
            # Ajout des features temporelles
            df_features = self.add_cyclic_features(df_predict)
            
            # Reconstruction des m√™mes features que lors de l'entra√Ænement (optimis√©)
            feature_cols = ['day_sin', 'day_cos', 'month_sin', 'month_cos']
            
            # Features d'historique (utiliser les derniers tirages disponibles)
            lag_features = {}
            
            if self.stats and self.stats.derniers_tirages:
                for lag in range(1, 6):
                    for ball_num in range(1, 21):
                        col_name = f'lag{lag}_boule{ball_num}'
                        # Utiliser les derniers tirages pour construire l'historique
                        if lag <= len(self.stats.derniers_tirages):
                            last_draw = self.stats.derniers_tirages[-(lag)]
                            if ball_num <= len(last_draw):
                                lag_features[col_name] = last_draw[ball_num - 1] if ball_num <= len(last_draw) else 0
                            else:
                                lag_features[col_name] = 0
                        else:
                            lag_features[col_name] = 0
                        feature_cols.append(col_name)
            else:
                # Valeurs par d√©faut si pas d'historique
                for lag in range(1, 6):
                    for ball_num in range(1, 21):
                        col_name = f'lag{lag}_boule{ball_num}'
                        lag_features[col_name] = 0
                        feature_cols.append(col_name)
            
            # Ajout des features d'historique en une fois pour √©viter la fragmentation
            lag_df = pd.DataFrame(lag_features, index=df_features.index)
            
            # Features de zones (moyennes historiques)
            zone_features = {}
            if self.stats:
                total_draws = len(self.stats.derniers_tirages) if self.stats.derniers_tirages else 1
                zone_features['zone1_count'] = self.stats.zones_freq.get("zone1_17", 0) / total_draws
                zone_features['zone2_count'] = self.stats.zones_freq.get("zone18_35", 0) / total_draws 
                zone_features['zone3_count'] = self.stats.zones_freq.get("zone36_52", 0) / total_draws
                zone_features['zone4_count'] = self.stats.zones_freq.get("zone53_70", 0) / total_draws
            else:
                zone_features['zone1_count'] = 5  # Valeurs moyennes
                zone_features['zone2_count'] = 5
                zone_features['zone3_count'] = 5
                zone_features['zone4_count'] = 5
            
            # Cr√©ation du DataFrame pour les zones
            zone_df = pd.DataFrame([zone_features], index=df_features.index)
            
            # Concat√©nation de toutes les nouvelles features en une seule fois
            df_features = pd.concat([df_features, lag_df, zone_df], axis=1)
                
            feature_cols.extend(['zone1_count', 'zone2_count', 'zone3_count', 'zone4_count'])
            
            # Pr√©paration des features pour la pr√©diction
            X_pred = df_features[feature_cols].fillna(0)
            
            # G√©n√©ration de grilles avec variation
            for grid_idx in range(num_grids):
                # Ajout de petites variations al√©atoires pour diversifier les pr√©dictions
                X_pred_variant = X_pred.copy()
                
                # Petites variations sur les features temporelles
                noise_factor = 0.1 * (grid_idx / max(1, num_grids - 1))  # Variation progressive
                X_pred_variant['day_sin'] += np.random.normal(0, noise_factor)
                X_pred_variant['day_cos'] += np.random.normal(0, noise_factor)
                
                # Pr√©diction des probabilit√©s pour tous les num√©ros
                probabilities = model.predict_proba(X_pred_variant)
                
                # Extraction des probabilit√©s pour la classe positive (num√©ro tir√©)
                # probabilities est une liste de probabilit√©s pour chaque output
                number_probs = {}
                for i in range(KENO_PARAMS['total_numbers']):
                    num = i + 1  # Num√©ros de 1 √† 70
                    if len(probabilities[i][0]) > 1:  # V√©rifier qu'on a bien les 2 classes
                        prob = probabilities[i][0][1]  # Probabilit√© de la classe positive
                    else:
                        prob = 0.5  # Valeur par d√©faut
                    number_probs[num] = prob
                
                # Simulation d'un tirage r√©aliste Keno
                # Dans un vrai tirage, on tire 20 num√©ros, mais le joueur en s√©lectionne 10
                
                # S√©lection des num√©ros avec pond√©ration par probabilit√©
                numbers = list(range(1, KENO_PARAMS['total_numbers'] + 1))
                probs = [number_probs[num] for num in numbers]
                
                # Normalisation des probabilit√©s
                probs_array = np.array(probs)
                probs_normalized = probs_array / np.sum(probs_array)
                
                # Ajustement pour tenir compte des corr√©lations
                # Boost des num√©ros qui apparaissent souvent ensemble
                if self.stats and self.stats.paires_freq:
                    correlation_boost = {num: 0 for num in numbers}
                    
                    # Identifier les paires fr√©quentes
                    top_pairs = sorted(self.stats.paires_freq.items(), 
                                     key=lambda x: x[1], reverse=True)[:50]
                    
                    for (num1, num2), freq in top_pairs:
                        if num1 in number_probs and num2 in number_probs:
                            # Si les deux num√©ros ont des probabilit√©s √©lev√©es, boost mutuel
                            if number_probs[num1] > 0.5 and number_probs[num2] > 0.5:
                                correlation_boost[num1] += 0.1 * (freq / 1000)
                                correlation_boost[num2] += 0.1 * (freq / 1000)
                    
                    # Application du boost
                    for num in numbers:
                        idx = num - 1
                        probs_normalized[idx] += correlation_boost[num]
                    
                    # Re-normalisation
                    probs_normalized = probs_normalized / np.sum(probs_normalized)
                
                # S√©lection pond√©r√©e de 10 num√©ros (pour le joueur)
                try:
                    selected = np.random.choice(
                        numbers, 
                        size=10, 
                        replace=False, 
                        p=probs_normalized
                    )
                    grid_numbers = sorted(selected.tolist())
                except:
                    # Fallback en cas d'erreur
                    top_indices = np.argsort(probs_normalized)[-10:]
                    grid_numbers = sorted([numbers[i] for i in top_indices])
                
                predictions.append(grid_numbers)
            
            return predictions
            
        except Exception as e:
            self._log(f"‚ùå Erreur lors de la pr√©diction ML: {e}", "ERROR")
            import traceback
            self._log(f"D√©tails: {traceback.format_exc()}")
            return []
    
    def generate_frequency_based_grids(self, num_grids: int = 10) -> List[List[int]]:
        """
        G√©n√®re des grilles bas√©es sur l'analyse compl√®te des patterns
        
        Args:
            num_grids: Nombre de grilles √† g√©n√©rer
            
        Returns:
            List[List[int]]: Liste des grilles g√©n√©r√©es optimis√©es
        """
        if not self.stats:
            self._log("‚ùå Statistiques non disponibles", "ERROR")
            return []
        
        self._log(f"üéØ G√©n√©ration de {num_grids} grilles avec analyse compl√®te des patterns")
        grids = []
        
        # 1. ANALYSE DES FR√âQUENCES SUR DIFF√âRENTES P√âRIODES
        freq_global = sorted(self.stats.frequences.items(), key=lambda x: x[1], reverse=True)
        freq_recente = sorted(self.stats.frequences_recentes.items(), key=lambda x: x[1], reverse=True)
        freq_50 = sorted(self.stats.frequences_50.items(), key=lambda x: x[1], reverse=True)
        freq_20 = sorted(self.stats.frequences_20.items(), key=lambda x: x[1], reverse=True)
        
        # 2. ANALYSE DES RETARDS (OVERDUE NUMBERS)
        retard_sorted = sorted(self.stats.retards.items(), key=lambda x: x[1], reverse=True)
        
        # 3. NUM√âROS CHAUDS ET FROIDS SELON DIFF√âRENTES P√âRIODES
        hot_global = [num for num, _ in freq_global[:25]]           # Top 25 historique
        hot_recent = [num for num, _ in freq_recente[:20]]          # Top 20 r√©cent (100 tirages)
        hot_tendance = [num for num, _ in freq_20[:15]]             # Top 15 tendance (20 tirages)
        cold_retard = [num for num, _ in retard_sorted[:30]]        # Top 30 en retard
        
        # 4. ANALYSE DES TENDANCES
        tendances_positives = []
        for num, tendance in self.stats.tendances_50.items():
            if tendance > 1.2:  # Num√©ros en forte hausse
                tendances_positives.append((num, tendance))
        tendances_positives.sort(key=lambda x: x[1], reverse=True)
        hot_tendances = [num for num, _ in tendances_positives[:15]]
        
        # 5. PAIRES ET TRIOS FR√âQUENTS
        top_pairs = sorted(self.stats.paires_freq.items(), key=lambda x: x[1], reverse=True)[:100]
        if self.stats.trios_freq:
            top_trios = sorted(self.stats.trios_freq.items(), key=lambda x: x[1], reverse=True)[:50]
        else:
            top_trios = []
        
        # 6. G√âN√âRATION DE GRILLES DIVERSIFI√âES
        strategies = [
            ("hot_global", "Num√©ros chauds historiques"),
            ("hot_recent", "Num√©ros chauds r√©cents"),
            ("cold_retard", "Num√©ros en retard"),
            ("hot_tendances", "Tendances positives"),
            ("mixed_smart", "Mix intelligent"),
            ("pairs_based", "Bas√© sur les paires"),
            ("balanced_zones", "√âquilibrage par zones"),
            ("pattern_parit√©", "Pattern parit√© optimis√©")
        ]
        
        for i in range(num_grids):
            strategy_name, strategy_desc = strategies[i % len(strategies)]
            grid = []
            
            if strategy_name == "hot_global":
                # Grille bas√©e sur les num√©ros historiquement fr√©quents
                grid = self._generate_hot_global_grid(hot_global)
                
            elif strategy_name == "hot_recent":
                # Grille bas√©e sur les tendances r√©centes
                grid = self._generate_hot_recent_grid(hot_recent, hot_tendance)
                
            elif strategy_name == "cold_retard":
                # Grille bas√©e sur les num√©ros en retard
                grid = self._generate_cold_retard_grid(cold_retard)
                
            elif strategy_name == "hot_tendances":
                # Grille bas√©e sur les tendances positives
                grid = self._generate_tendance_grid(hot_tendances, hot_recent)
                
            elif strategy_name == "mixed_smart":
                # Mix intelligent de tous les crit√®res
                grid = self._generate_mixed_smart_grid(hot_global, hot_recent, cold_retard, hot_tendances)
                
            elif strategy_name == "pairs_based":
                # Grille bas√©e sur les paires fr√©quentes
                grid = self._generate_pairs_based_grid(top_pairs)
                
            elif strategy_name == "balanced_zones":
                # Grille √©quilibr√©e par zones
                grid = self._generate_balanced_zones_grid()
                
            elif strategy_name == "pattern_parit√©":
                # Grille optimis√©e pour la parit√©
                grid = self._generate_parite_optimized_grid(hot_global, hot_recent)
            
            # Validation et ajustement de la grille
            if len(grid) < 10:
                # Compl√©ter avec des num√©ros manquants
                available = [n for n in range(1, 71) if n not in grid]
                weights = [self.stats.frequences.get(n, 1) for n in available]
                while len(grid) < 10 and available:
                    selected = random.choices(available, weights=weights, k=1)[0]
                    grid.append(selected)
                    idx = available.index(selected)
                    available.pop(idx)
                    weights.pop(idx)
            
            grid = sorted(grid[:10])
            grids.append(grid)
            self._log(f"   ‚úÖ Grille {i+1}: {strategy_desc}")
        
        return grids
    
    def _generate_hot_global_grid(self, hot_global: List[int]) -> List[int]:
        """G√©n√®re une grille bas√©e sur les num√©ros historiquement fr√©quents"""
        return random.sample(hot_global, min(10, len(hot_global)))
    
    def _generate_hot_recent_grid(self, hot_recent: List[int], hot_tendance: List[int]) -> List[int]:
        """G√©n√®re une grille bas√©e sur les tendances r√©centes"""
        grid = []
        # 6 num√©ros r√©cents + 4 tendances
        grid.extend(random.sample(hot_recent, min(6, len(hot_recent))))
        available_tendance = [n for n in hot_tendance if n not in grid]
        grid.extend(random.sample(available_tendance, min(4, len(available_tendance))))
        return grid
    
    def _generate_cold_retard_grid(self, cold_retard: List[int]) -> List[int]:
        """G√©n√®re une grille bas√©e sur les num√©ros en retard"""
        # Strat√©gie: les num√©ros en retard ont plus de chances de sortir
        return random.sample(cold_retard, min(10, len(cold_retard)))
    
    def _generate_tendance_grid(self, hot_tendances: List[int], hot_recent: List[int]) -> List[int]:
        """G√©n√®re une grille bas√©e sur les tendances positives"""
        grid = []
        # 7 tendances + 3 r√©cents
        grid.extend(random.sample(hot_tendances, min(7, len(hot_tendances))))
        available_recent = [n for n in hot_recent if n not in grid]
        grid.extend(random.sample(available_recent, min(3, len(available_recent))))
        return grid
    
    def _generate_mixed_smart_grid(self, hot_global: List[int], hot_recent: List[int], 
                                  cold_retard: List[int], hot_tendances: List[int]) -> List[int]:
        """G√©n√®re un mix intelligent de tous les crit√®res"""
        grid = []
        
        # 3 num√©ros historiquement chauds
        grid.extend(random.sample(hot_global, min(3, len(hot_global))))
        
        # 3 num√©ros r√©cemment chauds (non d√©j√† s√©lectionn√©s)
        available_recent = [n for n in hot_recent if n not in grid]
        grid.extend(random.sample(available_recent, min(3, len(available_recent))))
        
        # 2 num√©ros en retard
        available_cold = [n for n in cold_retard if n not in grid]
        grid.extend(random.sample(available_cold, min(2, len(available_cold))))
        
        # 2 num√©ros en tendance positive
        available_tendance = [n for n in hot_tendances if n not in grid]
        grid.extend(random.sample(available_tendance, min(2, len(available_tendance))))
        
        return grid
    
    def _generate_pairs_based_grid(self, top_pairs: List[Tuple[Tuple[int, int], int]]) -> List[int]:
        """G√©n√®re une grille bas√©e sur les paires fr√©quentes"""
        grid = []
        used_numbers = set()
        
        # S√©lectionner des paires fr√©quentes
        for (num1, num2), freq in top_pairs[:20]:  # Top 20 paires
            if len(grid) >= 8:  # Laisser de la place pour 2 autres num√©ros
                break
            if num1 not in used_numbers and num2 not in used_numbers:
                grid.extend([num1, num2])
                used_numbers.update([num1, num2])
        
        # Compl√©ter si n√©cessaire
        while len(grid) < 10:
            candidates = [n for n in range(1, 71) if n not in used_numbers]
            if candidates:
                selected = random.choice(candidates)
                grid.append(selected)
                used_numbers.add(selected)
            else:
                break
        
        return grid
    
    def _generate_balanced_zones_grid(self) -> List[int]:
        """G√©n√®re une grille √©quilibr√©e par zones"""
        zones = {
            "zone1": list(range(1, 18)),      # 1-17
            "zone2": list(range(18, 36)),     # 18-35
            "zone3": list(range(36, 53)),     # 36-52
            "zone4": list(range(53, 71))      # 53-70
        }
        
        # R√©partition √©quilibr√©e: 2-3 num√©ros par zone
        grid = []
        grid.extend(random.sample(zones["zone1"], 2))
        grid.extend(random.sample(zones["zone2"], 3))
        grid.extend(random.sample(zones["zone3"], 3))
        grid.extend(random.sample(zones["zone4"], 2))
        
        return grid
    
    def _generate_parite_optimized_grid(self, hot_global: List[int], hot_recent: List[int]) -> List[int]:
        """G√©n√®re une grille optimis√©e pour la parit√© (√©viter tout pair/tout impair)"""
        # Statistiques montrent que <1% des grilles sont tout pair ou tout impair
        grid = []
        
        # Pool de num√©ros chauds
        hot_pool = list(set(hot_global + hot_recent))
        
        # S√©lectionner 5-6 pairs et 4-5 impairs
        pairs = [n for n in hot_pool if n % 2 == 0]
        impairs = [n for n in hot_pool if n % 2 == 1]
        
        # Si pas assez dans le pool chaud, compl√©ter
        if len(pairs) < 6:
            pairs.extend([n for n in range(2, 71, 2) if n not in pairs])
        if len(impairs) < 5:
            impairs.extend([n for n in range(1, 71, 2) if n not in impairs])
        
        # S√©lection √©quilibr√©e
        grid.extend(random.sample(pairs, min(5, len(pairs))))
        grid.extend(random.sample(impairs, min(5, len(impairs))))
        
        return grid
    
    def calculate_grid_score(self, grid: List[int]) -> float:
        """
        Calcule un score de qualit√© pour une grille Keno
        
        Args:
            grid: Liste des num√©ros de la grille
            
        Returns:
            float: Score de qualit√©
        """
        if not self.stats:
            return 0.0
        
        score = 0.0
        
        # Score bas√© sur les fr√©quences normalis√©es
        total_freq = sum(self.stats.frequences.values())
        for num in grid:
            freq_score = self.stats.frequences[num] / total_freq
            score += freq_score
        
        # Bonus pour la r√©partition par zones
        zones = {
            "zone1_17": sum(1 for n in grid if 1 <= n <= 17),
            "zone18_35": sum(1 for n in grid if 18 <= n <= 35), 
            "zone36_52": sum(1 for n in grid if 36 <= n <= 52),
            "zone53_70": sum(1 for n in grid if 53 <= n <= 70)
        }
        
        # P√©nalit√© pour d√©s√©quilibre extr√™me des zones
        zone_counts = list(zones.values())
        if max(zone_counts) <= 4 and min(zone_counts) >= 1:  # R√©partition √©quilibr√©e
            score += 0.1
        
        # Bonus pour les paires fr√©quentes
        for i in range(len(grid)):
            for j in range(i + 1, len(grid)):
                pair = tuple(sorted([grid[i], grid[j]]))
                if pair in self.stats.paires_freq:
                    pair_freq = self.stats.paires_freq[pair]
                    score += pair_freq / 1000  # Normalisation
        
        # Score de dispersion (√©viter les num√©ros trop cons√©cutifs)
        consecutive_count = 0
        sorted_grid = sorted(grid)
        for i in range(len(sorted_grid) - 1):
            if sorted_grid[i + 1] - sorted_grid[i] == 1:
                consecutive_count += 1
        
        if consecutive_count <= 2:  # Maximum 2 paires cons√©cutives acceptables
            score += 0.05
        
        return score
    
    def generate_optimized_grids(self, num_grids: int = 100) -> List[Tuple[List[int], float]]:
        """
        G√©n√®re des grilles optimis√©es en combinant ML et analyse fr√©quentielle
        
        Args:
            num_grids: Nombre de grilles √† g√©n√©rer
            
        Returns:
            List[Tuple[List[int], float]]: Liste des grilles avec leurs scores
        """
        self._log(f"üéØ G√©n√©ration de {num_grids} grilles Keno optimis√©es...")
        
        all_grids = []
        
        # R√©partition selon les poids adaptatifs
        ml_count = int(num_grids * self.adaptive_weights['ml_weight'])
        freq_count = num_grids - ml_count
        
        self._log(f"   ü§ñ Poids adaptatifs: ML={self.adaptive_weights['ml_weight']:.1%}, Freq={self.adaptive_weights['freq_weight']:.1%}")
        
        # G√©n√©ration ML
        if ml_count > 0 and self.ml_models:
            ml_grids = self.predict_numbers_ml(ml_count)
            all_grids.extend(ml_grids)
        
        # G√©n√©ration bas√©e sur les fr√©quences
        if freq_count > 0:
            freq_grids = self.generate_frequency_based_grids(freq_count)
            all_grids.extend(freq_grids)
        
        # Calcul des scores et tri
        scored_grids = []
        for grid in all_grids:
            if len(grid) == 10:  # Validation
                score = self.calculate_grid_score(grid)
                scored_grids.append((grid, score))
        
        # Tri par score d√©croissant
        scored_grids.sort(key=lambda x: x[1], reverse=True)
        
        # Suppression des doublons
        unique_grids = []
        seen = set()
        for grid, score in scored_grids:
            grid_tuple = tuple(sorted(grid))
            if grid_tuple not in seen:
                unique_grids.append((grid, score))
                seen.add(grid_tuple)
        
        self._log(f"‚úÖ {len(unique_grids)} grilles uniques g√©n√©r√©es")
        return unique_grids[:num_grids]
    
    def save_results(self, grids_with_scores: List[Tuple[List[int], float]], 
                    filename: str = None) -> str:
        """
        Sauvegarde les r√©sultats dans un fichier CSV
        
        Args:
            grids_with_scores: Liste des grilles avec scores
            filename: Nom du fichier (optionnel)
            
        Returns:
            str: Chemin du fichier sauvegard√©
        """
        if not filename:
            # Utilisation d'un nom fixe qui remplace le fichier pr√©c√©dent
            filename = "grilles_keno.csv"
        
        filepath = self.output_dir / filename
        
        # Pr√©paration des donn√©es
        data_rows = []
        for grid, score in grids_with_scores:
            row = {}
            for i, num in enumerate(grid, 1):
                row[f'numero_{i}'] = num
            row['score'] = score
            data_rows.append(row)
        
        # Sauvegarde
        df_results = pd.DataFrame(data_rows)
        df_results.to_csv(filepath, index=False)
        
        self._log(f"üíæ R√©sultats sauvegard√©s: {filepath}")
        return str(filepath)
    
    def generate_report(self, grids_with_scores: List[Tuple[List[int], float]]) -> str:
        """
        G√©n√®re un rapport d'analyse d√©taill√©
        
        Args:
            grids_with_scores: Grilles avec leurs scores
            
        Returns:
            str: Contenu du rapport
        """
        if not grids_with_scores:
            return "Aucune grille g√©n√©r√©e."
        
        report = []
        report.append("# üé≤ RAPPORT D'ANALYSE KENO")
        report.append("=" * 50)
        report.append(f"üìÖ **Date**: {datetime.now().strftime('%d/%m/%Y %H:%M')}")
        report.append(f"üìä **Grilles g√©n√©r√©es**: {len(grids_with_scores)}")
        report.append("")
        
        # Top 5 des grilles
        report.append("## üèÜ TOP 5 DES GRILLES RECOMMAND√âES")
        report.append("")
        for i, (grid, score) in enumerate(grids_with_scores[:5], 1):
            nums_str = " - ".join(f"{n:2d}" for n in grid)
            report.append(f"**{i}.** [{nums_str}] | Score: {score:.4f}")
        report.append("")
        
        # Statistiques
        scores = [score for _, score in grids_with_scores]
        report.append("## üìà STATISTIQUES")
        report.append("")
        report.append(f"- **Score moyen**: {np.mean(scores):.4f}")
        report.append(f"- **Score m√©dian**: {np.median(scores):.4f}")
        report.append(f"- **Meilleur score**: {max(scores):.4f}")
        report.append(f"- **Score minimum**: {min(scores):.4f}")
        report.append("")
        
        # Analyse des num√©ros les plus s√©lectionn√©s
        all_numbers = []
        for grid, _ in grids_with_scores:
            all_numbers.extend(grid)
        
        num_counts = {}
        for num in all_numbers:
            num_counts[num] = num_counts.get(num, 0) + 1
        
        top_numbers = sorted(num_counts.items(), key=lambda x: x[1], reverse=True)[:10]
        
        report.append("## üî• NUM√âROS LES PLUS S√âLECTIONN√âS")
        report.append("")
        for num, count in top_numbers:
            percentage = (count / len(grids_with_scores)) * 100
            report.append(f"- **{num:2d}**: {count} fois ({percentage:.1f}%)")
        report.append("")
        
        # Strat√©gie adaptative
        report.append("## ü§ñ STRAT√âGIE ADAPTATIVE")
        report.append("")
        report.append(f"- **Poids ML**: {self.adaptive_weights['ml_weight']:.1%}")
        report.append(f"- **Poids Fr√©quence**: {self.adaptive_weights['freq_weight']:.1%}")
        report.append("")
        
        return "\n".join(report)
    
    def run(self, num_grids: int = 100, save_file: str = None, 
           retrain: bool = False) -> bool:
        """
        Lance la g√©n√©ration compl√®te de grilles Keno
        
        Args:
            num_grids: Nombre de grilles √† g√©n√©rer
            save_file: Nom du fichier de sauvegarde
            retrain: Force le r√©entra√Ænement des mod√®les
            
        Returns:
            bool: True si succ√®s
        """
        start_time = time.time()
        
        try:
            # 1. Chargement des donn√©es
            if not self.load_data():
                return False
            
            # 2. Analyse des patterns
            self.analyze_patterns()
            
            # 3. Mod√®les ML
            if HAS_ML:
                if retrain or not self.load_ml_models():
                    if not self.train_xgboost_models(retrain=retrain):
                        self._log("‚ö†Ô∏è  Utilisation du mode fr√©quence uniquement")
                        self.adaptive_weights['ml_weight'] = 0.0
                        self.adaptive_weights['freq_weight'] = 1.0
            else:
                self._log("‚ö†Ô∏è  Mode fr√©quence uniquement")
                self.adaptive_weights['ml_weight'] = 0.0
                self.adaptive_weights['freq_weight'] = 1.0
            
            # 4. G√©n√©ration des grilles
            grids_with_scores = self.generate_optimized_grids(num_grids)
            
            if not grids_with_scores:
                self._log("‚ùå Aucune grille g√©n√©r√©e", "ERROR")
                return False
            
            # 5. Sauvegarde
            result_file = self.save_results(grids_with_scores, save_file)
            
            # 6. Rapport
            report = self.generate_report(grids_with_scores)
            
            # Affichage des r√©sultats
            self._log("\n" + "=" * 70)
            self._log("üéØ G√âN√âRATION KENO TERMIN√âE")
            self._log("=" * 70)
            
            # Top 5
            self._log("\nüèÜ Top 5 des grilles recommand√©es:")
            for i, (grid, score) in enumerate(grids_with_scores[:5], 1):
                nums_str = " - ".join(f"{n:2d}" for n in grid)
                self._log(f"   {i}. [{nums_str}] | Score: {score:.4f}")
            
            # Statistiques
            scores = [score for _, score in grids_with_scores]
            self._log(f"\nüìä Statistiques:")
            self._log(f"   - Grilles g√©n√©r√©es: {len(grids_with_scores):,}")
            self._log(f"   - Score moyen: {np.mean(scores):.4f}")
            self._log(f"   - Meilleur score: {max(scores):.4f}")
            
            # Temps d'ex√©cution
            elapsed = time.time() - start_time
            self._log(f"\n‚è±Ô∏è  Temps d'ex√©cution: {elapsed:.2f} secondes")
            self._log(f"üìÅ R√©sultats sauvegard√©s: {result_file}")
            
            # Sauvegarde du rapport
            report_file = self.output_dir / "rapport_keno.md"
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write(report)
            self._log(f"üìÑ Rapport sauvegard√©: {report_file}")
            
            return True
            
        except Exception as e:
            self._log(f"‚ùå Erreur lors de l'ex√©cution: {e}", "ERROR")
            return False
    
    def calculate_and_export_top30(self, export_path: Optional[str] = None) -> List[int]:
        """
        Calcule le TOP 30 des num√©ros Keno avec scoring intelligent optimal et l'exporte en CSV
        
        Utilise un scoring multi-crit√®res avanc√© bas√© sur :
        - Fr√©quences multi-p√©riodes (30%) : Global + R√©cent + Moyen terme
        - Retard intelligent (25%) : Retard optimal avec bonus zones de retard
        - Tendances dynamiques (20%) : Analyse sur 10, 50 et 100 tirages
        - Popularit√© paires (15%) : Bonus pour num√©ros dans paires fr√©quentes
        - √âquilibrage zones (10%) : R√©partition g√©ographique optimale
        
        Args:
            export_path: Chemin d'export personnalis√© (optionnel)
            
        Returns:
            List[int]: Liste des 30 meilleurs num√©ros
        """
        self._log("üß† Calcul du TOP 30 Keno avec profil intelligent optimal...")
        
        if not hasattr(self, 'stats') or self.stats is None:
            self._log("‚ö†Ô∏è Statistiques non disponibles, analyse en cours...")
            self.analyze_patterns()
        
        # Calcul des maximums pour normalisation
        max_freq_global = max(self.stats.frequences.values()) if self.stats.frequences else 1
        max_freq_recent = max(self.stats.frequences_recentes.values()) if self.stats.frequences_recentes else 1
        max_freq_50 = max(self.stats.frequences_50.values()) if self.stats.frequences_50 else 1
        max_freq_20 = max(self.stats.frequences_20.values()) if self.stats.frequences_20 else 1
        max_retard = max(self.stats.retards.values()) if self.stats.retards else 1
        
        # Calcul du scoring intelligent optimal multi-crit√®res
        scores = {}
        
        for numero in range(1, 71):
            score_total = 0.0
            
            # 1. FR√âQUENCES MULTI-P√âRIODES (30%) - Pond√©ration intelligente
            freq_global = self.stats.frequences.get(numero, 0)
            freq_recent = self.stats.frequences_recentes.get(numero, 0)
            freq_50 = self.stats.frequences_50.get(numero, 0)
            freq_20 = self.stats.frequences_20.get(numero, 0)
            
            # Pond√©ration : r√©cent > moyen terme > global pour d√©tecter les tendances
            freq_score = (
                (freq_global / max_freq_global) * 0.10 +      # 10% fr√©quence globale
                (freq_recent / max_freq_recent) * 0.08 +      # 8% fr√©quence 100 tirages  
                (freq_50 / max_freq_50) * 0.08 +              # 8% fr√©quence 50 tirages
                (freq_20 / max_freq_20) * 0.04                # 4% fr√©quence 20 tirages (tendance imm√©diate)
            ) * 100  # Normalisation sur 30 points
            score_total += freq_score
            
            # 2. RETARD INTELLIGENT (25%) - Zones de retard optimales
            retard = self.stats.retards.get(numero, 0)
            retard_normalized = retard / max_retard if max_retard > 0 else 0
            
            # Bonus pour retards dans la zone optimale (ni trop faible, ni trop √©lev√©)
            if 0.15 <= retard_normalized <= 0.45:      # Zone optimale (15-45% du retard max)
                retard_bonus = 1.3                      # Bonus fort pour retards optimaux
            elif 0.45 < retard_normalized <= 0.70:     # Zone de retard mod√©r√©
                retard_bonus = 1.2                      # Bonus mod√©r√©
            elif retard_normalized > 0.70:             # Tr√®s en retard
                retard_bonus = 1.15                     # L√©ger bonus pour les grands retards
            else:                                       # Peu en retard
                retard_bonus = 0.9                      # L√©g√®re p√©nalit√©
                
            retard_score = (1 - retard_normalized) * 25 * retard_bonus
            score_total += retard_score
            
            # 3. TENDANCES DYNAMIQUES (20%) - Analyse multi-p√©riodes
            tendance_10 = self.stats.tendances_10.get(numero, 1.0) if hasattr(self.stats, 'tendances_10') else 1.0
            tendance_50 = self.stats.tendances_50.get(numero, 1.0) if hasattr(self.stats, 'tendances_50') else 1.0
            tendance_100 = self.stats.tendances_100.get(numero, 1.0) if hasattr(self.stats, 'tendances_100') else 1.0
            
            # Moyenne pond√©r√©e des tendances (court terme > moyen terme > long terme)
            tendance_moyenne = (tendance_10 * 0.5 + tendance_50 * 0.3 + tendance_100 * 0.2)
            
            # Score bas√© sur la force de la tendance positive
            if tendance_moyenne > 1.3:                 # Forte tendance positive
                tendance_score = 20
            elif tendance_moyenne > 1.1:               # Tendance positive mod√©r√©e
                tendance_score = 15
            elif tendance_moyenne > 0.9:               # Stable
                tendance_score = 10
            elif tendance_moyenne > 0.7:               # Tendance n√©gative mod√©r√©e
                tendance_score = 5
            else:                                       # Forte tendance n√©gative
                tendance_score = 2
                
            score_total += tendance_score
            
            # 4. POPULARIT√â DANS LES PAIRES (15%) - Bonus pour associations fr√©quentes
            pair_score = 0
            if hasattr(self.stats, 'paires_freq'):
                # Compter les paires fr√©quentes contenant ce num√©ro
                paires_importantes = 0
                total_freq_paires = 0
                
                for (n1, n2), freq in self.stats.paires_freq.items():
                    if n1 == numero or n2 == numero:
                        if freq > 50:  # Seuil pour paires significatives
                            paires_importantes += 1
                            total_freq_paires += freq
                
                # Score bas√© sur nombre et fr√©quence des paires importantes
                if paires_importantes > 0:
                    pair_score = min(
                        (paires_importantes * 3) + (total_freq_paires / 200),
                        15  # Maximum 15 points
                    )
            
            score_total += pair_score
            
            # 5. √âQUILIBRAGE ZONES (10%) - R√©partition g√©ographique optimale
            zone_score = 0
            if hasattr(self.stats, 'patterns_zones'):
                # D√©terminer la zone du num√©ro
                if 1 <= numero <= 17:
                    zone_key = 'zone_1_17'
                elif 18 <= numero <= 35:
                    zone_key = 'zone_18_35'
                elif 36 <= numero <= 52:
                    zone_key = 'zone_36_52'
                else:  # 53-70
                    zone_key = 'zone_53_70'
                
                # Score bas√© sur l'activit√© de la zone
                zone_freq = self.stats.patterns_zones.get(zone_key, 0)
                max_zone_freq = max(self.stats.patterns_zones.values()) if self.stats.patterns_zones else 1
                
                if max_zone_freq > 0:
                    zone_score = (zone_freq / max_zone_freq) * 10
                else:
                    zone_score = 5  # Score par d√©faut
            else:
                zone_score = 5  # Score par d√©faut si pas de donn√©es zones
            
            score_total += zone_score
            
            scores[numero] = round(score_total, 3)
        
        # Tri et s√©lection du TOP 30 avec scores d√©taill√©s
        sorted_numbers = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        top30_numbers = [num for num, score in sorted_numbers[:30]]
        
        # Pr√©paration des donn√©es enrichies pour export
        top30_data = []
        for num, score in sorted_numbers[:30]:
            # Calcul des composantes du score pour tra√ßabilit√©
            freq_global = self.stats.frequences.get(num, 0)
            freq_recent = self.stats.frequences_recentes.get(num, 0)
            freq_50 = self.stats.frequences_50.get(num, 0)
            freq_20 = self.stats.frequences_20.get(num, 0)
            retard = self.stats.retards.get(num, 0)
            
            # Tendances multi-p√©riodes
            tendance_10 = self.stats.tendances_10.get(num, 1.0) if hasattr(self.stats, 'tendances_10') else 1.0
            tendance_50 = self.stats.tendances_50.get(num, 1.0) if hasattr(self.stats, 'tendances_50') else 1.0
            tendance_100 = self.stats.tendances_100.get(num, 1.0) if hasattr(self.stats, 'tendances_100') else 1.0
            
            # Nombre de paires fr√©quentes
            nb_paires_freq = 0
            if hasattr(self.stats, 'paires_freq'):
                for (n1, n2), freq in self.stats.paires_freq.items():
                    if (n1 == num or n2 == num) and freq > 50:
                        nb_paires_freq += 1
            
            # Zone g√©ographique
            if 1 <= num <= 17:
                zone = 1
                zone_nom = "1-17"
            elif 18 <= num <= 35:
                zone = 2
                zone_nom = "18-35"
            elif 36 <= num <= 52:
                zone = 3
                zone_nom = "36-52"
            else:
                zone = 4
                zone_nom = "53-70"
            
            top30_data.append({
                'Numero': num,
                'Score_Total': round(score, 2),
                'Rang': len(top30_data) + 1,
                
                # Fr√©quences d√©taill√©es
                'Freq_Globale': freq_global,
                'Freq_100_Derniers': freq_recent,
                'Freq_50_Derniers': freq_50,
                'Freq_20_Derniers': freq_20,
                
                # Retard et tendances
                'Retard_Actuel': retard,
                'Tendance_10': round(tendance_10, 3),
                'Tendance_50': round(tendance_50, 3),
                'Tendance_100': round(tendance_100, 3),
                
                # Associations et r√©partition
                'Nb_Paires_Frequentes': nb_paires_freq,
                'Zone': zone,
                'Zone_Nom': zone_nom,
                'Parite': 'Pair' if num % 2 == 0 else 'Impair'
            })
        
        # Export enrichi en CSV
        if export_path is None:
            export_path = self.output_dir / f"keno_top30.csv"
        else:
            export_path = Path(export_path)
            
        # Cr√©er le dossier parent si n√©cessaire
        export_path.parent.mkdir(parents=True, exist_ok=True)
        
        # Sauvegarde du TOP 30 enrichi
        top30_df = pd.DataFrame(top30_data)
        top30_df.to_csv(export_path, index=False)
        
        # Statistiques du TOP 30 pour validation
        total_pairs = sum(1 for num in top30_numbers if num % 2 == 0)
        repartition_zones = [0, 0, 0, 0]
        for num in top30_numbers:
            zone_idx = min((num - 1) // 18, 3)
            repartition_zones[zone_idx] += 1
        
        self._log(f"‚úÖ TOP 30 optimis√© calcul√© et export√©:")
        self._log(f"   üìÅ Fichier: {export_path}")
        self._log(f"   üéØ Top 10: {', '.join(map(str, top30_numbers[:10]))}")
        self._log(f"   üìä R√©partition pairs/impairs: {total_pairs}/{30-total_pairs}")
        self._log(f"   üó∫Ô∏è  R√©partition zones: {'-'.join(map(str, repartition_zones))}")
        self._log(f"   üíØ Score moyen: {sum(scores[num] for num in top30_numbers)/30:.1f}")
        
        return top30_numbers

# ==============================================================================
# üöÄ POINT D'ENTR√âE PRINCIPAL
# ==============================================================================

def main():
    """Point d'entr√©e principal du script"""
    
    parser = argparse.ArgumentParser(
        description="üé≤ G√©n√©rateur Intelligent de Grilles Keno v2.0",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples d'utilisation:
  python keno_generator_advanced.py --quick                    # 10 grilles, entra√Ænement rapide
  python keno_generator_advanced.py --balanced                 # 100 grilles, entra√Ænement √©quilibr√© (d√©faut)
  python keno_generator_advanced.py --comprehensive            # 500 grilles, entra√Ænement complet
  python keno_generator_advanced.py --intensive                # 1000 grilles, entra√Ænement intensif
  python keno_generator_advanced.py --retrain --comprehensive  # Force r√©-entra√Ænement + mode complet
  python keno_generator_advanced.py --grids 50 --silent        # 50 grilles en mode silencieux

Profils d'entra√Ænement:
  - quick:        Rapide (50 arbres, profondeur 8)
  - balanced:     √âquilibr√© (100 arbres, profondeur 12) [D√âFAUT]
  - comprehensive: Complet (200 arbres, profondeur 15)
  - intensive:    Intensif (300 arbres, profondeur 20)
        """
    )
    
    parser.add_argument('--grids', type=int, default=100,
                       help='Nombre de grilles √† g√©n√©rer (d√©faut: 100)')
    parser.add_argument('--output', type=str,
                       help='Nom du fichier de sortie (optionnel)')
    parser.add_argument('--retrain', action='store_true',
                       help='Force le r√©entra√Ænement des mod√®les ML')
    parser.add_argument('--quick', action='store_true',
                       help='Mode rapide (10 grilles, entra√Ænement ultra-rapide)')
    parser.add_argument('--balanced', action='store_true',
                       help='Mode √©quilibr√© (100 grilles, entra√Ænement standard)')
    parser.add_argument('--comprehensive', action='store_true',
                       help='Mode complet (500 grilles, entra√Ænement optimis√©)')
    parser.add_argument('--intensive', action='store_true',
                       help='Mode intensif (1000 grilles, entra√Ænement maximal)')
    parser.add_argument('--silent', action='store_true',
                       help='Mode silencieux')
    parser.add_argument('--data', type=str,
                       help='Chemin vers les donn√©es (optionnel)')
    
    args = parser.parse_args()
    
    # Gestion des profils d'entra√Ænement mutuellement exclusifs
    profile_count = sum([args.quick, args.balanced, args.comprehensive, args.intensive])
    if profile_count > 1:
        print("‚ùå Erreur: Un seul profil d'entra√Ænement peut √™tre s√©lectionn√© √† la fois")
        sys.exit(1)
    
    # Configuration selon le profil s√©lectionn√©
    if args.quick:
        args.grids = 10
        training_profile = "quick"
    elif args.balanced:
        args.grids = 100
        training_profile = "balanced"
    elif args.comprehensive:
        args.grids = 500
        training_profile = "comprehensive"
    elif args.intensive:
        args.grids = 1000
        training_profile = "intensive"
    else:
        # Mode par d√©faut (√©quilibr√©)
        training_profile = "balanced"
    
    # Banner
    if not args.silent:
        profile_names = {
            'quick': 'Ultra-rapide',
            'balanced': '√âquilibr√©', 
            'comprehensive': 'Complet',
            'intensive': 'Intensif'
        }
        profile_display = profile_names.get(training_profile, '√âquilibr√©')
        
        print("=" * 70)
        print("üé≤ G√âN√âRATEUR INTELLIGENT DE GRILLES KENO v2.0 üé≤")
        print("=" * 70)
        print("Machine Learning + Analyse Statistique")
        print("Optimisation des combinaisons Keno")
        print(f"üìä Profil: {profile_display} ({args.grids} grilles)")
        print("=" * 70)
    
    # Initialisation et ex√©cution
    generator = KenoGeneratorAdvanced(
        data_path=args.data,
        silent=args.silent,
        training_profile=training_profile
    )
    
    success = generator.run(
        num_grids=args.grids,
        save_file=args.output,
        retrain=args.retrain
    )
    
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()
