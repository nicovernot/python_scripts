#!/usr/bin/env python3
"""
Assembleur de donn√©es Keno - Consolide tous les fichiers CSV en un seul
"""

import os
import pandas as pd
from pathlib import Path
from datetime import datetime
import sys

def assemble_keno_data(auto_cleanup=True):
    """Assemble tous les fichiers CSV de keno_data en un seul fichier consolid√©
    
    Args:
        auto_cleanup (bool): Si True, nettoie automatiquement les fichiers inutiles apr√®s assemblage
    """
    
    # R√©pertoires
    data_dir = Path("keno/keno_data")
    output_file = data_dir / "keno_consolidated.csv"
    backup_file = data_dir / f"keno_consolidated_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
    
    print("üîÑ ASSEMBLAGE DES DONN√âES KENO")
    print("=" * 50)
    
    # Sauvegarde de l'ancien fichier consolid√© s'il existe
    if output_file.exists():
        print(f"üìã Sauvegarde de l'ancien fichier: {backup_file.name}")
        os.rename(output_file, backup_file)
    
    # Recherche de tous les fichiers CSV (sauf les consolid√©s et backups)
    csv_files = []
    for csv_file in data_dir.glob("*.csv"):
        if not csv_file.name.startswith(("keno_consolidated", "keno_backup")):
            csv_files.append(csv_file)
    
    if not csv_files:
        print("‚ùå Aucun fichier CSV trouv√© dans keno_data/")
        return False
    
    csv_files.sort()  # Tri par nom (chronologique)
    
    print(f"üìÅ Fichiers trouv√©s: {len(csv_files)}")
    for f in csv_files:
        print(f"   - {f.name}")
    
    # Lecture et assemblage (avec gestion des doublons)
    all_dataframes = []
    total_rows = 0
    
    for i, csv_file in enumerate(csv_files):
        print(f"üìä Traitement {i+1}/{len(csv_files)}: {csv_file.name}", end="")
        
        try:
            # Lecture du fichier
            df = pd.read_csv(csv_file)
            
            # V√©rification de la structure
            expected_columns = ['date', 'numero_tirage', 'b1', 'b2', 'b3', 'b4', 'b5', 
                              'b6', 'b7', 'b8', 'b9', 'b10', 'b11', 'b12', 'b13', 
                              'b14', 'b15', 'b16', 'b17', 'b18', 'b19', 'b20']
            
            if list(df.columns) != expected_columns:
                print(f" ‚ö†Ô∏è  Structure diff√©rente, ignor√©")
                continue
            
            all_dataframes.append(df)
            rows = len(df)
            total_rows += rows
            print(f" ‚úÖ {rows} tirages")
            
        except Exception as e:
            print(f" ‚ùå Erreur: {str(e)}")
            continue
    
    # Consolidation et suppression des doublons
    print(f"\nüîÑ Consolidation de {len(all_dataframes)} fichiers...")
    if all_dataframes:
        # Concat√©nation de tous les DataFrames
        consolidated_df = pd.concat(all_dataframes, ignore_index=True)
        
        # Suppression des doublons bas√©e sur numero_tirage
        print(f"üìä Avant d√©doublonnage: {len(consolidated_df)} tirages")
        consolidated_df = consolidated_df.drop_duplicates(subset=['numero_tirage'], keep='first')
        print(f"üìä Apr√®s d√©doublonnage: {len(consolidated_df)} tirages")
        
        # Tri par numero_tirage
        consolidated_df = consolidated_df.sort_values('numero_tirage')
        
        # Sauvegarde
        consolidated_df.to_csv(output_file, index=False)
        print(f"üíæ Fichier sauvegard√©: {output_file}")
    else:
        print("‚ùå Aucune donn√©e √† consolider")
        return False

    print("\nüìà R√âSUM√â")
    print("=" * 50)
    print(f"üìÅ Fichiers trait√©s: {len(all_dataframes)}")
    print(f"üìä Total tirages (avant d√©doublonnage): {total_rows}")
    print(f"üíæ Fichier consolid√©: {output_file}")
    print(f"üì¶ Taille: {output_file.stat().st_size / 1024:.1f} KB")
    
    # V√©rification finale
    if output_file.exists():
        final_df = pd.read_csv(output_file)
        print(f"‚úÖ V√©rification: {len(final_df)} lignes dans le fichier final")
        
        # Affichage des statistiques
        print(f"üìÖ P√©riode: {final_df['date'].min()} ‚Üí {final_df['date'].max()}")
        print(f"üé≤ Num√©ros de tirage: {final_df['numero_tirage'].min()} ‚Üí {final_df['numero_tirage'].max()}")
        
        # V√©rification des doublons
        duplicates = final_df.duplicated(subset=['numero_tirage']).sum()
        if duplicates > 0:
            print(f"‚ö†Ô∏è  {duplicates} doublons restants")
        else:
            print("‚úÖ Aucun doublon dans le fichier final")
        
        # Nettoyage automatique si demand√©
        if auto_cleanup:
            cleanup_result = cleanup_unnecessary_files(data_dir, csv_files, keep_latest_backup=True)
            if cleanup_result:
                print("\nüßπ NETTOYAGE AUTOMATIQUE TERMIN√â")
        
        return True
    else:
        print("‚ùå √âchec de la cr√©ation du fichier consolid√©")
        return False

def cleanup_unnecessary_files(data_dir, processed_files, keep_latest_backup=True):
    """Nettoie les fichiers CSV inutiles apr√®s consolidation
    
    Args:
        data_dir (Path): R√©pertoire des donn√©es
        processed_files (list): Liste des fichiers qui ont √©t√© trait√©s
        keep_latest_backup (bool): Garder le backup le plus r√©cent
    """
    print("\nüßπ NETTOYAGE DES FICHIERS INUTILES")
    print("=" * 50)
    
    files_to_delete = []
    files_to_keep = []
    
    # 1. Garder le fichier consolid√© principal
    consolidated_main = data_dir / "keno_consolidated.csv"
    if consolidated_main.exists():
        files_to_keep.append(consolidated_main.name)
    
    # 2. Garder uniquement le backup le plus r√©cent
    backup_files = list(data_dir.glob("keno_consolidated_backup_*.csv"))
    if backup_files and keep_latest_backup:
        latest_backup = max(backup_files, key=lambda f: f.stat().st_mtime)
        files_to_keep.append(latest_backup.name)
        
        # Marquer les autres backups pour suppression
        for backup in backup_files:
            if backup != latest_backup:
                files_to_delete.append(backup)
    else:
        files_to_delete.extend(backup_files)
    
    # 3. Supprimer tous les fichiers individuels trait√©s (ils sont maintenant dans le consolid√©)
    for csv_file in processed_files:
        if csv_file.exists() and csv_file.name not in files_to_keep:
            files_to_delete.append(csv_file)
    
    # 4. Nettoyer les fichiers temporaires et anciens
    temp_patterns = [
        "*_20*_*.csv",  # Fichiers avec timestamp
        "keno_backup_*.csv",
        "*_extracted.csv",
        "*_temp.csv"
    ]
    
    for pattern in temp_patterns:
        temp_files = data_dir.glob(pattern)
        for temp_file in temp_files:
            if temp_file.name not in files_to_keep and temp_file not in files_to_delete:
                files_to_delete.append(temp_file)
    
    # Ex√©cution du nettoyage
    deleted_count = 0
    deleted_size = 0
    
    print(f"üìã Fichiers √† conserver: {len(files_to_keep)}")
    for file_to_keep in files_to_keep:
        print(f"   ‚úÖ {file_to_keep}")
    
    print(f"\nüóëÔ∏è  Fichiers √† supprimer: {len(files_to_delete)}")
    
    for file_to_delete in files_to_delete:
        try:
            file_size = file_to_delete.stat().st_size
            print(f"   üóëÔ∏è  {file_to_delete.name} ({file_size/1024:.1f} KB)")
            file_to_delete.unlink()
            deleted_count += 1
            deleted_size += file_size
        except Exception as e:
            print(f"   ‚ùå Erreur suppression {file_to_delete.name}: {e}")
    
    print(f"\nüìä R√âSUM√â DU NETTOYAGE")
    print(f"   üóëÔ∏è  Fichiers supprim√©s: {deleted_count}")
    print(f"   üíæ Espace lib√©r√©: {deleted_size/1024:.1f} KB")
    print(f"   üìÅ Fichiers conserv√©s: {len(files_to_keep)}")
    
    return deleted_count > 0

def get_consolidated_file_path():
    """Retourne le chemin du fichier consolid√©"""
    return Path("keno/keno_data/keno_consolidated.csv")

def is_consolidated_file_available():
    """V√©rifie si le fichier consolid√© existe et est r√©cent"""
    consolidated_file = get_consolidated_file_path()
    
    if not consolidated_file.exists():
        return False
    
    # V√©rifier si le fichier est plus r√©cent que les fichiers individuels
    consolidated_time = consolidated_file.stat().st_mtime
    data_dir = Path("keno/keno_data")
    
    for csv_file in data_dir.glob("*.csv"):
        if not csv_file.name.startswith("keno_consolidated"):
            if csv_file.stat().st_mtime > consolidated_time:
                return False  # Un fichier individuel est plus r√©cent
    
    return True

if __name__ == "__main__":
    # Assemblage avec nettoyage automatique par d√©faut
    success = assemble_keno_data(auto_cleanup=True)
    sys.exit(0 if success else 1)
