import os
import duckdb
import pandas as pd
import numpy as np
from dotenv import load_dotenv
import xgboost as xgb
from itertools import combinations
from pathlib import Path
from joblib import dump, load, Parallel, delayed
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import warnings
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, as_completed
import pickle
import scipy.stats
from numba import njit
import random
import argparse
import sys
import redis
import stumpy
from statsmodels.tsa.seasonal import STL
from scipy.fft import fft, fftfreq
import json
import time

warnings.filterwarnings("ignore")

# --- Configuration G√©n√©rale et Reproductibilit√© ---
load_dotenv()

# Configuration des chemins pour loto/data
SCRIPT_DIR = Path(__file__).parent
DATA_DIR = SCRIPT_DIR / 'data'
DATA_DIR.mkdir(exist_ok=True)

# Fonction pour convertir CSV en Parquet si n√©cessaire
def ensure_parquet_file():
    """S'assure qu'un fichier Parquet existe, en convertissant le CSV si n√©cessaire"""
    parquet_files = list(DATA_DIR.glob('*.parquet'))
    csv_files = list(DATA_DIR.glob('*.csv'))
    
    if parquet_files:
        # V√©rifier si le Parquet est plus r√©cent que le CSV
        parquet_path = parquet_files[0]
        if csv_files:
            csv_path = csv_files[0]
            if csv_path.stat().st_mtime > parquet_path.stat().st_mtime:
                print(f"ÔøΩ Le fichier CSV est plus r√©cent, reconversion n√©cessaire...")
                convert_csv_to_parquet(csv_path, parquet_path)
        return parquet_path
    elif csv_files:
        # Pas de Parquet, mais CSV disponible - conversion automatique
        csv_path = csv_files[0]
        parquet_path = DATA_DIR / csv_path.with_suffix('.parquet').name
        print(f"üìÇ Fichier CSV trouv√© : {csv_path.name}")
        print(f"üîÑ Conversion automatique en Parquet pour de meilleures performances...")
        convert_csv_to_parquet(csv_path, parquet_path)
        return parquet_path
    else:
        # Aucun fichier trouv√© - fallback
        fallback_path = Path(os.getenv('LOTO_PARQUET_PATH', '~/T√©l√©chargements/loto_201911.parquet')).expanduser()
        print(f"‚ö†Ô∏è  Aucun fichier CSV/Parquet trouv√© dans {DATA_DIR}")
        print(f"‚ö†Ô∏è  Utilisation du fallback : {fallback_path}")
        return fallback_path

def convert_csv_to_parquet(csv_path, parquet_path):
    """Convertit un fichier CSV en Parquet en utilisant DuckDB"""
    import duckdb
    try:
        con = duckdb.connect()
        con.execute(f"COPY (SELECT * FROM read_csv_auto('{str(csv_path)}')) TO '{str(parquet_path)}' (FORMAT PARQUET);")
        
        # V√©rifier la taille des fichiers pour information
        csv_size = csv_path.stat().st_size / (1024*1024)
        parquet_size = parquet_path.stat().st_size / (1024*1024)
        compression_ratio = (1 - parquet_size/csv_size) * 100 if csv_size > 0 else 0
        
        print(f"‚úÖ Conversion termin√©e : {csv_path.name} ‚Üí {parquet_path.name}")
        print(f"üìä Compression : {compression_ratio:.1f}% ({csv_size:.1f}MB ‚Üí {parquet_size:.1f}MB)")
        con.close()
    except Exception as e:
        print(f"‚ùå Erreur lors de la conversion : {e}")
        raise

# Obtenir le fichier Parquet (avec conversion automatique si n√©cessaire)
parquet_path = ensure_parquet_file()
print(f"üìÇ Utilisation du fichier Parquet : {parquet_path}")

def parse_arguments():
    """Parse les arguments de ligne de commande"""
    parser = argparse.ArgumentParser(
        description="G√©n√©rateur Loto Avanc√© avec IA et Machine Learning",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples d'utilisation :
  python loto_generator_advanced_Version2.py                    # Configuration par d√©faut
  python loto_generator_advanced_Version2.py -s 5000          # 5000 simulations
  python loto_generator_advanced_Version2.py -c 4             # 4 processeurs
  python loto_generator_advanced_Version2.py -s 20000 -c 8    # 20k simulations sur 8 c≈ìurs
  python loto_generator_advanced_Version2.py --quick          # Mode rapide (1000 simulations)
  python loto_generator_advanced_Version2.py --intensive      # Mode intensif (50000 simulations)
  python loto_generator_advanced_Version2.py --exclude 1,5,12 # Exclure les num√©ros 1, 5 et 12
  python loto_generator_advanced_Version2.py --exclude auto   # Exclure les 3 derniers tirages (d√©faut)
        """
    )
    
    parser.add_argument('-s', '--simulations', 
                        type=int, 
                        default=10000,
                        help='Nombre de simulations √† effectuer (d√©faut: 10000)')
    
    parser.add_argument('-c', '--cores', 
                        type=int, 
                        default=None,
                        help=f'Nombre de processeurs √† utiliser (d√©faut: auto = {mp.cpu_count() - 1 if mp.cpu_count() > 1 else 1})')
    
    parser.add_argument('--quick', 
                        action='store_true',
                        help='Mode rapide : 1000 simulations')
    
    parser.add_argument('--intensive', 
                        action='store_true',
                        help='Mode intensif : 50000 simulations')
    
    parser.add_argument('--seed', 
                        type=int, 
                        default=None,
                        help='Graine pour la reproductibilit√© (d√©faut: al√©atoire)')
    
    parser.add_argument('--silent', 
                        action='store_true',
                        help='Mode silencieux (pas de confirmation)')
    
    parser.add_argument('--exclude', 
                        type=str, 
                        default=None,
                        help='Num√©ros √† exclure, s√©par√©s par des virgules (ex: --exclude 1,5,12,30) ou "auto" pour les 3 derniers tirages (d√©faut: auto)')
    
    parser.add_argument('--retrain', 
                        action='store_true',
                        help='Force le r√©-entra√Ænement des mod√®les ML pour compatibilit√© avec les nouvelles features')
    
    args = parser.parse_args()
    
    # Gestion des modes pr√©d√©finis
    if args.quick:
        args.simulations = 1000
    elif args.intensive:
        args.simulations = 50000
    
    # Validation des arguments
    if args.simulations < 100:
        print("‚ùå Erreur: Le nombre de simulations doit √™tre d'au moins 100")
        sys.exit(1)
    
    if args.simulations > 100000:
        print("‚ö†Ô∏è  Attention: Plus de 100,000 simulations peuvent prendre beaucoup de temps")
        if not args.silent:
            confirm = input("Continuer ? (o/N): ").strip().lower()
            if confirm != 'o':
                sys.exit(0)
    
    # Validation des num√©ros exclus
    if args.exclude and args.exclude.lower() != 'auto':
        try:
            excluded_nums = [int(x.strip()) for x in args.exclude.split(',')]
            # V√©rifier que tous les num√©ros sont valides (1-49)
            invalid_nums = [num for num in excluded_nums if num < 1 or num > 49]
            if invalid_nums:
                print(f"‚ùå Erreur: Num√©ros invalides d√©tect√©s: {invalid_nums}. Les num√©ros doivent √™tre entre 1 et 49.")
                sys.exit(1)
            # V√©rifier qu'il ne faut pas exclure trop de num√©ros
            if len(excluded_nums) > 44:  # Il faut au moins 5 num√©ros pour faire une grille
                print(f"‚ùå Erreur: Trop de num√©ros exclus ({len(excluded_nums)}). Maximum autoris√©: 44.")
                sys.exit(1)
            args.excluded_numbers = set(excluded_nums)
        except ValueError:
            print("‚ùå Erreur: Format invalide pour --exclude. Utilisez des num√©ros s√©par√©s par des virgules (ex: 1,5,12)")
            sys.exit(1)
    else:
        args.excluded_numbers = None  # Sera d√©fini plus tard avec les 3 derniers tirages
    
    # Configuration automatique des cores si non sp√©cifi√©
    if args.cores is None:
        args.cores = mp.cpu_count() - 1 if mp.cpu_count() > 1 else 1
    elif args.cores < 1:
        args.cores = 1
    elif args.cores > mp.cpu_count():
        print(f"‚ö†Ô∏è  Limitation: {args.cores} c≈ìurs demand√©s, mais seulement {mp.cpu_count()} disponibles")
        args.cores = mp.cpu_count()
    
    return args

# Configuration globale (sera mise √† jour par les arguments)
ARGS = parse_arguments()

# Gestion de la seed : al√©atoire si non sp√©cifi√©e
if ARGS.seed is None:
    import time
    GLOBAL_SEED = int(time.time()) % 2**31  # Seed bas√©e sur le timestamp
else:
    GLOBAL_SEED = ARGS.seed

N_SIMULATIONS = ARGS.simulations
N_CORES = ARGS.cores
EXCLUDED_NUMBERS = ARGS.excluded_numbers

random.seed(GLOBAL_SEED)
np.random.seed(GLOBAL_SEED)

BALLS = np.arange(1, 50)
CHANCE_BALLS = np.arange(1, 11)

print(f"Configuration : {N_CORES} processeurs, {N_SIMULATIONS:,} grilles, SEED={GLOBAL_SEED}.")

# --- Gestion des Dossiers & Connexion Redis ---
BASE_DIR = Path.cwd()
OUTPUT_DIR = BASE_DIR / 'output'
MODEL_DIR = BASE_DIR / 'boost_models'
for directory in [OUTPUT_DIR, MODEL_DIR]:
    directory.mkdir(exist_ok=True)

# --- Strat√©gie Adaptative ---
class AdaptiveStrategy:
    """Strat√©gie adaptative qui ajuste les param√®tres selon les performances r√©centes"""
    
    def __init__(self, history_file=None):
        self.history_file = history_file or (MODEL_DIR / 'performance_history.json')
        self.performance_history = self.load_history()
        self.ml_weight = 0.6  # Poids initial pour ML vs fr√©quences
        self.adaptation_rate = 0.1  # Vitesse d'adaptation
        
    def load_history(self):
        """Charge l'historique des performances"""
        if self.history_file.exists():
            try:
                with open(self.history_file, 'r') as f:
                    return json.load(f)
            except:
                pass
        return {
            'predictions': [],
            'actuals': [],
            'ml_scores': [],
            'freq_scores': [],
            'dates': [],
            'ml_weight_history': []
        }
    
    def save_history(self):
        """Sauvegarde l'historique des performances"""
        try:
            with open(self.history_file, 'w') as f:
                json.dump(self.performance_history, f, indent=2)
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur sauvegarde historique: {e}")
    
    def evaluate_prediction_accuracy(self, predicted_probs, actual_draw):
        """√âvalue la pr√©cision des pr√©dictions ML vs fr√©quences avec mod√®les binaires"""
        if len(actual_draw) != 5:
            return 0.0, 0.0
        
        # Avec les mod√®les binaires, predicted_probs contient les probabilit√©s pour chaque boule (1-49)
        if isinstance(predicted_probs, np.ndarray) and len(predicted_probs) == 49:
            # Score ML : probabilit√© moyenne des boules tir√©es (predicted_probs[boule-1])
            ml_score = np.mean([predicted_probs[boule-1] for boule in actual_draw])
        else:
            # Fallback si format ancien ou erreur
            ml_score = 0.0
            
        # Score fr√©quences : score bas√© sur les fr√©quences historiques
        freq_weights = np.ones(49) / 49  # Uniforme en fallback
        freq_score = np.mean([freq_weights[boule-1] for boule in actual_draw])
        
        return ml_score, freq_score
    
    def update_performance(self, prediction, actual_draw, ml_probs=None):
        """Met √† jour les performances et ajuste la strat√©gie"""
        if ml_probs is not None:
            ml_score, freq_score = self.evaluate_prediction_accuracy(ml_probs, actual_draw)
            
            # Ajouter √† l'historique
            self.performance_history['predictions'].append(prediction)
            self.performance_history['actuals'].append(actual_draw)
            self.performance_history['ml_scores'].append(float(ml_score))
            self.performance_history['freq_scores'].append(float(freq_score))
            self.performance_history['dates'].append(datetime.now().isoformat())
            
            # Adapter le poids ML
            self.adapt_ml_weight(ml_score, freq_score)
            
            # Garder seulement les 100 derni√®res pr√©dictions
            for key in self.performance_history:
                if len(self.performance_history[key]) > 100:
                    self.performance_history[key] = self.performance_history[key][-100:]
            
            self.save_history()
    
    def adapt_ml_weight(self, ml_score, freq_score):
        """Adapte le poids du ML selon les performances relatives"""
        if len(self.performance_history['ml_scores']) >= 10:  # Minimum 10 observations
            recent_ml = np.mean(self.performance_history['ml_scores'][-10:])
            recent_freq = np.mean(self.performance_history['freq_scores'][-10:])
            
            # Si ML performe mieux, augmenter son poids
            if recent_ml > recent_freq:
                self.ml_weight = min(0.9, self.ml_weight + self.adaptation_rate)
            else:
                self.ml_weight = max(0.1, self.ml_weight - self.adaptation_rate)
                
            self.performance_history['ml_weight_history'].append(float(self.ml_weight))
    
    def get_adaptive_weights(self):
        """Retourne les poids adapt√©s pour ML vs fr√©quences"""
        return self.ml_weight, 1.0 - self.ml_weight
    
    def get_performance_summary(self):
        """Retourne un r√©sum√© des performances"""
        if len(self.performance_history['ml_scores']) < 5:
            return "Donn√©es insuffisantes pour l'analyse adaptative"
        
        recent_ml = np.mean(self.performance_history['ml_scores'][-10:])
        recent_freq = np.mean(self.performance_history['freq_scores'][-10:])
        
        return {
            'ml_score_recent': recent_ml,
            'freq_score_recent': recent_freq,
            'current_ml_weight': self.ml_weight,
            'total_predictions': len(self.performance_history['ml_scores'])
        }

# Initialiser la strat√©gie adaptative
adaptive_strategy = AdaptiveStrategy()

def update_adaptive_strategy_with_recent_draws(con, strategy):
    """Met √† jour la strat√©gie adaptative avec les tirages r√©cents"""
    try:
        # R√©cup√©rer les 5 derniers tirages pour √©valuation
        recent_draws = con.execute("""
            SELECT boule_1, boule_2, boule_3, boule_4, boule_5, date_de_tirage
            FROM loto_draws 
            ORDER BY date_de_tirage DESC 
            LIMIT 5
        """).fetchdf()
        
        if len(recent_draws) >= 2:
            print("   üéØ Mise √† jour de la strat√©gie adaptative...")
            
            # Simuler des pr√©dictions pour les tirages pass√©s
            for i in range(1, min(len(recent_draws), 4)):
                actual_draw = recent_draws.iloc[i-1][['boule_1', 'boule_2', 'boule_3', 'boule_4', 'boule_5']].values
                previous_draw = recent_draws.iloc[i][['boule_1', 'boule_2', 'boule_3', 'boule_4', 'boule_5']].values
                
                # Mettre √† jour la strat√©gie (sans probabilities ML pour simplifier)
                strategy.update_performance(
                    prediction=previous_draw.tolist(), 
                    actual_draw=actual_draw.tolist()
                )
            
            # Afficher le r√©sum√© des performances
            summary = strategy.get_performance_summary()
            if isinstance(summary, dict):
                print(f"      ML Weight: {summary['current_ml_weight']:.2f}")
                print(f"      Total Predictions: {summary['total_predictions']}")
    except Exception as e:
        print(f"   ‚ö†Ô∏è Erreur mise √† jour strat√©gie: {e}")

try:
    redis_client = redis.Redis(host='localhost', port=6379, db=0, decode_responses=False)
    redis_client.ping()
    print("‚úì Connexion √† Redis r√©ussie.")
except redis.exceptions.ConnectionError:
    print("‚ö†Ô∏è Impossible de se connecter √† Redis. Le caching sera d√©sactiv√©.")
    redis_client = None

# --- Fonctions de Scoring Optimis√©es ---
@njit
def _count_consecutive_numba(grid: np.ndarray) -> int:
    if len(grid) < 2:
        return 0
    count = 0
    sorted_grid = np.sort(grid)
    for i in range(len(sorted_grid) - 1):
        if sorted_grid[i+1] - sorted_grid[i] == 1:
            count += 1
    return count

def count_consecutive_safe(row_values, balls_cols):
    grid = np.array([row_values[col] for col in balls_cols], dtype=np.int32)
    return _count_consecutive_numba(grid)

# --- Extraction des features cycliques ---
def add_cyclic_features(df, date_col='date_de_tirage'):
    df2 = df.copy()
    if date_col in df2.columns:
        df2[date_col] = pd.to_datetime(df2[date_col])
        df2['dayofweek'] = df2[date_col].dt.dayofweek
        df2['month'] = df2[date_col].dt.month
        df2['sin_day'] = np.sin(2 * np.pi * df2['dayofweek'] / 7)
        df2['cos_day'] = np.cos(2 * np.pi * df2['dayofweek'] / 7)
        df2['sin_month'] = np.sin(2 * np.pi * df2['month'] / 12)
        df2['cos_month'] = np.cos(2 * np.pi * df2['month'] / 12)
    for i in range(1, 6):
        col = f'boule_{i}'
        df2[f'sin_boule_{i}'] = np.sin(2 * np.pi * df2[col] / 49)
        df2[f'cos_boule_{i}'] = np.cos(2 * np.pi * df2[col] / 49)
    return df2

# --- Fonctions d'Analyse (DuckDB) ---
def analyze_criteria_duckdb(db_con: duckdb.DuckDBPyConnection, table_name: str) -> dict:
    print("D√©but de l'analyse des crit√®res avec DuckDB...")

    db_con.execute(f"""
        CREATE OR REPLACE TEMPORARY VIEW BaseData AS
        SELECT ROW_NUMBER() OVER (ORDER BY date_de_tirage) AS draw_index, 
               date_de_tirage::DATE as date_de_tirage,
               boule_1, boule_2, boule_3, boule_4, boule_5
        FROM {table_name};
    """)

    df_pandas = db_con.table('BaseData').fetchdf()
    balls_cols = [f'boule_{i}' for i in range(1, 6)]

    freq = pd.Series(df_pandas[balls_cols].values.flatten()).value_counts().reindex(BALLS, fill_value=0)
    last_3_draws = df_pandas.iloc[-3:][balls_cols].values.flatten()
    last_3_numbers_set = set(last_3_draws)
    
    # Utiliser les num√©ros exclus configurables
    if EXCLUDED_NUMBERS is not None:
        numbers_to_exclude = EXCLUDED_NUMBERS
        exclusion_source = "param√®tre utilisateur"
    else:
        numbers_to_exclude = last_3_numbers_set
        exclusion_source = "3 derniers tirages (auto)"
    
    print(f"   ‚úì Num√©ros exclus ({exclusion_source}): {sorted(numbers_to_exclude)}")

    last_appearance = df_pandas.melt(
        id_vars=['date_de_tirage'], 
        value_vars=balls_cols, 
        value_name='numero'
    ).groupby('numero')['date_de_tirage'].max()

    pair_counts_df = db_con.execute("""
        SELECT n1, n2, COUNT(*) as compte
        FROM (
            SELECT LEAST(boule_1, boule_2) AS n1, GREATEST(boule_1, boule_2) AS n2 FROM BaseData
            UNION ALL SELECT LEAST(boule_1, boule_3), GREATEST(boule_1, boule_3) FROM BaseData
            UNION ALL SELECT LEAST(boule_1, boule_4), GREATEST(boule_1, boule_4) FROM BaseData
            UNION ALL SELECT LEAST(boule_1, boule_5), GREATEST(boule_1, boule_5) FROM BaseData
            UNION ALL SELECT LEAST(boule_2, boule_3), GREATEST(boule_2, boule_3) FROM BaseData
            UNION ALL SELECT LEAST(boule_2, boule_4), GREATEST(boule_2, boule_4) FROM BaseData
            UNION ALL SELECT LEAST(boule_2, boule_5), GREATEST(boule_2, boule_5) FROM BaseData
            UNION ALL SELECT LEAST(boule_3, boule_4), GREATEST(boule_3, boule_4) FROM BaseData
            UNION ALL SELECT LEAST(boule_3, boule_5), GREATEST(boule_3, boule_5) FROM BaseData
            UNION ALL SELECT LEAST(boule_4, boule_5), GREATEST(boule_4, boule_5) FROM BaseData
        )
        WHERE n1 IS NOT NULL AND n2 IS NOT NULL
        GROUP BY n1, n2
        ORDER BY compte DESC
        LIMIT 20
    """).fetchdf()

    pair_counts = pd.Series(pair_counts_df.set_index(['n1', 'n2'])['compte']) if not pair_counts_df.empty else pd.Series(dtype='int64')

    gaps_query = """
        SELECT numero, AVG(draw_index - lag) as periodicite
        FROM (
            SELECT numero, draw_index, LAG(draw_index, 1) OVER (PARTITION BY numero ORDER BY draw_index) as lag
            FROM (
                SELECT draw_index, unnest(list_value(boule_1, boule_2, boule_3, boule_4, boule_5)) as numero
                FROM BaseData
            )
        )
        WHERE lag IS NOT NULL
        GROUP BY numero
    """
    gaps_df = db_con.execute(gaps_query).fetchdf()
    gaps = pd.Series(gaps_df.set_index('numero')['periodicite']) if not gaps_df.empty else pd.Series(dtype='float64', index=BALLS)

    pair_impair_dist = (df_pandas[balls_cols] % 2 == 0).sum(axis=1).value_counts().sort_index()

    consecutive_list = []
    for _, row in df_pandas.iterrows():
        grid = np.array([row[col] for col in balls_cols], dtype=np.int32)
        consecutive_list.append(_count_consecutive_numba(grid))
    consecutive_counts = pd.Series(consecutive_list).value_counts().sort_index()

    hot_numbers = freq.nlargest(10).index.tolist()
    cold_numbers = freq.nsmallest(10).index.tolist()

    correlation_matrix = df_pandas[balls_cols].corr()

    regression_results = {}
    for ball in balls_cols:
        result = db_con.execute(f"""
            SELECT draw_index, {ball} as value
            FROM BaseData
            ORDER BY draw_index
        """).fetchdf()
        X = result['draw_index'].values.reshape(-1, 1)
        y = result['value'].values
        slope, intercept = np.polyfit(X.flatten(), y, 1)
        regression_results[ball] = {'slope': slope, 'intercept': intercept}

    sums = df_pandas[balls_cols].sum(axis=1)
    stds = df_pandas[balls_cols].std(axis=1)

    # === NOUVEAUX CRIT√àRES STATISTIQUES ===
    
    # 1. Distribution par dizaines (0-9, 10-19, 20-29, 30-39, 40-49)
    decades_dist = []
    for _, row in df_pandas.iterrows():
        grid = row[balls_cols].values
        decades = [(n-1)//10 for n in grid]  # 0,1,2,3,4 pour les dizaines
        decades_count = pd.Series(decades).value_counts()
        decades_dist.append(decades_count.reindex(range(5), fill_value=0).values)
    decades_dist = np.array(decades_dist)
    decades_entropy = -np.sum(decades_dist * np.log(decades_dist + 1e-10), axis=1)
    
    # 2. Espacement entre num√©ros cons√©cutifs (gaps)
    gaps_between_numbers = []
    for _, row in df_pandas.iterrows():
        sorted_nums = sorted(row[balls_cols].values)
        local_gaps = [sorted_nums[i+1] - sorted_nums[i] for i in range(4)]
        gaps_between_numbers.append(np.std(local_gaps))  # √âcart-type des espacements
    gaps_between_numbers = np.array(gaps_between_numbers)
    
    # 3. R√©partition haute/basse (1-25 vs 26-49)
    high_low_ratio = []
    for _, row in df_pandas.iterrows():
        grid = row[balls_cols].values
        low_count = sum(1 for n in grid if n <= 25)
        high_count = 5 - low_count
        ratio = low_count / 5.0  # Proportion de num√©ros bas
        high_low_ratio.append(ratio)
    high_low_ratio = np.array(high_low_ratio)
    
    # 4. Nombres premiers vs compos√©s
    primes = {2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47}
    prime_ratio = []
    for _, row in df_pandas.iterrows():
        grid = row[balls_cols].values
        prime_count = sum(1 for n in grid if n in primes)
        prime_ratio.append(prime_count / 5.0)
    prime_ratio = np.array(prime_ratio)
    
    # 5. Variance des positions (mesure de dispersion)
    position_variance = []
    for _, row in df_pandas.iterrows():
        grid = sorted(row[balls_cols].values)
        positions = [(n-1)/48.0 for n in grid]  # Normalisation 0-1
        position_variance.append(np.var(positions))
    position_variance = np.array(position_variance)

    # Calcul des variances pour tous les crit√®res
    sum_var = np.var(sums)
    std_var = np.var(stds)
    pair_impair_var = np.var((pair_impair_dist/pair_impair_dist.sum()).values)
    decades_entropy_var = np.var(decades_entropy)
    gaps_var = np.var(gaps_between_numbers)
    high_low_var = np.var(high_low_ratio)
    prime_var = np.var(prime_ratio)
    position_var = np.var(position_variance)
    
    # Poids √©quilibr√©s bas√©s sur l'inverse des variances, mais avec normalisation
    raw_weights = {
        'sum': 1.0 / (1.0 + sum_var),
        'std': 1.0 / (1.0 + std_var), 
        'pair_impair': 1.0 / (1.0 + pair_impair_var),
        'decades_entropy': 1.0 / (1.0 + decades_entropy_var),
        'gaps': 1.0 / (1.0 + gaps_var),
        'high_low': 1.0 / (1.0 + high_low_var),
        'prime_ratio': 1.0 / (1.0 + prime_var),
        'position_variance': 1.0 / (1.0 + position_var)
    }
    
    # Normalisation pour que les poids soient plus √©quilibr√©s (pas de domination extr√™me)
    total_weight = sum(raw_weights.values())
    dynamic_weights = {key: value / total_weight for key, value in raw_weights.items()}
    
    # Ajustement pour √©viter qu'un crit√®re domine compl√®tement (max 25% pour un crit√®re maintenant)
    max_weight = 0.25
    for key in dynamic_weights:
        if dynamic_weights[key] > max_weight:
            excess = dynamic_weights[key] - max_weight
            dynamic_weights[key] = max_weight
            # Redistribuer l'exc√®s sur les autres crit√®res
            other_keys = [k for k in dynamic_weights.keys() if k != key]
            for other_key in other_keys:
                dynamic_weights[other_key] += excess / len(other_keys)
    
    # Re-normalisation finale
    total_weight = sum(dynamic_weights.values())
    dynamic_weights = {key: value / total_weight for key, value in dynamic_weights.items()}

    delta = pd.to_datetime('now').normalize() - pd.to_datetime(last_appearance.reindex(BALLS))
    numbers_analysis = pd.DataFrame({
        'Numero': BALLS,
        'Frequence': freq.reindex(BALLS, fill_value=0),
        'Dernier_Tirage': last_appearance.reindex(BALLS),
        'Jours_Depuis_Tirage': delta.dt.days,
        'Ecart_Moyen_Tirages': gaps.reindex(BALLS)
    }).sort_values('Frequence', ascending=False).reset_index(drop=True)

    print("   ‚úì Analyse DuckDB termin√©e.")

    return {
        'freq': freq,
        'hot_numbers': hot_numbers,
        'cold_numbers': cold_numbers,
        'last_draw': df_pandas[balls_cols].iloc[-1].tolist(),
        'pair_impair_probs': pair_impair_dist / pair_impair_dist.sum(),
        'consecutive_counts': consecutive_counts,
        'pair_counts': pair_counts,
        'numbers_to_exclude': numbers_to_exclude,
        'sums': sums,
        'stds': stds,
        'dynamic_weights': dynamic_weights,
        'last_appearance': last_appearance,
        'numbers_analysis': numbers_analysis,
        'correlation_matrix': correlation_matrix,
        'regression_results': regression_results,
        # Nouveaux crit√®res statistiques
        'decades_entropy': decades_entropy,
        'gaps_between_numbers': gaps_between_numbers,
        'high_low_ratio': high_low_ratio,
        'prime_ratio': prime_ratio,
        'position_variance': position_variance
    }

# --- Fonctions d'analyse de cycles et motifs ---
def autocorrelation_analysis(series, max_lag=50):
    autocorr = [series.autocorr(lag) for lag in range(1, max_lag + 1)]
    return pd.Series(autocorr, index=range(1, max_lag + 1))

def plot_autocorrelation(series, output_dir, max_lag=50):
    ac = autocorrelation_analysis(series, max_lag)
    plt.figure(figsize=(10, 5))
    plt.bar(ac.index, ac.values)
    plt.title("Autocorr√©lation des tirages (lags)")
    plt.xlabel("D√©calage (lag)")
    plt.ylabel("Autocorr√©lation")
    plt.tight_layout()
    plt.savefig(output_dir / 'autocorrelation_plot.png', dpi=150)
    plt.close()

def fft_analysis(series):
    N = len(series)
    yf = fft(series - np.mean(series))
    xf = fftfreq(N, 1)[:N // 2]
    spectrum = np.abs(yf[:N // 2])
    return xf, spectrum

def plot_fft(series, output_dir):
    xf, spectrum = fft_analysis(series)
    plt.figure(figsize=(10, 5))
    plt.plot(xf, spectrum)
    plt.title("Spectre de fr√©quences (FFT)")
    plt.xlabel("Fr√©quence")
    plt.ylabel("Amplitude")
    plt.tight_layout()
    plt.savefig(output_dir / 'fft_spectrum_plot.png', dpi=150)
    plt.close()

def seasonal_decomposition(series, period=10):
    res = STL(series, period=period).fit()
    return res

def plot_seasonal_decomposition(series, output_dir, period=10):
    res = seasonal_decomposition(series, period)
    res.plot()
    plt.tight_layout()
    plt.savefig(output_dir / 'stl_decomposition_plot.png', dpi=150)
    plt.close()

def matrix_profile_motifs(series, window=10):
    mp = stumpy.stump(series.values, m=window)
    motifs = stumpy.motifs(series.values, mp, max_motifs=5)
    return motifs

def plot_matrix_profile(series, output_dir, window=10):
    mp = stumpy.stump(series.values.astype(np.float64), m=window)
    plt.figure(figsize=(10,5))
    plt.plot(mp[:,0], label="Matrix Profile")
    plt.title(f"Matrix Profile (window={window})")
    plt.xlabel("Index")
    plt.ylabel("Distance")
    plt.legend()
    plt.tight_layout()
    plt.savefig(output_dir / 'matrix_profile_plot.png', dpi=150)
    plt.close()

# --- Fonctions de Machine Learning ---
def train_xgboost_parallel(df: pd.DataFrame):
    print("Entra√Ænement des mod√®les XGBoost...")
    balls_cols = [f'boule_{i}' for i in range(1, 6)]

    df_features = add_cyclic_features(df)
    feature_cols = balls_cols + [col for col in df_features.columns if col.startswith('sin_') or col.startswith('cos_')]
    X = df_features[feature_cols].iloc[:-1].values
    
    def train_ball_model(ball_num):
        """Entra√Æne un mod√®le binaire pour pr√©dire si une boule sera tir√©e"""
        print(f"  - Entra√Ænement mod√®le boule principale {ball_num}/49...")
        
        # Cr√©er les labels binaires : 1 si la boule est tir√©e, 0 sinon
        y = np.zeros(len(X))
        for i, row in enumerate(df_features[balls_cols].iloc[1:].values):
            if ball_num in row:
                y[i] = 1
        
        # V√©rifier qu'il y a assez d'occurrences positives
        positive_samples = np.sum(y)
        if positive_samples < 10:
            print(f"   ‚ö†Ô∏è  Boule {ball_num}: seulement {positive_samples} occurrences")
        
        model = xgb.XGBClassifier(
            n_estimators=100,
            random_state=GLOBAL_SEED,
            use_label_encoder=False,
            objective='binary:logistic',
            n_jobs=1
        )
        model.fit(X, y)
        dump(model, MODEL_DIR / f'model_ball_{ball_num}.joblib')
        return model
    
    def train_chance_model(chance_num):
        """Entra√Æne un mod√®le binaire pour pr√©dire si un num√©ro chance sera tir√©"""
        print(f"  - Entra√Ænement mod√®le num√©ro chance {chance_num}/10...")
        
        # Cr√©er les labels binaires pour le num√©ro chance
        y = np.zeros(len(X))
        for i, chance_val in enumerate(df_features['numero_chance'].iloc[1:].values):
            if chance_val == chance_num:
                y[i] = 1
        
        positive_samples = np.sum(y)
        if positive_samples < 5:
            print(f"   ‚ö†Ô∏è  Chance {chance_num}: seulement {positive_samples} occurrences")
        
        model = xgb.XGBClassifier(
            n_estimators=100,
            random_state=GLOBAL_SEED,
            use_label_encoder=False,
            objective='binary:logistic',
            n_jobs=1
        )
        model.fit(X, y)
        dump(model, MODEL_DIR / f'model_chance_{chance_num}.joblib')
        return model
    
    print("  üìä Entra√Ænement de 49 mod√®les pour les boules principales...")
    ball_models = Parallel(n_jobs=min(8, N_CORES))(
        delayed(train_ball_model)(ball) for ball in range(1, 50)
    )
    
    print("  üéØ Entra√Ænement de 10 mod√®les pour les num√©ros chance...")
    chance_models = Parallel(n_jobs=min(8, N_CORES))(
        delayed(train_chance_model)(chance) for chance in range(1, 11)
    )
    
    models = ball_models + chance_models
    
    # Sauvegarder les m√©tadonn√©es
    metadata = {
        'features_count': len(feature_cols),
        'model_type': 'xgboost_binary',
        'created_date': datetime.now().strftime('%Y-%m-%d'),
        'version': '3.0',
        'ball_models': 49,
        'chance_models': 10,
        'total_models': 59
    }
    with open(MODEL_DIR / 'metadata.json', 'w') as f:
        json.dump(metadata, f, indent=4)
    
    print("   ‚úì Entra√Ænement termin√© et mod√®les sauvegard√©s.")
    return models

def load_saved_models() -> dict:
    """Charge les 59 mod√®les binaires (49 boules + 10 num√©ros chance)"""
    models = {'balls': {}, 'chance': {}}
    
    # V√©rifier la compatibilit√© des features
    metadata_path = MODEL_DIR / 'metadata.json'
    if metadata_path.exists():
        import json
        with open(metadata_path, 'r') as f:
            metadata = json.load(f)
        expected_features = metadata.get('features_count', 19)
        total_models = metadata.get('total_models', 59)
        print(f"   üìä Mod√®les attendus avec {expected_features} features ({total_models} mod√®les)")
        
        # Si incompatibilit√© d√©tect√©e, signaler
        if expected_features != 19:  # 19 = nouvelles features avec cycliques compl√®tes
            print(f"   ‚ö†Ô∏è INCOMPATIBILIT√â: Mod√®les avec {expected_features} features, code actuel avec 19 features")
            print(f"   üîÑ Recommandation: R√©-entra√Æner les mod√®les avec --retrain")
    
    # Charger les mod√®les pour les boules (1-49)
    loaded_balls = 0
    for ball in range(1, 50):
        model_path = MODEL_DIR / f'model_ball_{ball}.joblib'
        if model_path.exists():
            try:
                models['balls'][ball] = load(model_path)
                loaded_balls += 1
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur chargement mod√®le boule {ball}: {e}")
                models['balls'][ball] = None
        else:
            models['balls'][ball] = None
    
    # Charger les mod√®les pour les num√©ros chance (1-10)
    loaded_chance = 0
    for chance in range(1, 11):
        model_path = MODEL_DIR / f'model_chance_{chance}.joblib'
        if model_path.exists():
            try:
                models['chance'][chance] = load(model_path)
                loaded_chance += 1
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur chargement mod√®le chance {chance}: {e}")
                models['chance'][chance] = None
        else:
            models['chance'][chance] = None
    
    total_loaded = loaded_balls + loaded_chance
    if total_loaded == 59:
        print(f"   ‚úì 59 mod√®les XGBoost binaires charg√©s depuis '{MODEL_DIR}' ({loaded_balls} boules + {loaded_chance} chance).")
    else:
        print(f"   ‚ö†Ô∏è Seulement {total_loaded}/59 mod√®les charg√©s ({loaded_balls}/49 boules + {loaded_chance}/10 chance)")
    
    return models

# --- Fonctions de Scoring et G√©n√©ration ---
def score_grid(grid: np.ndarray, criteria: dict, diversity_factor: float = 0.0) -> float:
    W = criteria['dynamic_weights']
    W_DECADES, W_PAIRS, W_CONSECUTIVE, PENALTY_OVERLAP = 0.05, 0.05, 0.05, 0.1
    W_HOT_NUMBERS = 0.03
    PENALTY_TOO_MANY_HOT = 0.08

    score = 0
    
    # Crit√®res principaux avec poids dynamiques
    score += W.get('sum', 0.125) * scipy.stats.norm.pdf(
        np.sum(grid), 
        loc=criteria['sums'].mean(), 
        scale=criteria['sums'].std()
    )

    score += W.get('std', 0.125) * scipy.stats.norm.pdf(
        np.std(grid), 
        loc=criteria['stds'].mean(), 
        scale=criteria['stds'].std()
    )

    score += W.get('pair_impair', 0.125) * criteria['pair_impair_probs'].get(np.sum(grid % 2 == 0), 0)
    
    # === NOUVEAUX CRIT√àRES STATISTIQUES ===
    
    # 1. Entropie des dizaines
    grid_decades = [(n-1)//10 for n in grid]
    grid_decades_count = pd.Series(grid_decades).value_counts()
    grid_decades_dist = grid_decades_count.reindex(range(5), fill_value=0).values
    grid_decades_entropy = -np.sum(grid_decades_dist * np.log(grid_decades_dist + 1e-10))
    score += W.get('decades_entropy', 0.125) * scipy.stats.norm.pdf(
        grid_decades_entropy,
        loc=criteria['decades_entropy'].mean(),
        scale=criteria['decades_entropy'].std() + 1e-10
    )
    
    # 2. Espacement entre num√©ros
    sorted_grid = sorted(grid)
    grid_gaps = [sorted_grid[i+1] - sorted_grid[i] for i in range(4)]
    grid_gaps_std = np.std(grid_gaps)
    score += W.get('gaps', 0.125) * scipy.stats.norm.pdf(
        grid_gaps_std,
        loc=criteria['gaps_between_numbers'].mean(),
        scale=criteria['gaps_between_numbers'].std() + 1e-10
    )
    
    # 3. R√©partition haute/basse
    low_count = sum(1 for n in grid if n <= 25)
    grid_high_low_ratio = low_count / 5.0
    score += W.get('high_low', 0.125) * scipy.stats.norm.pdf(
        grid_high_low_ratio,
        loc=criteria['high_low_ratio'].mean(),
        scale=criteria['high_low_ratio'].std() + 1e-10
    )
    
    # 4. Ratio de nombres premiers
    primes = {2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47}
    prime_count = sum(1 for n in grid if n in primes)
    grid_prime_ratio = prime_count / 5.0
    score += W.get('prime_ratio', 0.125) * scipy.stats.norm.pdf(
        grid_prime_ratio,
        loc=criteria['prime_ratio'].mean(),
        scale=criteria['prime_ratio'].std() + 1e-10
    )
    
    # 5. Variance des positions
    grid_positions = [(n-1)/48.0 for n in grid]
    grid_position_variance = np.var(grid_positions)
    score += W.get('position_variance', 0.125) * scipy.stats.norm.pdf(
        grid_position_variance,
        loc=criteria['position_variance'].mean(),
        scale=criteria['position_variance'].std() + 1e-10
    )

    # Crit√®res traditionnels avec poids r√©duits
    score += W_DECADES * (len(set((n - 1) // 10 for n in grid)) / 5.0)

    grid_pairs = set(combinations(sorted(grid), 2))
    score += W_PAIRS * (len(grid_pairs.intersection(criteria['pair_counts'].index)) / 2.0)

    consecutive_count = _count_consecutive_numba(grid)
    if consecutive_count == 0:
        score += W_CONSECUTIVE * 1.0
    elif consecutive_count == 1:
        score += W_CONSECUTIVE * 0.5
    else:
        score += W_CONSECUTIVE * 0.0

    score -= PENALTY_OVERLAP * len(set(grid).intersection(set(criteria['last_draw'][:5])))

    hot_count = len(set(grid).intersection(set(criteria['hot_numbers'])))
    score += W_HOT_NUMBERS * hot_count

    if hot_count > 3:
        score -= PENALTY_TOO_MANY_HOT * (hot_count - 3)

    # Ajout d'un facteur de diversit√© pour √©viter la convergence vers des grilles identiques
    if diversity_factor > 0:
        # Ajouter une petite perturbation al√©atoire bas√©e sur le hash de la grille
        grid_hash = hash(tuple(sorted(grid))) 
        np.random.seed(grid_hash % 2**31)  # Seed bas√© sur le hash pour reproductibilit√©
        diversity_bonus = np.random.normal(0, diversity_factor)
        score += diversity_bonus

    return max(0, score)

def generate_grid_vectorized(criteria: dict, models: dict, X_last: np.ndarray) -> list:
    """G√©n√©ration de grille avec 59 mod√®les binaires (49 boules + 10 chance)"""
    N_CANDIDATES, EXPLORATION_RATE, TOP_PERCENT_SELECTION = 500, 0.2, 0.05
    
    # V√©rifier si les mod√®les sont disponibles
    has_ball_models = models and 'balls' in models and any(models['balls'].values())
    has_chance_models = models and 'chance' in models and any(models['chance'].values())
    use_models = has_ball_models
    
    freq_weights = criteria['freq'].reindex(BALLS, fill_value=0).values / criteria['freq'].sum()

    # Obtenir les poids adaptatifs
    ml_weight, freq_weight = adaptive_strategy.get_adaptive_weights()

    # Ajout des features cycliques pour la grille √† pr√©dire
    # Cr√©er un DataFrame avec une date fictive pour g√©n√©rer toutes les features
    df_last = pd.DataFrame([X_last[0]], columns=[f'boule_{i}' for i in range(1, 6)])
    df_last['date_de_tirage'] = pd.Timestamp.now()  # Date fictive pour g√©n√©rer les features cycliques
    df_last_features = add_cyclic_features(df_last)
    feature_cols = [f'boule_{i}' for i in range(1, 6)] + [col for col in df_last_features.columns if col.startswith('sin_') or col.startswith('cos_')]
    X_last_features = df_last_features[feature_cols].values

    if use_models:
        try:
            # Pr√©dictions pour chaque boule (1-49) avec mod√®les binaires
            ball_predictions = np.zeros(49)
            successful_predictions = 0
            
            for ball in range(1, 50):
                model = models['balls'].get(ball)
                if model is not None:
                    try:
                        if hasattr(model, 'predict_proba'):
                            # Classification binaire : probabilit√© de la classe 1 (boule tir√©e)
                            prob = model.predict_proba(X_last_features)[0][1]
                        elif hasattr(model, 'predict'):
                            # Si pas de predict_proba, utiliser predict et convertir en probabilit√©
                            prediction = model.predict(X_last_features)[0]
                            prob = max(0.01, min(0.99, prediction))  # Borner entre 0.01 et 0.99
                        else:
                            prob = freq_weights[ball-1]  # Fallback sur fr√©quence historique
                        
                        ball_predictions[ball-1] = prob
                        successful_predictions += 1
                    except Exception as e:
                        # En cas d'erreur, utiliser la fr√©quence historique
                        ball_predictions[ball-1] = freq_weights[ball-1]
                else:
                    # Mod√®le manquant, utiliser fr√©quence historique
                    ball_predictions[ball-1] = freq_weights[ball-1]
            
            if successful_predictions > 0:
                # Normaliser les probabilit√©s
                ball_predictions = ball_predictions / ball_predictions.sum()
                # Combiner avec les fr√©quences historiques selon les poids adaptatifs
                exploitation_weights = (ml_weight * ball_predictions + freq_weight * freq_weights)
                exploitation_weights /= exploitation_weights.sum()
                
                # Affichage des poids adaptatifs si en mode debug
                if not ARGS.silent:
                    print(f"   üéØ Poids adaptatifs: ML={ml_weight:.2f}, Freq={freq_weight:.2f} ({successful_predictions}/49 mod√®les)")
            else:
                exploitation_weights = freq_weights
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur avec les mod√®les ML: {e}. Utilisation des fr√©quences historiques.")
            exploitation_weights = freq_weights
    else:
        exploitation_weights = freq_weights

    excluded_numbers = criteria.get('numbers_to_exclude', set())
    print(f"DEBUG: Num√©ros exclus: {excluded_numbers} (total: {len(excluded_numbers)})")

    available_numbers = [n for n in BALLS if n not in excluded_numbers]
    print(f"DEBUG: Num√©ros disponibles: {len(available_numbers)}/49")

    if len(available_numbers) < 5:
        print(f"‚ö†Ô∏è ATTENTION: Seulement {len(available_numbers)} num√©ros disponibles, g√©n√©ration impossible!")
        return []

    candidates = []
    max_attempts = N_CANDIDATES * 10
    attempts = 0

    while len(candidates) < N_CANDIDATES and attempts < max_attempts:
        attempts += 1
        p = freq_weights if random.random() < EXPLORATION_RATE else exploitation_weights
        candidate = np.random.choice(BALLS, size=5, replace=False, p=p)
        if not any(num in excluded_numbers for num in candidate):
            candidates.append(candidate)

    print(f"DEBUG: Candidats g√©n√©r√©s: {len(candidates)}/{N_CANDIDATES} en {attempts} tentatives")

    if not candidates:
        print("‚ùå ERREUR: Aucun candidat g√©n√©r√©!")
        return []

    candidates_matrix = np.array(candidates)
    scores = np.zeros(len(candidates))
    
    # Ajouter un facteur de diversit√© (5% de perturbation) pour √©viter la convergence
    diversity_factor = 0.05
    for i, grid in enumerate(candidates_matrix):
        scores[i] = score_grid(grid, criteria, diversity_factor)

    top_n = max(1, int(len(candidates) * TOP_PERCENT_SELECTION))
    best_indices = np.argpartition(scores, -top_n)[-top_n:]
    chosen_index = np.random.choice(best_indices)

    best_grid_list = [int(b) for b in candidates_matrix[chosen_index]]
    
    # G√©n√©ration du num√©ro chance avec mod√®les si disponibles
    if has_chance_models:
        try:
            chance_predictions = np.zeros(10)
            successful_chance_predictions = 0
            
            for chance in range(1, 11):
                model = models['chance'].get(chance)
                if model is not None:
                    try:
                        if hasattr(model, 'predict_proba'):
                            prob = model.predict_proba(X_last_features)[0][1]
                        elif hasattr(model, 'predict'):
                            prediction = model.predict(X_last_features)[0]
                            prob = max(0.01, min(0.99, prediction))
                        else:
                            prob = 1.0 / 10  # Probabilit√© uniforme
                        
                        chance_predictions[chance-1] = prob
                        successful_chance_predictions += 1
                    except Exception as e:
                        chance_predictions[chance-1] = 1.0 / 10
                else:
                    chance_predictions[chance-1] = 1.0 / 10
            
            if successful_chance_predictions > 0:
                chance_predictions = chance_predictions / chance_predictions.sum()
                chance_ball = int(np.random.choice(CHANCE_BALLS, p=chance_predictions))
                if not ARGS.silent:
                    print(f"   üé≤ Num√©ro chance pr√©dit avec {successful_chance_predictions}/10 mod√®les")
            else:
                chance_ball = int(np.random.choice(CHANCE_BALLS))
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur avec les mod√®les chance: {e}")
            chance_ball = int(np.random.choice(CHANCE_BALLS))
    else:
        chance_ball = int(np.random.choice(CHANCE_BALLS))
    
    return sorted(best_grid_list) + [chance_ball]
    max_attempts = N_CANDIDATES * 10
    attempts = 0

    while len(candidates) < N_CANDIDATES and attempts < max_attempts:
        attempts += 1
        p = freq_weights if random.random() < EXPLORATION_RATE else exploitation_weights
        candidate = np.random.choice(BALLS, size=5, replace=False, p=p)
        if not any(num in excluded_numbers for num in candidate):
            candidates.append(candidate)

    print(f"DEBUG: Candidats g√©n√©r√©s: {len(candidates)}/{N_CANDIDATES} en {attempts} tentatives")

    if not candidates:
        print("‚ùå ERREUR: Aucun candidat g√©n√©r√©!")
        return []

    candidates_matrix = np.array(candidates)
    scores = np.zeros(len(candidates))
    
    # Ajouter un facteur de diversit√© (5% de perturbation) pour √©viter la convergence
    diversity_factor = 0.05
    for i, grid in enumerate(candidates_matrix):
        scores[i] = score_grid(grid, criteria, diversity_factor)

    top_n = max(1, int(len(candidates) * TOP_PERCENT_SELECTION))
    best_indices = np.argpartition(scores, -top_n)[-top_n:]
    chosen_index = np.random.choice(best_indices)

    best_grid_list = [int(b) for b in candidates_matrix[chosen_index]]
    chance_ball = int(np.random.choice(CHANCE_BALLS))
    return sorted(best_grid_list) + [chance_ball]

# --- Simulation Parall√®le ---
def simulate_chunk(args_tuple):
    n_sims_chunk, criteria, X_last_shared, models, chunk_seed = args_tuple
    random.seed(chunk_seed)
    np.random.seed(chunk_seed)
    results = []
    for _ in range(n_sims_chunk):
        grid = generate_grid_vectorized(criteria, models, X_last_shared)
        if grid and len(grid) >= 5:
            # Pas de facteur de diversit√© dans le scoring final pour garder la pr√©cision
            score = score_grid(np.array(grid[:-1]), criteria, diversity_factor=0.0)
            results.append({'grid': grid, 'score': score})
    return results

def simulate_grids_parallel(n_simulations: int, criteria: dict, X_last: np.ndarray, models: dict):
    chunk_size = max(1, n_simulations // (N_CORES * 4))
    chunks_args = []
    sims_left, i = n_simulations, 0
    
    while sims_left > 0:
        size = min(chunk_size, sims_left)
        chunks_args.append((size, criteria, X_last, models, GLOBAL_SEED + i))
        sims_left -= size
        i += 1

    all_results = []
    print(f"Simulation de {n_simulations} grilles sur {N_CORES} coeurs...")
    print(f"Nombre de chunks: {len(chunks_args)}")
    
    with ProcessPoolExecutor(max_workers=N_CORES) as executor:
        futures = [executor.submit(simulate_chunk, args) for args in chunks_args]
        for future in tqdm(as_completed(futures), total=len(futures), desc="Simulation des grilles"):
            try:
                chunk_result = future.result()
                print(f"Chunk termin√© avec {len(chunk_result)} grilles")
                all_results.extend(chunk_result)
            except Exception as e:
                print(f"‚ùå ERREUR dans un chunk de simulation: {e}")
                import traceback
                traceback.print_exc()
    
    print(f"Total grilles g√©n√©r√©es: {len(all_results)}")
    return sorted(all_results, key=lambda x: x['score'], reverse=True)

def analyze_generated_grids(grids: list, criteria: dict):
    print("\n" + "="*50 + "\nüî¨ Contr√¥le Qualit√© des Grilles G√©n√©r√©es\n" + "="*50)
    if not grids:
        print("Aucune grille √† analyser.")
        return
    
    grids_df = pd.DataFrame([g['grid'][:-1] for g in grids])
    sums = grids_df.sum(axis=1)
    print(f"Somme des boules :")
    print(f"  - G√©n√©r√©e : Moy={sums.mean():.1f}, Std={sums.std():.1f}")
    print(f"  - Cible    : Moy={criteria['sums'].mean():.1f}, Std={criteria['sums'].std():.1f}")
    
    evens = (grids_df % 2 == 0).sum(axis=1)
    print("\nR√©partition Pair/Impair:")
    print("  - G√©n√©r√©e:")
    print(evens.value_counts(normalize=True).sort_index().to_string())

# --- Fonctions de Visualisation ---
def plot_frequency_analysis(criteria: dict, output_dir: Path):
    plt.figure(figsize=(15, 7))
    sns.barplot(x=criteria['freq'].index, y=criteria['freq'].values, palette="viridis")
    plt.title("Fr√©quence de sortie des num√©ros", fontsize=16)
    plt.xlabel('Num√©ro')
    plt.ylabel('Fr√©quence')
    plt.xticks(rotation=90, fontsize=8)
    plt.tight_layout()
    plt.savefig(output_dir / 'frequence_numeros.png', dpi=150)
    plt.close()

def plot_gap_analysis(criteria: dict, output_dir: Path):
    plt.figure(figsize=(15, 7))
    analysis_df = criteria['numbers_analysis']
    sns.barplot(x='Numero', y='Ecart_Moyen_Tirages', data=analysis_df.sort_values('Numero'), palette='YlOrRd')
    plt.title("P√©riodicit√© Moyenne par Num√©ro", fontsize=16)
    plt.xlabel('Num√©ro')
    plt.ylabel('√âcart moyen entre tirages')
    plt.xticks(rotation=90, fontsize=8)
    plt.tight_layout()
    plt.savefig(output_dir / 'periodicite_plot.png', dpi=150)
    plt.close()

def plot_odd_even_analysis(criteria: dict, output_dir: Path):
    pair_impair_probs = criteria['pair_impair_probs']
    plt.figure(figsize=(10, 5))
    sns.barplot(x=pair_impair_probs.index, y=pair_impair_probs.values, palette="coolwarm")
    plt.title("R√©partition des num√©ros pairs et impairs", fontsize=16)
    plt.xlabel('Nombre de num√©ros pairs')
    plt.ylabel('Probabilit√©')
    plt.tight_layout()
    plt.savefig(output_dir / 'pair_impair_plot.png', dpi=150)
    plt.close()

def plot_consecutive_numbers_analysis(grids_df: pd.DataFrame, output_dir: Path):
    consecutive_counts = []
    for _, row in grids_df.iterrows():
        grid = np.array([row['boule_1'], row['boule_2'], row['boule_3'], row['boule_4'], row['boule_5']], dtype=np.int32)
        consecutive_counts.append(_count_consecutive_numba(grid))
    
    plt.figure(figsize=(10, 5))
    sns.countplot(x=consecutive_counts, palette="mako")
    plt.title("Fr√©quence des num√©ros cons√©cutifs", fontsize=16)
    plt.xlabel('Nombre de paires cons√©cutives')
    plt.ylabel('Fr√©quence')
    plt.tight_layout()
    plt.savefig(output_dir / 'consecutive_numbers_plot.png', dpi=150)
    plt.close()

def create_visualizations(criteria: dict, grids_df: pd.DataFrame, output_dir: Path):
    print("\nCr√©ation des visualisations...")
    plt.style.use('seaborn-v0_8-whitegrid')

    plot_frequency_analysis(criteria, output_dir)
    plot_gap_analysis(criteria, output_dir)
    plot_odd_even_analysis(criteria, output_dir)
    plot_consecutive_numbers_analysis(grids_df, output_dir)

    # Ajout des visualisations de cycles et motifs
    sums_series = criteria['sums']
    plot_autocorrelation(sums_series, output_dir)
    plot_fft(sums_series, output_dir)
    plot_seasonal_decomposition(sums_series, output_dir, period=10)
    plot_matrix_profile(sums_series, output_dir, window=10)

    print(f"   ‚úì Visualisations sauvegard√©es dans '{output_dir}'.")

# --- G√©n√©ration de Rapports ---
def create_report(criteria: dict, best_grids: list, output_dir: Path, exec_time: float, adaptive_strategy=None):
    print("G√©n√©ration du rapport Markdown...")
    
    top_5_grids_str = "\n".join([
        f"- **Grille {i+1}**: `{g['grid'][:-1]}` + Chance **{g['grid'][-1]}** (Score: {g['score']:.4f})" 
        for i, g in enumerate(best_grids[:5])
    ])
    
    hot_numbers_str = ", ".join(map(str, criteria['hot_numbers']))
    last_draw_str = ", ".join(map(str, criteria['last_draw']))
    
    top_pairs_str = "N/A"
    if not criteria['pair_counts'].empty:
        top_pairs_str = "\n".join([
            f"- `{int(p[0])}-{int(p[1])}`: {c} fois" 
            for (p, c) in criteria['pair_counts'].head(5).items()
        ])
        
    excluded_numbers_str = ", ".join(map(str, sorted(criteria.get('numbers_to_exclude', []))))    

    # Informations sur la strat√©gie adaptative
    adaptive_info = ""
    if adaptive_strategy:
        history = adaptive_strategy.performance_history
        if history and 'ml_scores' in history and len(history['ml_scores']) > 0:
            recent_count = min(5, len(history['ml_scores']))
            avg_ml_acc = np.mean(history['ml_scores'][-recent_count:])  # Moyenne sur les derni√®res √©valuations
            avg_freq_acc = np.mean(history['freq_scores'][-recent_count:])
            current_ml_weight = adaptive_strategy.ml_weight
            current_freq_weight = 1.0 - adaptive_strategy.ml_weight
            last_date = history['dates'][-1] if 'dates' in history and history['dates'] else "N/A"
            
            adaptive_info = f"""
## ü§ñ Strat√©gie Adaptative Machine Learning
- **Poids ML actuel**: `{current_ml_weight:.3f}` ({current_ml_weight*100:.1f}%)
- **Poids Fr√©quence actuel**: `{current_freq_weight:.3f}` ({current_freq_weight*100:.1f}%)
- **Pr√©cision ML r√©cente**: `{avg_ml_acc:.3f}` (moyenne {recent_count} derni√®res √©valuations)
- **Pr√©cision Fr√©quence r√©cente**: `{avg_freq_acc:.3f}` (moyenne {recent_count} derni√®res √©valuations)
- **√âvaluations effectu√©es**: `{len(history['ml_scores'])}`
- **Derni√®re √©valuation**: `{last_date[:19] if last_date != "N/A" else "N/A"}`
"""
        else:
            adaptive_info = f"""
## ü§ñ Strat√©gie Adaptative Machine Learning
- **Poids ML initial**: `{adaptive_strategy.ml_weight:.3f}` ({adaptive_strategy.ml_weight*100:.1f}%)
- **Poids Fr√©quence initial**: `{1.0 - adaptive_strategy.ml_weight:.3f}` ({(1.0 - adaptive_strategy.ml_weight)*100:.1f}%)
- **√âtat**: Premi√®re utilisation - collecte des donn√©es de performance en cours
"""    

    report_content = f"""# Rapport d'Analyse Loto - {datetime.now().strftime('%d/%m/%Y %H:%M')}

## ‚öôÔ∏è Configuration d'Ex√©cution
- **Temps d'ex√©cution**: `{exec_time:.2f} secondes`
- **Simulations**: `{N_SIMULATIONS:,}`
- **Seed reproductibilit√©**: `{GLOBAL_SEED}`
- **Processeurs utilis√©s**: `{N_CORES}`

## üéØ Top 5 Grilles Recommand√©es
{top_5_grids_str}


...
## üö´ Num√©ros Exclu (3 Derniers Tirages)
- **Num√©ros exclus des grilles g√©n√©r√©es** : `{excluded_numbers_str}`

*üìÅ Toutes les grilles sont disponibles dans `grilles_conseillees.csv`.*

## üìä R√©sum√© de l'Analyse Statistique
- **Dernier tirage**: `{last_draw_str}`
- **Num√©ros Hot (Top 10)**: `{hot_numbers_str}`
- **Poids de scoring dynamiques**: `{dict((k, round(v, 3)) for k, v in criteria['dynamic_weights'].items())}`

### üîó Top 5 Paires Fr√©quentes
{top_pairs_str}
{adaptive_info}

## üìà Fichiers G√©n√©r√©s
- **Analyse d√©taill√©e**: `numbers_analysis.csv`
- **Grilles compl√®tes**: `grilles_conseillees.csv`
- **Graphiques**: 
  - `frequence_numeros.png`
  - `periodicite_plot.png`
  - `pair_impair_plot.png`
  - `consecutive_numbers_plot.png`
  - `autocorrelation_plot.png`
  - `fft_spectrum_plot.png`
  - `stl_decomposition_plot.png`
  - `matrix_profile_plot.png`

## üìà Visualisations Cl√©s
![Fr√©quence des num√©ros](frequence_numeros.png)
![Autocorr√©lation des tirages](autocorrelation_plot.png)
![Spectre FFT](fft_spectrum_plot.png)
![D√©composition saisonni√®re STL](stl_decomposition_plot.png)
![Matrix Profile](matrix_profile_plot.png)

---
*Rapport g√©n√©r√© automatiquement par le syst√®me d'analyse Loto*
"""
    report_path = output_dir / 'rapport_analyse.md'
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report_content)
    
    print(f"   ‚úì Rapport sauvegard√© dans '{report_path}'.")

# --- Fonction Principale ---
def main():
    start_time = datetime.now()
    if not parquet_path.exists():
        print(f"‚ùå ERREUR: Fichier Parquet '{parquet_path}' non trouv√©.")
        return

    # Gestion du r√©-entra√Ænement forc√©
    if hasattr(parse_arguments(), 'retrain') and parse_arguments().retrain:
        print("üîÑ FORCE RE-TRAINING: Suppression des anciens mod√®les...")
        import shutil
        if MODEL_DIR.exists():
            shutil.rmtree(MODEL_DIR)
        MODEL_DIR.mkdir(exist_ok=True)
        print("   ‚úì Dossier mod√®les nettoy√©, entra√Ænement forc√©.")

    criteria = None
    cache_key = f"loto_criteria:{parquet_path.stat().st_mtime}"
    if redis_client:
        cached_criteria = redis_client.get(cache_key)
        if cached_criteria:
            print("CACHE HIT: Chargement des crit√®res depuis Redis...")
            criteria = pickle.loads(cached_criteria)
        else:
            print("CACHE MISS: Crit√®res non trouv√©s dans Redis.")

    if criteria is None:
        con = None
        try:
            con = duckdb.connect(database=':memory:', read_only=False)
            print(f"1. Chargement et analyse des donn√©es depuis '{parquet_path}'...")
            con.execute(f"CREATE TABLE loto_draws AS SELECT * FROM read_parquet('{str(parquet_path)}')")
            criteria = analyze_criteria_duckdb(con, 'loto_draws')
            
            # Mise √† jour de la strat√©gie adaptative avec les donn√©es r√©centes
            update_adaptive_strategy_with_recent_draws(con, adaptive_strategy)
            
            if redis_client:
                print("Sauvegarde des nouveaux crit√®res dans Redis (expiration: 4h)...")
                ttl_seconds = int(timedelta(hours=4).total_seconds())
                redis_client.setex(cache_key, ttl_seconds, pickle.dumps(criteria))
        finally:
            if con:
                con.close()

    analysis_csv_path = OUTPUT_DIR / 'numbers_analysis.csv'
    criteria['numbers_analysis'].to_csv(analysis_csv_path, index=False, float_format='%.2f', date_format='%Y-%m-%d')
    print(f"   ‚úì Analyse d√©taill√©e des num√©ros export√©e vers '{analysis_csv_path}'.")

    print("\n2. Gestion des mod√®les XGBoost...")
    
    if ARGS.retrain:
        print("  ‚ö†Ô∏è FLAG --retrain d√©tect√© : Nettoyage et r√©-entra√Ænement forc√©...")
        # Nettoyage complet des mod√®les existants
        import shutil
        boost_dir = Path("boost_models")
        if boost_dir.exists():
            print("  üóëÔ∏è Suppression du r√©pertoire boost_models existant...")
            shutil.rmtree(boost_dir)
        # Recr√©er le r√©pertoire
        boost_dir.mkdir(exist_ok=True)
        con = duckdb.connect(database=':memory:', read_only=False)
        con.execute(f"CREATE TABLE loto_draws AS SELECT * FROM read_parquet('{str(parquet_path)}')")
        df_full = con.table('loto_draws').fetchdf()
        con.close()
        models = train_xgboost_parallel(df_full)
    else:
        models = load_saved_models()
        # V√©rifier si on a suffisamment de mod√®les fonctionnels (au moins 70% des mod√®les)
        ball_models_available = sum(1 for m in models.get('balls', {}).values() if m is not None)
        chance_models_available = sum(1 for m in models.get('chance', {}).values() if m is not None)
        total_available = ball_models_available + chance_models_available
        
        if total_available < 42:  # Au moins 70% des 59 mod√®les (42/59)
            print(f"  Insuffisamment de mod√®les disponibles ({total_available}/59), r√©-entra√Ænement complet...")
            con = duckdb.connect(database=':memory:', read_only=False)
            con.execute(f"CREATE TABLE loto_draws AS SELECT * FROM read_parquet('{str(parquet_path)}')")
            df_full = con.table('loto_draws').fetchdf()
            con.close()
            models = train_xgboost_parallel(df_full)
        else:
            print(f"  ‚úì {total_available}/59 mod√®les disponibles ({ball_models_available} boules + {chance_models_available} chance)")

    print("\n3. Simulation intelligente des grilles...")
    X_last = np.array(criteria['last_draw']).reshape(1, -1)
    grids = simulate_grids_parallel(N_SIMULATIONS, criteria, X_last, models)
    print(f"   ‚úì {len(grids)} grilles g√©n√©r√©es et class√©es par score de conformit√©.")

    grids_df = pd.DataFrame()
    if grids:
        grids_df = pd.DataFrame(grids)
        grid_cols = grids_df['grid'].apply(pd.Series)
        grid_cols.columns = ['boule_1', 'boule_2', 'boule_3', 'boule_4', 'boule_5', 'numero_chance']
        grids_df = pd.concat([grid_cols, grids_df['score']], axis=1)
        
        grids_csv_path = OUTPUT_DIR / 'grilles_conseillees.csv'
        grids_df.to_csv(grids_csv_path, index=False, float_format='%.4f')
        print(f"   ‚úì {len(grids)} grilles sauvegard√©es dans '{grids_csv_path}'.")
    else:
        print("‚ö†Ô∏è Aucune grille g√©n√©r√©e, cr√©ation d'un DataFrame vide.")

    analyze_generated_grids(grids, criteria)

    print("\n4. Finalisation des rapports et visualisations...")
    if not grids_df.empty:
        create_visualizations(criteria, grids_df, OUTPUT_DIR)
    else:
        print("‚ö†Ô∏è Pas de visualisations cr√©√©es car aucune grille n'a √©t√© g√©n√©r√©e.")
    execution_time = (datetime.now() - start_time).total_seconds()
    create_report(criteria, grids, OUTPUT_DIR, execution_time, adaptive_strategy)

    print(f"\n‚úÖ Analyse termin√©e avec succ√®s en {execution_time:.2f} secondes.")
    print(f"üìÅ Tous les r√©sultats sont disponibles dans : '{OUTPUT_DIR.resolve()}'")
    
    if grids:
        print("\nüéØ Top 5 des grilles recommand√©es (score de conformit√© statistique) :")
        for i, grid_info in enumerate(grids[:5]):
            balls_str = ', '.join(map(str, grid_info['grid'][:-1]))
            chance = grid_info['grid'][-1]
            score = grid_info['score']
            print(f"   {i+1}. Boules: [{balls_str}] | Chance: {chance} | Score: {score:.4f}")

        print(f"\nüìä Statistiques de g√©n√©ration :")
        print(f"   - Grilles simul√©es : {N_SIMULATIONS:,}")
        print(f"   - Score moyen : {np.mean([g['score'] for g in grids]):.4f}")
        print(f"   - Score m√©dian : {np.median([g['score'] for g in grids]):.4f}")
        print(f"   - Meilleur score : {grids[0]['score']:.4f}")
        print(f"   - Pire score : {grids[-1]['score']:.4f}")
        
        # Affichage des informations de strat√©gie adaptative
        if adaptive_strategy:
            freq_weight = 1.0 - adaptive_strategy.ml_weight
            print(f"\nü§ñ Strat√©gie Adaptative Machine Learning :")
            print(f"   - Poids ML actuel : {adaptive_strategy.ml_weight:.3f} ({adaptive_strategy.ml_weight*100:.1f}%)")
            print(f"   - Poids Fr√©quence actuel : {freq_weight:.3f} ({freq_weight*100:.1f}%)")
            
            history = adaptive_strategy.performance_history
            if history and 'ml_scores' in history and len(history['ml_scores']) > 0:
                recent_count = min(5, len(history['ml_scores']))
                avg_ml_acc = np.mean(history['ml_scores'][-recent_count:])
                avg_freq_acc = np.mean(history['freq_scores'][-recent_count:])
                print(f"   - Pr√©cision ML r√©cente : {avg_ml_acc:.3f} (moyenne sur {recent_count} √©valuations)")
                print(f"   - Pr√©cision Fr√©quence r√©cente : {avg_freq_acc:.3f}")
                print(f"   - Total √©valuations : {len(history['ml_scores'])}")
            else:
                print(f"   - √âtat : Premi√®re utilisation - collecte des donn√©es en cours")
    else:
        print("\n‚ö†Ô∏è Aucune grille n'a √©t√© g√©n√©r√©e.")
        print("V√©rifiez les crit√®res d'exclusion et la configuration.")

    print(f"\nüé≤ Rappel : Ces grilles sont optimis√©es selon des crit√®res statistiques")
    print(f"   bas√©s sur l'historique, mais chaque tirage reste enti√®rement al√©atoire !")

if __name__ == '__main__':
    try:
        mp.set_start_method('spawn', force=True)
    except RuntimeError:
        pass
    
    print("üé≤ " + "="*60)
    print("   G√âN√âRATEUR INTELLIGENT DE GRILLES LOTO")
    print("   Analyse statistique + Machine Learning + Optimisation")
    print("="*64)
    
    main()
    
    print("\n" + "="*64)
    print("üèÅ Fin d'ex√©cution du g√©n√©rateur de grilles Loto")
    print("="*64)